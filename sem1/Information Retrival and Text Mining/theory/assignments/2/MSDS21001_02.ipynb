{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87488e24",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Structure</font>\n",
    "* **Preprocessing**: \n",
    "In preprocessing I am extracting files from .zip folder and creating a list containing name of all files. This list is then used in all tasks.\n",
    "* **Task**:\n",
    "The code is written separately for each task. Each task has two outputs. One is the the execution time of that task for the whole corpus and second is the output of code for the first document. \n",
    "* **Complete Model**:\n",
    "Combining all tasks to make a comepel model. From reading file to ranking a query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7a137",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Part 1: Preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8348223",
   "metadata": {},
   "source": [
    "## **Importing Libraries**\n",
    "\n",
    "I am not using NLTK and implementing everything using data structures of python. Numpy is only used to calculate logrithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d32d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a91f3",
   "metadata": {},
   "source": [
    "## **Extracting files from zip folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb1c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "file_name = \"ACL txt.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    #zip.printdir()\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa40d53",
   "metadata": {},
   "source": [
    "## **Creating a list of all file names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ee7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './ACL txt'\n",
    "docs_name_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492bcbc",
   "metadata": {},
   "source": [
    "## Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0907e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations =  '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b983f78",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef914019",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "              'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "              'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "              'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "              'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "              'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "              'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should',\n",
    "              \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n",
    "              'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affaf702",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Part 2: Tasks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c381dfc",
   "metadata": {},
   "source": [
    "# <font color='blue'>Task 1</font>\n",
    "Write a function named **wordList(doc)** in such a way that it takes a txt file as input argument and returns a list of words in your document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573fdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO READ TEXT FILE AND CONVERT IT INTO LIST OF WORDS\n",
    "\n",
    "def wordList(f_name):\n",
    "    #print('Opening: {}'.format(f_name))\n",
    "    with open(f_name, \"r\") as f:\n",
    "        word_list =[i for line in f for i in line.split()]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56543e",
   "metadata": {},
   "source": [
    "### **Output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6732f627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-1: 25.004003524780273\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus is printed\n",
    "\n",
    "word_list=[]\n",
    "start = time.time()\n",
    "for document in (docs_name_list):\n",
    "    word_list.append(wordList(os.path.join(path,document)))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Collective Execution Time for TASK-1: {}\".format(end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26adb43a",
   "metadata": {},
   "source": [
    "### **Output (code output)**\n",
    "Printing output for the whole corpus dosent make sense. So printing output for the first file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc118c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', 'Proceedings', 'of', 'the', 'Conference', 'April', '29--May', '4,', '2000', 'Seattle,', 'Washington,', 'USA', 'ANLP', '2000-PREFACE', '131', 'papers', 'were', 'submitted', 'to', 'ANLP-2000.', '46', 'were', 'accepted', 'for', 'presentation', 'at', 'the', 'conference.', 'Papers', 'came', 'from', '24', 'countries:', 'fifty', 'eight', 'from', 'the', 'United', 'States', 'of', 'America,', 'eleven', 'each', 'from', 'Germany', 'and', 'United', 'Kingdom,', 'nine', 'from', 'Canada,', 'eight', 'from', 'Japan,', 'four', 'each', 'from', 'Italy', 'and', 'Spain,', 'three', 'ach', 'from', 'France,', 'Korea', 'and', 'Switzerland,', 'two', 'each', 'from', 'Australia,', 'China,', 'The', 'Netherlands', 'and', 'Sweden', 'and', 'one', 'each', 'from', 'Czech', 'Republic,', 'Denmark,', 'Finland,', 'Greece,', 'India,', 'Hong', 'Kong,', 'Malaysia,', 'Norway,', 'Russia', 'and', 'Taiwan.', '40', 'papers', 'were', 'submitted', 'from', 'industry.', '85', 'papers', 'came', 'from', 'academia.', '2', 'papers', 'were', 'submitted', 'from', 'government', 'organizations', 'and', 'four', 'submissions', 'were', 'combined.', 'The', 'reviewing', 'process', 'was', 'supported', 'by', 'a', 'web-based', 'reviewer', 'interface', 'developed', 'by', 'Elisha', 'Kane', 'at', 'New', 'Mexico', 'State', \"University's\", 'Computing', 'Research', 'Lab.', 'Linda', 'Fresques', 'of', 'CRL', 'coordinated', 'the', 'refereeing', 'process.', 'I', 'would', 'like', 'to', 'express', 'my', 'gratitude', 'to', 'and', 'appreciation', 'fthe', 'Program', 'Committee', 'members', 'responsible', 'for', 'the', 'six', 'areas:', 'Lynn', 'Carlson,', 'Tools', 'and', 'Resources', 'for', 'Developing', 'NLP', 'Systems', 'Subcommittee', 'Eduard', 'Hovy,', 'Integrated', 'NLP', 'Systems', 'Subcommittee', 'Richard', 'Kittredge,', 'Multilingual', 'Text', 'Processing', 'Subcommittee', 'Ray', 'Perrault,', 'Spoken', 'Language', 'Systems', 'Subcommittee', 'Oliviero', 'Stock,', 'Monolingual', 'Text', 'Processing', 'Systems', 'Subcommittee', 'John', 'White,', 'Evaluation', 'Subcommittee', 'The', 'following', 'colleagues', 'did', 'Doug', 'Appelt', 'Fabio', 'Ciravegna', 'Robert', 'Dale', 'Michael', 'Elhadad', 'Ralph', 'G-rishman', 'Lynette', 'Hirschman', 'Yuval', 'Krymolowski', 'Inderjeet', 'Mani', 'Zvi', 'Marx', 'Martha', 'Palmer', 'Harold', 'Somers', 'Toshiyuki', 'Takezawa', 'Takehito', 'Utsuro', 'Dekai', 'Wu', 'the', 'bulk', 'of', 'the', 'reviewing:', 'Igor', 'Boguslavsky', 'Jim', 'Cowie', 'John', 'Dowding', 'Jim', 'Glass', 'Jan', 'Haji~', 'Pierre', 'IsabeIIe', 'Alberto', 'Lavelli', 'Daniel', 'Marcu', 'David', 'McDonald', 'Owen', 'Rainbow', 'Tomek', 'Strzalkowski', 'Kathryn', 'B.', 'Taylor', 'Pick', 'Vossen', 'R6mi', 'Zajac', 'David', 'Carter', 'Ido', 'Dagan', 'Andreas', 'Eiscle', 'Oren', 'Glikman', 'Donna', 'Harman', 'Tanya', 'Korelsky', 'Chin-Yew', 'Lin', 'Paul', 'Martin', 'Teruko', 'Mitamura', 'Norbert', 'Reithinger', 'Beth', 'Sundheim', 'Hans', 'Uszkoreit', 'Ralph', 'Weischedel', 'We', 'believe', 'that', 'he', 'quality', 'of', 'the', 'papers', 'elected', 'israther', 'high', 'and', 'hope', 'that', 'he', 'conference', 'will', 'be', 'a', 'success.', 'Sergei', 'Nirenburg', 'Chair,', 'Program', 'Committee', 'ANLP-2000', 'ANLPi', 'ANLP', 'Table', 'of', 'Contents', 'Section', '1:', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', '(ANLP)', 'ANLP', 'Preface', 'and', 'List', 'of', 'Reviewers', 'Sergei', 'Nirenburg,', 'Program', 'Committee', 'Chair', '...............................................................', 'ANLPi', 'BusTUC--A', 'Natural', 'Language', 'Bus', 'Route', 'Oracle', 'Tore', 'Amble', '..............................................................................................................................', '1', 'Machine', 'Translation', 'of', 'Very', 'Close', 'Languages', 'Jan', 'Haji~,', 'Jan', 'Hric,', 'Vladislav', 'Kubofi', '.......................................................................................', '7', 'Cross-Language', 'Multimedia', 'Information', 'Retrieval', 'Sharon', 'Flank', '..........................................................................................................................', '13', 'Automatic', 'Construction', 'of', 'Parallel', 'English-Chinese', 'Corpus', 'for', 'Cross-Language', 'Information', 'Retrieval', 'Jiang', 'Chen,', 'Jian-Yun', 'Nie', '......................................................................................................', '21', 'PartslD:', 'A', 'Dialogue-Based', 'System', 'for', 'Identifying', 'Parts', 'for', 'Medical', 'Systems', 'Amit', 'Bagga,', 'Tomek', 'Strzalkowski,', 'G.', 'Bowden', 'Wise', '...........................................................', '29', 'Translation', 'Using', 'Information', 'on', 'Dialogue', 'Participants', 'Setsuo', 'Yamada,', 'Eiichiro', 'Sumita,', 'Hideki', 'Kashioka', '..............................................................', '37', 'Distilling', 'Dialogues--A', 'Method', 'Using', 'Natural', 'Dialogue', 'Corpora', 'for', 'Dialogue', 'Systems', 'Development', 'Arne', 'J6nsson,', 'Nils', 'Dahlb~ick', '.................................................................................................', '44', 'Plan-Based', 'Dialogue', 'Management', 'ina', 'Physics', 'Tour', 'Reva', 'Freedman', '......................................................................................................................', '52', 'A', 'Framework', 'for', 'MT', 'and', 'Multilingual', 'NLG', 'Systems', 'Based', 'on', 'Uniform', 'Lexico-Structural', 'Processing', 'Benoit', 'Lavoie,', 'Richard', 'Kittredge,', 'Tanya', 'Korelsky,', 'Owen', 'Rambow', '...................................', '60', \"Talk'N'Travel:\", 'A', 'Conversational', 'System', 'for', 'Air', 'Travel', 'Planning', 'David', 'Stallard', '........................................................................................................................', '68', 'REES:', 'A', 'Large-Scale', 'Relation', 'and', 'Event', 'Extraction', 'System', 'Chinatsu', 'Aone,', 'Mila', 'Ramos-Santacruz', '.................................................................................', '76', 'Experiments', 'on', 'Sentence', 'Boundary', 'Detection', 'Mark', 'Stevenson,', 'Robert', 'Gaizauskas', '.....................................................................................', '84', 'DP:', 'A', 'Detector', 'for', 'Presuppositions', 'in', 'Survey', 'Questions', 'Katja', 'Wiemer-Hastings,', 'Peter', 'Wiemer-Hastings,', 'Sonya', 'Rajan,', 'Art', 'Graesser,', 'Roger', 'Kreuz,', 'Ashish', 'Karnavat', '..............................................................................................', '90', 'ANLPiii', 'MIMIC:', 'An', 'Adaptive', 'Mixed', 'Initiative', 'Spoken', 'Dialogue', 'System', 'for', 'Information', 'Queries', 'Jennifer', 'Chu-Carroll', '..............................................................................................................', '97', 'Javox:', 'A', 'Toolkit', 'for', 'Building', 'Speech-Enabled', 'Applications', 'Michael', 'S.', 'Fulkerson,', 'Alan', 'W.', 'Biermann', '...........................................................................', '105', 'A', 'Compact', 'Architecture', 'for', 'Dialogue', 'Management', 'Based', 'on', 'Scripts', 'and', 'Meta-Outputs', 'Manny', 'Rayner,', 'Beth', 'Ann', 'Hockey,', 'Frankie', 'James', '..............................................................', '112', 'A', 'Representation', 'for', 'Complex', 'and', 'Evolving', 'Data', 'Dependencies', 'in', 'Generation', 'C.', 'Mellish,', 'R.', 'Evans,', 'L.', 'Cahill,', 'C.', 'Doran,', 'D.', 'Paiva,', 'M.', 'Reape,', 'D.', 'Scott,', 'N.', 'Tipper', '.............................................................................', '119', 'An', 'Automatic', 'Reviser:', 'The', 'TransCheck', 'System', 'Jean-Marc', 'Jutras', '..................................................................................................................', '127', 'Unit', 'Completion', 'for', 'a', 'Computer-Aided', 'Translation', 'Typing', 'System', 'Philippe', 'Langlais,', 'George', 'Foster,', 'Guy', 'Lapalme', '.................................................................', '135', 'Multilingual', 'Coreference', 'Resolution', 'Sanda', 'M.', 'Harabagiu,', 'Steven', 'J.', 'Maiorano', '...........................................................................', '142', 'Ranking', 'Suspected', 'Answers', 'to', 'Natural', 'Language', 'Questions', 'Using', 'Predictive', 'Annotation', 'Dragomir', 'R.', 'Radev,', 'John', 'Prager,', 'Valerie', 'Samn', '.................................................................', '150', 'Message', 'Classification', 'i', 'the', 'Call', 'Center', 'Stephan', 'Busemann,', 'Sven', 'Schmeier,', 'Roman', 'G.', 'Arens', '.......................................................', '158', 'A', 'Question', 'Answering', 'System', 'Supported', 'by', 'Information', 'Extraction', 'Rohini', 'Srihari,', 'Wei', 'Li', '........................................................................................................', '166', 'Categorizing', 'Unknown', 'Words:', 'Using', 'Decision', 'Trees', 'to', 'Identify', 'Names', 'and', 'Misspellings', 'Janine', 'Toole', '.........................................................................................................................', '173', 'Examining', 'the', 'Role', 'of', 'Statistical', 'and', 'Linguistic', 'Knowledge', 'Sources', 'in', 'a', 'General-Knowledge', 'Question-Answering', 'System', 'Claire', 'Cardie,', 'Vincent', 'Ng,', 'David', 'Pierce,', 'Chris', 'Buckley', '....................................................', '180', 'Extracting', 'Molecular', 'Binding', 'Relationships', 'from', 'Biomedical', 'Text', 'Thomas', 'C.', 'Rindflesch,', 'Jayant', 'V.', 'Rajan,', 'Lawrence', 'Hunter', '................................................', '188', 'Compound', 'Noun', 'Segmentation', 'Based', 'on', 'Lexical', 'Data', 'Extracted', 'from', 'Corpus', 'Juntae', 'Yoon', '.........................................................................................................................', '196', 'Experiments', 'with', 'Corpus-Based', 'LFG', 'Specialization', 'Nicola', 'Cancedda,', 'Christer', 'Samuelsson', '...............................................................................', '204', 'A', 'Tool', 'for', 'Automated', 'Revision', 'of', 'Grammars', 'for', 'NLP', 'Systems', 'Nanda', 'Kambhatla,', 'Wlodek', 'Zadrozny', '.................................................................................', '210', 'ANLPiv', 'Aggressive', 'Morphology', 'for', 'Robust', 'Lexical', 'Coverage', 'William', 'A.', 'Woods', '...............................................................................................................', '218', 'TnT--A', 'Statistical', 'Part-of-Speech', 'Tagger', 'Thorsten', 'Brants', '...................................................................................................................', '224', 'Language', 'Independent', 'Morphological', 'Analysis', 'Tatsuo', 'Yamashita,', 'Yuji', 'Matsumoto', '....................................................................................', '232', 'A', 'Divide-and-Conquer', 'Strategy', 'for', 'Shallow', 'Parsing', 'of', 'German', 'Free', 'Texts', 'Giinter', 'Neumann,', 'Christian', 'Braun,', 'Jakub', 'Piskorski', '...........................................................', '239', 'A', 'Hybrid', 'Approach', 'for', 'Named', 'Entity', 'and', 'Sub-Type', 'Tagging', 'Rohini', 'Srihari,', 'Cheng', 'Niu,', 'Wei', 'Li', '......................................................................................', '247', 'Spelling', 'and', 'Grammar', 'Correction', 'for', 'Danish', 'in', 'SCARRIE', 'Patrizia', 'Paggio', '.....................................................................................................................', '255', 'Linguistic', 'Knowledge', 'Can', 'Improve', 'Information', 'Retrieval', 'William', 'A.', 'Woods,', 'Lawrence', 'A.', 'Bookman,', 'Ann', 'Houston,', 'Robert', 'J.', 'Kuhns,', 'Paul', 'Martin,', 'Stephen', 'Green', '.................................................................................................', '262', 'Domain-Specific', 'Knowledge', 'Acquisition', 'from', 'Text', 'Dan', 'Moldovan,', 'Roxana', 'Girju,', 'Vasile', 'Rus', '..........................................................................', '268', 'Large-Scale', 'Controlled', 'Vocabulary', 'Indexing', 'for', 'Named', 'Entities', 'Mark', 'Wasson', '.......................................................................................................................', '276', 'Unsupervised', 'Discovery', 'of', 'Scenario-Level', 'Patterns', 'for', 'Information', 'Extraction', 'Roman', 'Yangarber,', 'Ralph', 'Grishman,', 'Pasi', 'Tapanainen,', 'Silja', 'Huttunen', '..............................', '282', 'Using', 'Corpus-Derived', 'Name', 'Lists', 'for', 'Named', 'Entity', 'Recognition', 'Mark', 'Stevenson,', 'Robert', 'Gaizauskas', '...................................................................................', '290', 'Answer', 'Extraction', 'Steven', 'Abney,', 'Michael', 'Collins,', 'Amit', 'Singhal', '....................................................................', '296', 'Evaluation', 'of', 'Automatically', 'Identified', 'Index', 'Terms', 'for', 'Browsing', 'Electronic', 'Documents', 'Nina', 'Wacholder,', 'Judith', 'L.', 'Klavans,', 'David', 'K.', 'Evans', '.........................................................', '302', 'Sentence', 'Reduction', 'for', 'Automatic', 'Text', 'Summarization', 'Hongyan', 'Jing', '.......................................................................................................................', '310', 'Named', 'Entity', 'Extraction', 'from', 'Noisy', 'Input:', 'Speech', 'and', 'OCR', 'David', 'Miller,', 'Sean', 'Boisen,', 'Richard', 'Schwartz,', 'Rebecca', 'Stone,', 'Ralph', 'Weischedel', '...........', '316', 'Improving', 'Testsuites', 'via', 'Instrumentation', 'Norbert', 'Br6ker', '.....................................................................................................................', '325', 'The', 'Efficiency', 'of', 'Multimodal', 'Interaction', 'for', 'a', 'Map-Based', 'Task', 'Philip', 'Cohen,', 'David', 'McGee,', 'Josh', 'Clow', '................................................................', '331', 'ANLPv']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673f620",
   "metadata": {},
   "source": [
    "# <font color='blue'>Task 2</font>\n",
    "Write a function named **removePuncs(wordList)** that takes list of words then iterate through this list. During iteration it do some processing on each word. Function should replace punctuation marks as well as \\n. and check either this word in stopword on not? if it is in stopword then we didn't append this into resulting List. You must also handle case insensitivity of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e84c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNTION TO REMOVE STOP WORDS AND PUNCTUATIONS FROM A LIST OF WORDS\n",
    "\n",
    "def removePuncs(w_list):\n",
    "    #print(\"removing punctuations and stop words\")\n",
    "    return [ i for i in list(map(w_stop_check,w_list)) if i != '']\n",
    "\n",
    "\n",
    "# FUNCTION TO REMOVE PUNCTUATIONS\n",
    "\n",
    "# This function compare each character in a provided word to a list containing punctuation character\n",
    "# and returns empty string if character is a punctuation character\n",
    "\n",
    "def c_punc_check(c):\n",
    "    if c in punctuations:\n",
    "        return '' \n",
    "    else:\n",
    "        return c\n",
    "        \n",
    "\n",
    "# FUNTION TO REMOVE STOP WORDS AND PUNCTUATIONS FROM A WORD\n",
    "\n",
    "def w_stop_check(w):\n",
    "    if w in stop_words:\n",
    "        return ''\n",
    "    else:\n",
    "        r = map(c_punc_check,w)\n",
    "        #converting list of characters retured by map funtion to string\n",
    "        return (''.join(map(str,list(r)))).lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73cab",
   "metadata": {},
   "source": [
    "### **Output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5379467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-2: 345.8576776981354\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus\n",
    "filter_word_list =[]\n",
    "start = time.time()\n",
    "for l in word_list:\n",
    "    filter_word_list.append(removePuncs(l))  \n",
    "end = time.time()\n",
    "print(\"Collective Execution Time for TASK-2: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ae9fd",
   "metadata": {},
   "source": [
    "### **Output (Code Output)**\n",
    "Printing output for the whole corpus dosent make sense. So printing output for the first file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2219dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['association', 'computational', 'linguistics', '6', 'th', 'applied', 'natural', 'language', 'processing', 'conference', 'proceedings', 'conference', 'april', '29may', '4', '2000', 'seattle', 'washington', 'usa', 'anlp', '2000preface', '131', 'papers', 'submitted', 'anlp2000', '46', 'accepted', 'presentation', 'conference', 'papers', 'came', '24', 'countries', 'fifty', 'eight', 'united', 'states', 'america', 'eleven', 'germany', 'united', 'kingdom', 'nine', 'canada', 'eight', 'japan', 'four', 'italy', 'spain', 'three', 'ach', 'france', 'korea', 'switzerland', 'two', 'australia', 'china', 'the', 'netherlands', 'sweden', 'one', 'czech', 'republic', 'denmark', 'finland', 'greece', 'india', 'hong', 'kong', 'malaysia', 'norway', 'russia', 'taiwan', '40', 'papers', 'submitted', 'industry', '85', 'papers', 'came', 'academia', '2', 'papers', 'submitted', 'government', 'organizations', 'four', 'submissions', 'combined', 'the', 'reviewing', 'process', 'supported', 'webbased', 'reviewer', 'interface', 'developed', 'elisha', 'kane', 'new', 'mexico', 'state', 'universitys', 'computing', 'research', 'lab', 'linda', 'fresques', 'crl', 'coordinated', 'refereeing', 'process', 'i', 'would', 'like', 'express', 'gratitude', 'appreciation', 'fthe', 'program', 'committee', 'members', 'responsible', 'six', 'areas', 'lynn', 'carlson', 'tools', 'resources', 'developing', 'nlp', 'systems', 'subcommittee', 'eduard', 'hovy', 'integrated', 'nlp', 'systems', 'subcommittee', 'richard', 'kittredge', 'multilingual', 'text', 'processing', 'subcommittee', 'ray', 'perrault', 'spoken', 'language', 'systems', 'subcommittee', 'oliviero', 'stock', 'monolingual', 'text', 'processing', 'systems', 'subcommittee', 'john', 'white', 'evaluation', 'subcommittee', 'the', 'following', 'colleagues', 'doug', 'appelt', 'fabio', 'ciravegna', 'robert', 'dale', 'michael', 'elhadad', 'ralph', 'grishman', 'lynette', 'hirschman', 'yuval', 'krymolowski', 'inderjeet', 'mani', 'zvi', 'marx', 'martha', 'palmer', 'harold', 'somers', 'toshiyuki', 'takezawa', 'takehito', 'utsuro', 'dekai', 'wu', 'bulk', 'reviewing', 'igor', 'boguslavsky', 'jim', 'cowie', 'john', 'dowding', 'jim', 'glass', 'jan', 'haji', 'pierre', 'isabeiie', 'alberto', 'lavelli', 'daniel', 'marcu', 'david', 'mcdonald', 'owen', 'rainbow', 'tomek', 'strzalkowski', 'kathryn', 'b', 'taylor', 'pick', 'vossen', 'r6mi', 'zajac', 'david', 'carter', 'ido', 'dagan', 'andreas', 'eiscle', 'oren', 'glikman', 'donna', 'harman', 'tanya', 'korelsky', 'chinyew', 'lin', 'paul', 'martin', 'teruko', 'mitamura', 'norbert', 'reithinger', 'beth', 'sundheim', 'hans', 'uszkoreit', 'ralph', 'weischedel', 'we', 'believe', 'quality', 'papers', 'elected', 'israther', 'high', 'hope', 'conference', 'success', 'sergei', 'nirenburg', 'chair', 'program', 'committee', 'anlp2000', 'anlpi', 'anlp', 'table', 'contents', 'section', '1', 'applied', 'natural', 'language', 'processing', 'conference', 'anlp', 'anlp', 'preface', 'list', 'reviewers', 'sergei', 'nirenburg', 'program', 'committee', 'chair', 'anlpi', 'bustuca', 'natural', 'language', 'bus', 'route', 'oracle', 'tore', 'amble', '1', 'machine', 'translation', 'very', 'close', 'languages', 'jan', 'haji', 'jan', 'hric', 'vladislav', 'kubofi', '7', 'crosslanguage', 'multimedia', 'information', 'retrieval', 'sharon', 'flank', '13', 'automatic', 'construction', 'parallel', 'englishchinese', 'corpus', 'crosslanguage', 'information', 'retrieval', 'jiang', 'chen', 'jianyun', 'nie', '21', 'partsld', 'a', 'dialoguebased', 'system', 'identifying', 'parts', 'medical', 'systems', 'amit', 'bagga', 'tomek', 'strzalkowski', 'g', 'bowden', 'wise', '29', 'translation', 'using', 'information', 'dialogue', 'participants', 'setsuo', 'yamada', 'eiichiro', 'sumita', 'hideki', 'kashioka', '37', 'distilling', 'dialoguesa', 'method', 'using', 'natural', 'dialogue', 'corpora', 'dialogue', 'systems', 'development', 'arne', 'j6nsson', 'nils', 'dahlbick', '44', 'planbased', 'dialogue', 'management', 'ina', 'physics', 'tour', 'reva', 'freedman', '52', 'a', 'framework', 'mt', 'multilingual', 'nlg', 'systems', 'based', 'uniform', 'lexicostructural', 'processing', 'benoit', 'lavoie', 'richard', 'kittredge', 'tanya', 'korelsky', 'owen', 'rambow', '60', 'talkntravel', 'a', 'conversational', 'system', 'air', 'travel', 'planning', 'david', 'stallard', '68', 'rees', 'a', 'largescale', 'relation', 'event', 'extraction', 'system', 'chinatsu', 'aone', 'mila', 'ramossantacruz', '76', 'experiments', 'sentence', 'boundary', 'detection', 'mark', 'stevenson', 'robert', 'gaizauskas', '84', 'dp', 'a', 'detector', 'presuppositions', 'survey', 'questions', 'katja', 'wiemerhastings', 'peter', 'wiemerhastings', 'sonya', 'rajan', 'art', 'graesser', 'roger', 'kreuz', 'ashish', 'karnavat', '90', 'anlpiii', 'mimic', 'an', 'adaptive', 'mixed', 'initiative', 'spoken', 'dialogue', 'system', 'information', 'queries', 'jennifer', 'chucarroll', '97', 'javox', 'a', 'toolkit', 'building', 'speechenabled', 'applications', 'michael', 's', 'fulkerson', 'alan', 'w', 'biermann', '105', 'a', 'compact', 'architecture', 'dialogue', 'management', 'based', 'scripts', 'metaoutputs', 'manny', 'rayner', 'beth', 'ann', 'hockey', 'frankie', 'james', '112', 'a', 'representation', 'complex', 'evolving', 'data', 'dependencies', 'generation', 'c', 'mellish', 'r', 'evans', 'l', 'cahill', 'c', 'doran', 'd', 'paiva', 'm', 'reape', 'd', 'scott', 'n', 'tipper', '119', 'an', 'automatic', 'reviser', 'the', 'transcheck', 'system', 'jeanmarc', 'jutras', '127', 'unit', 'completion', 'computeraided', 'translation', 'typing', 'system', 'philippe', 'langlais', 'george', 'foster', 'guy', 'lapalme', '135', 'multilingual', 'coreference', 'resolution', 'sanda', 'm', 'harabagiu', 'steven', 'j', 'maiorano', '142', 'ranking', 'suspected', 'answers', 'natural', 'language', 'questions', 'using', 'predictive', 'annotation', 'dragomir', 'r', 'radev', 'john', 'prager', 'valerie', 'samn', '150', 'message', 'classification', 'call', 'center', 'stephan', 'busemann', 'sven', 'schmeier', 'roman', 'g', 'arens', '158', 'a', 'question', 'answering', 'system', 'supported', 'information', 'extraction', 'rohini', 'srihari', 'wei', 'li', '166', 'categorizing', 'unknown', 'words', 'using', 'decision', 'trees', 'identify', 'names', 'misspellings', 'janine', 'toole', '173', 'examining', 'role', 'statistical', 'linguistic', 'knowledge', 'sources', 'generalknowledge', 'questionanswering', 'system', 'claire', 'cardie', 'vincent', 'ng', 'david', 'pierce', 'chris', 'buckley', '180', 'extracting', 'molecular', 'binding', 'relationships', 'biomedical', 'text', 'thomas', 'c', 'rindflesch', 'jayant', 'v', 'rajan', 'lawrence', 'hunter', '188', 'compound', 'noun', 'segmentation', 'based', 'lexical', 'data', 'extracted', 'corpus', 'juntae', 'yoon', '196', 'experiments', 'corpusbased', 'lfg', 'specialization', 'nicola', 'cancedda', 'christer', 'samuelsson', '204', 'a', 'tool', 'automated', 'revision', 'grammars', 'nlp', 'systems', 'nanda', 'kambhatla', 'wlodek', 'zadrozny', '210', 'anlpiv', 'aggressive', 'morphology', 'robust', 'lexical', 'coverage', 'william', 'a', 'woods', '218', 'tnta', 'statistical', 'partofspeech', 'tagger', 'thorsten', 'brants', '224', 'language', 'independent', 'morphological', 'analysis', 'tatsuo', 'yamashita', 'yuji', 'matsumoto', '232', 'a', 'divideandconquer', 'strategy', 'shallow', 'parsing', 'german', 'free', 'texts', 'giinter', 'neumann', 'christian', 'braun', 'jakub', 'piskorski', '239', 'a', 'hybrid', 'approach', 'named', 'entity', 'subtype', 'tagging', 'rohini', 'srihari', 'cheng', 'niu', 'wei', 'li', '247', 'spelling', 'grammar', 'correction', 'danish', 'scarrie', 'patrizia', 'paggio', '255', 'linguistic', 'knowledge', 'can', 'improve', 'information', 'retrieval', 'william', 'a', 'woods', 'lawrence', 'a', 'bookman', 'ann', 'houston', 'robert', 'j', 'kuhns', 'paul', 'martin', 'stephen', 'green', '262', 'domainspecific', 'knowledge', 'acquisition', 'text', 'dan', 'moldovan', 'roxana', 'girju', 'vasile', 'rus', '268', 'largescale', 'controlled', 'vocabulary', 'indexing', 'named', 'entities', 'mark', 'wasson', '276', 'unsupervised', 'discovery', 'scenariolevel', 'patterns', 'information', 'extraction', 'roman', 'yangarber', 'ralph', 'grishman', 'pasi', 'tapanainen', 'silja', 'huttunen', '282', 'using', 'corpusderived', 'name', 'lists', 'named', 'entity', 'recognition', 'mark', 'stevenson', 'robert', 'gaizauskas', '290', 'answer', 'extraction', 'steven', 'abney', 'michael', 'collins', 'amit', 'singhal', '296', 'evaluation', 'automatically', 'identified', 'index', 'terms', 'browsing', 'electronic', 'documents', 'nina', 'wacholder', 'judith', 'l', 'klavans', 'david', 'k', 'evans', '302', 'sentence', 'reduction', 'automatic', 'text', 'summarization', 'hongyan', 'jing', '310', 'named', 'entity', 'extraction', 'noisy', 'input', 'speech', 'ocr', 'david', 'miller', 'sean', 'boisen', 'richard', 'schwartz', 'rebecca', 'stone', 'ralph', 'weischedel', '316', 'improving', 'testsuites', 'via', 'instrumentation', 'norbert', 'br6ker', '325', 'the', 'efficiency', 'multimodal', 'interaction', 'mapbased', 'task', 'philip', 'cohen', 'david', 'mcgee', 'josh', 'clow', '331', 'anlpv']\n"
     ]
    }
   ],
   "source": [
    "print(filter_word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f8dfb",
   "metadata": {},
   "source": [
    "# <font color= 'blue'>Task 3</font> \n",
    "Write a function named **termFrequencyInDoc(wordList)** which should tak a list of words as input argument, and output a dictionary of words such that each word that appears in the document is key in the dictionary and it's value is term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1da14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(wordList):\n",
    "    word_dict = {}\n",
    "    \n",
    "    for word in wordList:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda095d",
   "metadata": {},
   "source": [
    "### **output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0ed3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-3: 118.6269166469574\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus is printed\n",
    "\n",
    "dic_list = []\n",
    "start = time.time()\n",
    "for i,l in enumerate(filter_word_list):\n",
    "    dic_list.append({docs_name_list[i]:termFrequencyInDoc(l)})   \n",
    "end = time.time()\n",
    "print(\"Collective Execution Time for TASK-3: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c57bd",
   "metadata": {},
   "source": [
    "### **output (code Output)**\n",
    "Printing output for the whole corpus dosent make sense. So printing output for the first file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b375fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A00-1000.pdf.txt': {'association': 1, 'computational': 1, 'linguistics': 1, '6': 1, 'th': 1, 'applied': 2, 'natural': 5, 'language': 6, 'processing': 5, 'conference': 5, 'proceedings': 1, 'april': 1, '29may': 1, '4': 1, '2000': 1, 'seattle': 1, 'washington': 1, 'usa': 1, 'anlp': 4, '2000preface': 1, '131': 1, 'papers': 6, 'submitted': 3, 'anlp2000': 2, '46': 1, 'accepted': 1, 'presentation': 1, 'came': 2, '24': 1, 'countries': 1, 'fifty': 1, 'eight': 2, 'united': 2, 'states': 1, 'america': 1, 'eleven': 1, 'germany': 1, 'kingdom': 1, 'nine': 1, 'canada': 1, 'japan': 1, 'four': 2, 'italy': 1, 'spain': 1, 'three': 1, 'ach': 1, 'france': 1, 'korea': 1, 'switzerland': 1, 'two': 1, 'australia': 1, 'china': 1, 'the': 5, 'netherlands': 1, 'sweden': 1, 'one': 1, 'czech': 1, 'republic': 1, 'denmark': 1, 'finland': 1, 'greece': 1, 'india': 1, 'hong': 1, 'kong': 1, 'malaysia': 1, 'norway': 1, 'russia': 1, 'taiwan': 1, '40': 1, 'industry': 1, '85': 1, 'academia': 1, '2': 1, 'government': 1, 'organizations': 1, 'submissions': 1, 'combined': 1, 'reviewing': 2, 'process': 2, 'supported': 2, 'webbased': 1, 'reviewer': 1, 'interface': 1, 'developed': 1, 'elisha': 1, 'kane': 1, 'new': 1, 'mexico': 1, 'state': 1, 'universitys': 1, 'computing': 1, 'research': 1, 'lab': 1, 'linda': 1, 'fresques': 1, 'crl': 1, 'coordinated': 1, 'refereeing': 1, 'i': 1, 'would': 1, 'like': 1, 'express': 1, 'gratitude': 1, 'appreciation': 1, 'fthe': 1, 'program': 3, 'committee': 3, 'members': 1, 'responsible': 1, 'six': 1, 'areas': 1, 'lynn': 1, 'carlson': 1, 'tools': 1, 'resources': 1, 'developing': 1, 'nlp': 3, 'systems': 8, 'subcommittee': 6, 'eduard': 1, 'hovy': 1, 'integrated': 1, 'richard': 3, 'kittredge': 2, 'multilingual': 3, 'text': 5, 'ray': 1, 'perrault': 1, 'spoken': 2, 'oliviero': 1, 'stock': 1, 'monolingual': 1, 'john': 3, 'white': 1, 'evaluation': 2, 'following': 1, 'colleagues': 1, 'doug': 1, 'appelt': 1, 'fabio': 1, 'ciravegna': 1, 'robert': 4, 'dale': 1, 'michael': 3, 'elhadad': 1, 'ralph': 4, 'grishman': 2, 'lynette': 1, 'hirschman': 1, 'yuval': 1, 'krymolowski': 1, 'inderjeet': 1, 'mani': 1, 'zvi': 1, 'marx': 1, 'martha': 1, 'palmer': 1, 'harold': 1, 'somers': 1, 'toshiyuki': 1, 'takezawa': 1, 'takehito': 1, 'utsuro': 1, 'dekai': 1, 'wu': 1, 'bulk': 1, 'igor': 1, 'boguslavsky': 1, 'jim': 2, 'cowie': 1, 'dowding': 1, 'glass': 1, 'jan': 3, 'haji': 2, 'pierre': 1, 'isabeiie': 1, 'alberto': 1, 'lavelli': 1, 'daniel': 1, 'marcu': 1, 'david': 7, 'mcdonald': 1, 'owen': 2, 'rainbow': 1, 'tomek': 2, 'strzalkowski': 2, 'kathryn': 1, 'b': 1, 'taylor': 1, 'pick': 1, 'vossen': 1, 'r6mi': 1, 'zajac': 1, 'carter': 1, 'ido': 1, 'dagan': 1, 'andreas': 1, 'eiscle': 1, 'oren': 1, 'glikman': 1, 'donna': 1, 'harman': 1, 'tanya': 2, 'korelsky': 2, 'chinyew': 1, 'lin': 1, 'paul': 2, 'martin': 2, 'teruko': 1, 'mitamura': 1, 'norbert': 2, 'reithinger': 1, 'beth': 2, 'sundheim': 1, 'hans': 1, 'uszkoreit': 1, 'weischedel': 2, 'we': 1, 'believe': 1, 'quality': 1, 'elected': 1, 'israther': 1, 'high': 1, 'hope': 1, 'success': 1, 'sergei': 2, 'nirenburg': 2, 'chair': 2, 'anlpi': 2, 'table': 1, 'contents': 1, 'section': 1, '1': 2, 'preface': 1, 'list': 1, 'reviewers': 1, 'bustuca': 1, 'bus': 1, 'route': 1, 'oracle': 1, 'tore': 1, 'amble': 1, 'machine': 1, 'translation': 3, 'very': 1, 'close': 1, 'languages': 1, 'hric': 1, 'vladislav': 1, 'kubofi': 1, '7': 1, 'crosslanguage': 2, 'multimedia': 1, 'information': 7, 'retrieval': 3, 'sharon': 1, 'flank': 1, '13': 1, 'automatic': 3, 'construction': 1, 'parallel': 1, 'englishchinese': 1, 'corpus': 2, 'jiang': 1, 'chen': 1, 'jianyun': 1, 'nie': 1, '21': 1, 'partsld': 1, 'a': 15, 'dialoguebased': 1, 'system': 8, 'identifying': 1, 'parts': 1, 'medical': 1, 'amit': 2, 'bagga': 1, 'g': 2, 'bowden': 1, 'wise': 1, '29': 1, 'using': 5, 'dialogue': 6, 'participants': 1, 'setsuo': 1, 'yamada': 1, 'eiichiro': 1, 'sumita': 1, 'hideki': 1, 'kashioka': 1, '37': 1, 'distilling': 1, 'dialoguesa': 1, 'method': 1, 'corpora': 1, 'development': 1, 'arne': 1, 'j6nsson': 1, 'nils': 1, 'dahlbick': 1, '44': 1, 'planbased': 1, 'management': 2, 'ina': 1, 'physics': 1, 'tour': 1, 'reva': 1, 'freedman': 1, '52': 1, 'framework': 1, 'mt': 1, 'nlg': 1, 'based': 3, 'uniform': 1, 'lexicostructural': 1, 'benoit': 1, 'lavoie': 1, 'rambow': 1, '60': 1, 'talkntravel': 1, 'conversational': 1, 'air': 1, 'travel': 1, 'planning': 1, 'stallard': 1, '68': 1, 'rees': 1, 'largescale': 2, 'relation': 1, 'event': 1, 'extraction': 5, 'chinatsu': 1, 'aone': 1, 'mila': 1, 'ramossantacruz': 1, '76': 1, 'experiments': 2, 'sentence': 2, 'boundary': 1, 'detection': 1, 'mark': 3, 'stevenson': 2, 'gaizauskas': 2, '84': 1, 'dp': 1, 'detector': 1, 'presuppositions': 1, 'survey': 1, 'questions': 2, 'katja': 1, 'wiemerhastings': 2, 'peter': 1, 'sonya': 1, 'rajan': 2, 'art': 1, 'graesser': 1, 'roger': 1, 'kreuz': 1, 'ashish': 1, 'karnavat': 1, '90': 1, 'anlpiii': 1, 'mimic': 1, 'an': 2, 'adaptive': 1, 'mixed': 1, 'initiative': 1, 'queries': 1, 'jennifer': 1, 'chucarroll': 1, '97': 1, 'javox': 1, 'toolkit': 1, 'building': 1, 'speechenabled': 1, 'applications': 1, 's': 1, 'fulkerson': 1, 'alan': 1, 'w': 1, 'biermann': 1, '105': 1, 'compact': 1, 'architecture': 1, 'scripts': 1, 'metaoutputs': 1, 'manny': 1, 'rayner': 1, 'ann': 2, 'hockey': 1, 'frankie': 1, 'james': 1, '112': 1, 'representation': 1, 'complex': 1, 'evolving': 1, 'data': 2, 'dependencies': 1, 'generation': 1, 'c': 3, 'mellish': 1, 'r': 2, 'evans': 2, 'l': 2, 'cahill': 1, 'doran': 1, 'd': 2, 'paiva': 1, 'm': 2, 'reape': 1, 'scott': 1, 'n': 1, 'tipper': 1, '119': 1, 'reviser': 1, 'transcheck': 1, 'jeanmarc': 1, 'jutras': 1, '127': 1, 'unit': 1, 'completion': 1, 'computeraided': 1, 'typing': 1, 'philippe': 1, 'langlais': 1, 'george': 1, 'foster': 1, 'guy': 1, 'lapalme': 1, '135': 1, 'coreference': 1, 'resolution': 1, 'sanda': 1, 'harabagiu': 1, 'steven': 2, 'j': 2, 'maiorano': 1, '142': 1, 'ranking': 1, 'suspected': 1, 'answers': 1, 'predictive': 1, 'annotation': 1, 'dragomir': 1, 'radev': 1, 'prager': 1, 'valerie': 1, 'samn': 1, '150': 1, 'message': 1, 'classification': 1, 'call': 1, 'center': 1, 'stephan': 1, 'busemann': 1, 'sven': 1, 'schmeier': 1, 'roman': 2, 'arens': 1, '158': 1, 'question': 1, 'answering': 1, 'rohini': 2, 'srihari': 2, 'wei': 2, 'li': 2, '166': 1, 'categorizing': 1, 'unknown': 1, 'words': 1, 'decision': 1, 'trees': 1, 'identify': 1, 'names': 1, 'misspellings': 1, 'janine': 1, 'toole': 1, '173': 1, 'examining': 1, 'role': 1, 'statistical': 2, 'linguistic': 2, 'knowledge': 3, 'sources': 1, 'generalknowledge': 1, 'questionanswering': 1, 'claire': 1, 'cardie': 1, 'vincent': 1, 'ng': 1, 'pierce': 1, 'chris': 1, 'buckley': 1, '180': 1, 'extracting': 1, 'molecular': 1, 'binding': 1, 'relationships': 1, 'biomedical': 1, 'thomas': 1, 'rindflesch': 1, 'jayant': 1, 'v': 1, 'lawrence': 2, 'hunter': 1, '188': 1, 'compound': 1, 'noun': 1, 'segmentation': 1, 'lexical': 2, 'extracted': 1, 'juntae': 1, 'yoon': 1, '196': 1, 'corpusbased': 1, 'lfg': 1, 'specialization': 1, 'nicola': 1, 'cancedda': 1, 'christer': 1, 'samuelsson': 1, '204': 1, 'tool': 1, 'automated': 1, 'revision': 1, 'grammars': 1, 'nanda': 1, 'kambhatla': 1, 'wlodek': 1, 'zadrozny': 1, '210': 1, 'anlpiv': 1, 'aggressive': 1, 'morphology': 1, 'robust': 1, 'coverage': 1, 'william': 2, 'woods': 2, '218': 1, 'tnta': 1, 'partofspeech': 1, 'tagger': 1, 'thorsten': 1, 'brants': 1, '224': 1, 'independent': 1, 'morphological': 1, 'analysis': 1, 'tatsuo': 1, 'yamashita': 1, 'yuji': 1, 'matsumoto': 1, '232': 1, 'divideandconquer': 1, 'strategy': 1, 'shallow': 1, 'parsing': 1, 'german': 1, 'free': 1, 'texts': 1, 'giinter': 1, 'neumann': 1, 'christian': 1, 'braun': 1, 'jakub': 1, 'piskorski': 1, '239': 1, 'hybrid': 1, 'approach': 1, 'named': 4, 'entity': 3, 'subtype': 1, 'tagging': 1, 'cheng': 1, 'niu': 1, '247': 1, 'spelling': 1, 'grammar': 1, 'correction': 1, 'danish': 1, 'scarrie': 1, 'patrizia': 1, 'paggio': 1, '255': 1, 'can': 1, 'improve': 1, 'bookman': 1, 'houston': 1, 'kuhns': 1, 'stephen': 1, 'green': 1, '262': 1, 'domainspecific': 1, 'acquisition': 1, 'dan': 1, 'moldovan': 1, 'roxana': 1, 'girju': 1, 'vasile': 1, 'rus': 1, '268': 1, 'controlled': 1, 'vocabulary': 1, 'indexing': 1, 'entities': 1, 'wasson': 1, '276': 1, 'unsupervised': 1, 'discovery': 1, 'scenariolevel': 1, 'patterns': 1, 'yangarber': 1, 'pasi': 1, 'tapanainen': 1, 'silja': 1, 'huttunen': 1, '282': 1, 'corpusderived': 1, 'name': 1, 'lists': 1, 'recognition': 1, '290': 1, 'answer': 1, 'abney': 1, 'collins': 1, 'singhal': 1, '296': 1, 'automatically': 1, 'identified': 1, 'index': 1, 'terms': 1, 'browsing': 1, 'electronic': 1, 'documents': 1, 'nina': 1, 'wacholder': 1, 'judith': 1, 'klavans': 1, 'k': 1, '302': 1, 'reduction': 1, 'summarization': 1, 'hongyan': 1, 'jing': 1, '310': 1, 'noisy': 1, 'input': 1, 'speech': 1, 'ocr': 1, 'miller': 1, 'sean': 1, 'boisen': 1, 'schwartz': 1, 'rebecca': 1, 'stone': 1, '316': 1, 'improving': 1, 'testsuites': 1, 'via': 1, 'instrumentation': 1, 'br6ker': 1, '325': 1, 'efficiency': 1, 'multimodal': 1, 'interaction': 1, 'mapbased': 1, 'task': 1, 'philip': 1, 'cohen': 1, 'mcgee': 1, 'josh': 1, 'clow': 1, '331': 1, 'anlpv': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(dic_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e997d02",
   "metadata": {},
   "source": [
    "# <font color = 'blue'> Task 4 </font>\n",
    "Write a function named **wordDocFre(dicList)** that takes list of dictionary as input argument, each dictionary in this list is the word that appears in the given document as keys and the no. of times the word appears as value. This function should construct a dictionary which has all the words that appear in the corpus as keys and no. of docs that contain this word as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c6d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordDocFre(dic_list):\n",
    "    df_dict ={}\n",
    "    for dic in dic_list:\n",
    "        document = list(dic.keys())[0]\n",
    "        for k,v in dic[document].items():\n",
    "            if (k in df_dict.keys()) and (v!=0):\n",
    "                df_dict[k]+=1\n",
    "            elif(v!=0):\n",
    "                df_dict[k] = 1\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c617ae",
   "metadata": {},
   "source": [
    "### **output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "087dd92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-4: 15.979388236999512\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus is printed\n",
    "\n",
    "df_dict = []\n",
    "start = time.time()\n",
    "df_dict = wordDocFre(dic_list)\n",
    "end = time.time()\n",
    "print(\"Collective Execution Time for TASK-4: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5789bd5",
   "metadata": {},
   "source": [
    "### **output (code Output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cedc11b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'association': 17065, 'computational': 19777, 'linguistics': 19565, '6': 17339, 'th': 2442, 'applied': 12900, 'natural': 18031, 'language': 20770, 'processing': 17433, 'conference': 16460}\n"
     ]
    }
   ],
   "source": [
    "#printing first 10 key: value pairs\n",
    "df_dict_10 = {k: df_dict[k] for k in list(df_dict.keys())[:10]}\n",
    "print(df_dict_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28240a",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>Task 5</font>\n",
    "Construct a function named **inverseDocFre(dicList,base)** that takes dictionary returned from wordDocFre functions above and outputs inverse document frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7381a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocFre(df_dict,base,M=5):\n",
    "    idf_dict = {k:np.log((M+1)/v)/np.log(base) for k,v in df_dict.items()}\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6fec7",
   "metadata": {},
   "source": [
    "### **output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc88d933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-5: 4.121631860733032\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus is printed\n",
    "\n",
    "idf_dict = []\n",
    "start = time.time()\n",
    "idf_dict= inverseDocFre(df_dict,base=2,M= len(docs_name_list))\n",
    "end = time.time()\n",
    "print(\"Collective Execution Time for TASK-5: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82942615",
   "metadata": {},
   "source": [
    "### **output (code Output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11104f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'association': 0.3626546180762967, 'computational': 0.14987143419019816, 'linguistics': 0.16541992227436778, '6': 0.3396743367585574, 'th': 3.1675599270751214, 'applied': 0.7663239668645456, 'natural': 0.28321562159782954, 'language': 0.0791938163305143, 'processing': 0.33187417201695835, 'conference': 0.41473069675353635}\n"
     ]
    }
   ],
   "source": [
    "#printing first 10 key: value pairs\n",
    "idf_dict_10 = {k: idf_dict[k] for k in list(idf_dict.keys())[:10]}\n",
    "print(idf_dict_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3adeb3",
   "metadata": {},
   "source": [
    "# <font color = 'blue'> Task 6</font>\n",
    "This function named **tfidf(docList)** takes list of documents it calls the function wordList to split the document in list of words and remove stopwords and punctuation marks from them, then calls termFrequencyInDoc() uses its output to create dictionary of vocabulary using the function wordDocFre(), it then should call inverseDocFre() function. It then outputs a list of dictionary,where each document corresponds to the dictionary, its words should be keys values should be tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe503e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(docList):\n",
    "    tfidf_doc_dict = {}\n",
    "    tf_doc_dict =[{doc:termFrequencyInDoc(removePuncs(wordList(os.path.join(path,doc))))} for doc in docList]\n",
    "    df_dict = wordDocFre(tf_doc_dict)\n",
    "    idf_dict = inverseDocFre(df_dict,2, len(docList))\n",
    "    \n",
    "    #print(tf_doc_dict)\n",
    "\n",
    "    tfidf_doc_dict = [{fname:{}} for fname in docs_name_list]\n",
    "    \n",
    "    #tfidf_doc_dict = list(tfidf_doc_dict)\n",
    "    #print(tfidf_doc_dict)\n",
    "    \n",
    "    for i, dic in enumerate(tf_doc_dict):\n",
    "        #print(dic)\n",
    "        document = list(dic.keys())[0]\n",
    "        #print(document)\n",
    "        for k,v in dic[document].items():\n",
    "            tfidf_doc_dict[i][document][k] = v*idf_dict[k]\n",
    "    return (tfidf_doc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bd587",
   "metadata": {},
   "source": [
    "### **output (Execution Time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f59224e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Execution Time for TASK-6: 416.93659710884094\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# Collective execution time for whole corpus is printed\n",
    "start = time.time()\n",
    "tfidf = tfidf(docs_name_list)\n",
    "end = time.time()\n",
    "print(\"Collective Execution Time for TASK-6: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8932b7c",
   "metadata": {},
   "source": [
    "### **output (code Output)**\n",
    "Printing output for the whole corpus dosent make sense. So printing output for the first file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9db5f882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A00-1000.pdf.txt': {'association': 0.3626546180762967, 'computational': 0.14987143419019816, 'linguistics': 0.16541992227436778, '6': 0.3396743367585574, 'th': 3.1675599270751214, 'applied': 1.5326479337290912, 'natural': 1.4160781079891476, 'language': 0.47516289798308575, 'processing': 1.6593708600847918, 'conference': 2.0736534837676817, 'proceedings': 0.18786262797763295, 'april': 2.8317562655477393, '29may': 12.421407412062523, '4': 0.12126938419511618, '2000': 1.2742024871202955, 'seattle': 3.822424441026434, 'washington': 3.2552443294164113, 'usa': 1.222655446868788, 'anlp': 23.56010380618485, '2000preface': 13.421407412062525, '131': 4.3783801284679775, 'papers': 11.258657875639726, 'submitted': 9.244672227533698, 'anlp2000': 17.49796414018206, '46': 2.856781549267152, 'accepted': 3.089930590202905, 'presentation': 3.144701392170094, 'came': 7.68606944140344, '24': 1.7246571755403566, 'countries': 4.320745073057326, 'fifty': 6.261536075284135, 'eight': 6.52307215056827, 'united': 7.0425906981701365, 'states': 2.180020048063588, 'america': 4.612443237143263, 'eleven': 5.724439885828238, 'germany': 2.8076179475899443, 'kingdom': 5.197405737864419, 'nine': 3.8487071855752326, 'canada': 2.299226510802772, 'japan': 2.7218349587752555, 'four': 2.182101390209163, 'italy': 3.2546166613443446, 'spain': 2.980019382715837, 'three': 0.4093066959109553, 'ach': 6.099479317175162, 'france': 2.7166391726999257, 'korea': 3.6310589153106103, 'switzerland': 4.312882955284356, 'two': 0.13225305833916853, 'australia': 3.504781489851289, 'china': 2.839265430403747, 'the': 0.19391692743804068, 'netherlands': 3.765876688906882, 'sweden': 3.0496307677246004, 'one': 0.12198472288534133, 'czech': 2.9416271480334246, 'republic': 3.035545011421063, 'denmark': 5.044196881673972, 'finland': 5.401816683704644, 'greece': 4.004609884456465, 'india': 4.06056032788286, 'hong': 3.68585138815099, 'kong': 3.8783755918072864, 'malaysia': 7.653223087285598, 'norway': 5.957883038791344, 'russia': 6.427053975203666, 'taiwan': 4.668190662883569, '40': 1.7696829789544604, 'industry': 4.88807767975669, '85': 2.9640265329899895, 'academia': 5.832692776480261, '2': 0.08281029248798154, 'government': 3.4723102563333166, 'organizations': 4.622125790540602, 'submissions': 4.303764310673433, 'combined': 1.5647873583856793, 'reviewing': 10.140936461032188, 'process': 1.1600792603603687, 'supported': 2.8509272885951447, 'webbased': 4.295993941579209, 'reviewer': 5.58220362396558, 'interface': 2.5810228613508586, 'developed': 1.2924464612068272, 'elisha': 10.614052490004921, 'kane': 9.563426416934952, 'new': 0.2866602433935087, 'mexico': 4.598040172016289, 'state': 1.220355597759253, 'universitys': 9.836444911341369, 'computing': 1.5282955223512171, 'research': 0.31803763467000323, 'lab': 3.845868165227994, 'linda': 6.382488422770223, 'fresques': 13.421407412062525, 'crl': 8.211954046433576, 'coordinated': 4.907679816110087, 'refereeing': 7.76319592931073, 'i': 0.53590168227027, 'would': 0.4282921123833469, 'like': 0.5721978114932601, 'express': 2.4316581366003915, 'gratitude': 6.7920507919829145, 'appreciation': 6.9867791844258, 'fthe': 8.961975793425228, 'program': 5.282228803803737, 'committee': 10.997055775260728, 'members': 2.83503671694856, 'responsible': 4.049630767724601, 'six': 2.4018166837046437, 'areas': 2.9047224627529147, 'lynn': 5.828950374794444, 'carlson': 5.415782862868647, 'tools': 1.7780021349468507, 'resources': 1.2446109339149256, 'developing': 2.491039664623573, 'nlp': 4.353904564351032, 'systems': 3.605378939595138, 'subcommittee': 58.3258061635286, 'eduard': 4.048542351949937, 'hovy': 3.4650312548128546, 'integrated': 2.5288645954139723, 'richard': 6.42974514187586, 'kittredge': 12.969538946119908, 'multilingual': 7.316310835179303, 'text': 1.939922052625371, 'ray': 5.197405737864419, 'perrault': 6.484769473059954, 'spoken': 4.382173392802823, 'oliviero': 7.366124976561334, 'stock': 4.630244523507507, 'monolingual': 3.2113449288222933, 'john': 3.9269037158429168, 'white': 3.6690267655096305, 'evaluation': 1.0928521288175, 'following': 0.39982011393802236, 'colleagues': 4.610835777321378, 'doug': 5.401816683704644, 'appelt': 5.547963299547148, 'fabio': 5.7775512222878, 'ciravegna': 8.0815574091779, 'robert': 8.68680542037128, 'dale': 4.164019569369873, 'michael': 4.776837950081946, 'elhadad': 4.876442979273287, 'ralph': 15.571813205190942, 'grishman': 8.762235382073616, 'lynette': 5.855353373891433, 'hirschman': 5.197405737864419, 'yuval': 6.103994798297655, 'krymolowski': 7.7071618943964015, 'inderjeet': 6.216836267813321, 'mani': 5.152280732913107, 'zvi': 10.099479317175163, 'marx': 7.490670074499639, 'martha': 3.6408676445905637, 'palmer': 3.3811176910368075, 'harold': 5.974324185852872, 'somers': 6.01626594892618, 'toshiyuki': 7.9136127718638285, 'takezawa': 8.0463679807156, 'takehito': 7.653223087285598, 'utsuro': 7.302466339339017, 'dekai': 4.945673981096126, 'wu': 3.0333901267173897, 'bulk': 5.700308223355339, 'igor': 5.859164987841452, 'boguslavsky': 8.312882955284357, 'jim': 11.047123912114026, 'cowie': 7.421407412062524, 'dowding': 7.126786663170898, 'glass': 5.368839361258371, 'jan': 9.284933774820665, 'haji': 15.38697391499865, 'pierre': 4.396267849784016, 'isabeiie': 14.421407412062525, 'alberto': 5.893930406002128, 'lavelli': 7.154620871367623, 'daniel': 2.053992660815697, 'marcu': 3.405992359675837, 'david': 10.563555795628002, 'mcdonald': 3.7401690002847197, 'owen': 9.359880851322755, 'rainbow': 7.76319592931073, 'tomek': 13.455840909126398, 'strzalkowski': 12.309241742735246, 'kathryn': 7.626991545712419, 'b': 0.5422070941151319, 'taylor': 4.323375329101998, 'pick': 3.752522427796278, 'vossen': 5.836444911341369, 'r6mi': 10.514516816454007, 'zajac': 7.9136127718638285, 'carter': 6.307665246013337, 'ido': 4.899806972338798, 'dagan': 4.477427497718787, 'andreas': 3.7454503791207756, 'eiscle': 14.421407412062525, 'oren': 4.577486360773491, 'glikman': 13.421407412062525, 'donna': 6.0815574091779, 'harman': 6.473040180477847, 'tanya': 15.359880851322755, 'korelsky': 15.584101583965829, 'chinyew': 5.415782862868647, 'lin': 1.2519521143654937, 'paul': 5.077528725401367, 'martin': 5.055453347343233, 'teruko': 7.231582853182507, 'mitamura': 6.897845456005512, 'norbert': 13.373395583673373, 'reithinger': 7.3233753291019985, 'beth': 10.198958634350324, 'sundheim': 6.271660292557843, 'hans': 4.510015424219065, 'uszkoreit': 5.276749169230642, 'weischedel': 8.691856525149015, 'we': 0.18016903293911546, 'believe': 2.0331209922998776, 'quality': 1.3957487391190253, 'elected': 6.410180156639271, 'israther': 14.421407412062525, 'high': 0.8054330039036621, 'hope': 2.8252176559181144, 'success': 2.6020266211979246, 'sergei': 12.636239207301005, 'nirenburg': 12.118927276654565, 'chair': 9.293240704922702, 'anlpi': 26.84281482412505, 'table': 0.41666025451576366, 'contents': 3.1534503276596855, 'section': 0.4544514136240033, '1': 0.11783121203292328, 'preface': 5.566539028802288, 'list': 1.0494946708384196, 'reviewers': 2.516396549672229, 'bustuca': 14.421407412062525, 'bus': 5.520540604081776, 'route': 5.026944717452207, 'oracle': 4.424227931124903, 'tore': 8.012016475924824, 'amble': 11.251482410620213, 'machine': 0.622530643929225, 'translation': 3.8211764528761476, 'very': 3.419999217669716, 'close': 1.9474480463677635, 'languages': 1.1957480737712747, 'hric': 10.333944570812186, 'vladislav': 8.231582853182507, 'kubofi': 11.421407412062525, '7': 0.5431648681932545, 'crosslanguage': 9.565942996144106, 'multimedia': 4.868738314548253, 'information': 1.2622262324116802, 'retrieval': 5.407001295734327, 'sharon': 4.801187586555038, 'flank': 9.614052490004921, '13': 1.3414227050285505, 'automatic': 2.5564354174392996, 'construction': 1.8565434218355825, 'parallel': 1.9210634427921212, 'englishchinese': 5.9255523851753535, 'corpus': 1.0634266653608768, 'jiang': 4.39212018509428, 'chen': 2.4991946942770764, 'jianyun': 7.302466339339017, 'nie': 5.886132035441721, '21': 1.021461974914578, 'partsld': 13.421407412062525, 'a': 0.8068185330776044, 'dialoguebased': 8.292124395117558, 'system': 2.585686380282525, 'identifying': 1.8233549119009247, 'parts': 1.6066246970004148, 'medical': 3.58377947889112, 'amit': 10.987258899960365, 'bagga': 6.76319592931073, 'g': 1.9442595522320865, 'bowden': 8.866818560384887, 'wise': 4.072679257831447, '29': 2.5017991734592706, 'using': 0.6558626137218203, 'dialogue': 16.297242203996454, 'participants': 2.727049524841073, 'setsuo': 9.666519909899057, 'yamada': 5.042029044991262, 'eiichiro': 5.961975793425227, 'sumita': 5.572784471633186, 'hideki': 5.623745886208765, 'kashioka': 8.538764362700684, '37': 2.7739489856076043, 'distilling': 8.866818560384887, 'dialoguesa': 14.421407412062525, 'method': 0.6280073135787054, 'corpora': 1.096804921972809, 'development': 1.0065900673736743, 'arne': 6.432722725290359, 'j6nsson': 11.421407412062525, 'nils': 6.843978584026776, 'dahlbick': 10.614052490004921, '44': 2.225420114034016, 'planbased': 6.756071494877348, 'management': 5.224886474286526, 'ina': 5.941627148033425, 'physics': 4.7384128283808415, 'tour': 6.355318221604752, 'reva': 9.37701329270407, 'freedman': 7.851551803731578, '52': 1.801187586555038, 'framework': 1.171849483337818, 'mt': 2.5948589247716094, 'nlg': 4.8553533738914325, 'based': 0.6071705665872172, 'uniform': 3.1812123580825995, 'lexicostructural': 10.17347989861894, 'benoit': 6.39350141549264, 'lavoie': 8.029089989283763, 'rambow': 4.513014791288774, '60': 2.1546208713676234, 'talkntravel': 13.421407412062525, 'conversational': 3.6326890793720663, 'air': 4.336599024258163, 'travel': 4.5220504891394135, 'planning': 3.4995664749880344, 'stallard': 7.3233753291019985, '68': 3.2533621438384346, 'rees': 7.666519909899057, 'largescale': 5.4367380461522155, 'relation': 1.2898719433861794, 'event': 2.3624010168614133, 'extraction': 6.6996198698442555, 'chinatsu': 7.344591815011694, 'aone': 6.62050751214222, 'mila': 9.514516816454005, 'ramossantacruz': 10.720967693921432, '76': 3.3128829552843557, 'experiments': 1.5989400481972884, 'sentence': 1.250735606465509, 'boundary': 3.10682416778833, 'detection': 2.1096590970575653, 'mark': 4.626621282345395, 'stevenson': 9.863118903246454, 'gaizauskas': 10.699890099011801, '84': 3.3600362194027134, 'dp': 5.022663720124331, 'detector': 5.676573574562979, 'presuppositions': 6.9867791844258, 'survey': 3.076002165344729, 'questions': 4.077566770975216, 'katja': 6.086017057368599, 'wiemerhastings': 16.42390809286715, 'peter': 2.3392583707086527, 'sonya': 10.333944570812186, 'rajan': 19.126852833869904, 'art': 2.6449743796177914, 'graesser': 6.727920454563199, 'roger': 4.54642606440875, 'kreuz': 9.211954046433576, 'ashish': 6.0507200052553065, 'karnavat': 12.421407412062523, '90': 2.3303026536860245, 'anlpiii': 13.421407412062525, 'mimic': 5.999342645889712, 'an': 0.581512831562308, 'adaptive': 3.9345723904994743, 'mixed': 3.542590127448296, 'initiative': 4.492149003425551, 'queries': 2.93507515929386, 'jennifer': 4.554128672352863, 'chucarroll': 6.633504852671093, '97': 3.3580123307740153, 'javox': 13.421407412062525, 'toolkit': 2.529244102200559, 'building': 1.3207450730573258, 'speechenabled': 8.614052490004921, 'applications': 1.1129261674185031, 's': 0.5414413323981678, 'fulkerson': 10.251482410620213, 'alan': 3.808538914771484, 'w': 0.8957642629172743, 'biermann': 7.344591815011694, '105': 3.8252176559181144, 'compact': 3.8002712987878833, 'architecture': 2.3993864424161684, 'scripts': 4.137161662232854, 'metaoutputs': 11.61405249000492, 'manny': 7.514516816454005, 'rayner': 6.557221267408244, 'ann': 5.4306862894551085, 'hockey': 6.897845456005512, 'frankie': 11.099479317175163, 'james': 2.344257896472791, '112': 3.969166171631706, 'representation': 1.1383190590385226, 'complex': 1.2743615231168066, 'evolving': 5.707161894396402, 'data': 0.5043740471936029, 'dependencies': 2.490300579033183, 'generation': 1.5816131218599674, 'c': 1.3064296628659835, 'mellish': 5.496594908456744, 'r': 0.9983893885541525, 'evans': 10.097084703899874, 'l': 1.3634645326155959, 'cahill': 5.825217655918115, 'doran': 6.756071494877348, 'd': 1.0636167750681949, 'paiva': 7.261536075284135, 'm': 1.089972785118048, 'reape': 7.9619757934252275, 'scott': 3.618891046941301, 'n': 0.5301933061566955, 'tipper': 9.563426416934952, '119': 4.359361274342034, 'reviser': 11.251482410620213, 'transcheck': 11.099479317175163, 'jeanmarc': 8.7775512222878, 'jutras': 11.421407412062525, '127': 4.3770132927040715, 'unit': 2.473405281084854, 'completion': 4.403207233249299, 'computeraided': 6.312882955284356, 'typing': 4.997241123244425, 'philippe': 5.79569856899806, 'langlais': 6.659856179618045, 'george': 2.827549473204301, 'foster': 4.180616079900568, 'guy': 5.4242279311249035, 'lapalme': 6.6866977918366866, '135': 4.26658930301042, 'coreference': 3.41718694574433, 'resolution': 2.5514280791625628, 'sanda': 5.8066975679473165, 'harabagiu': 5.2291145975917575, 'steven': 6.924259812684044, 'j': 0.931151815653286, 'maiorano': 7.626991545712419, '142': 4.38523379950904, 'ranking': 2.388328605931905, 'suspected': 6.720967693921432, 'answers': 3.0371632898194463, 'predictive': 3.8851601963743967, 'annotation': 1.5235619560570128, 'dragomir': 5.24149832204759, 'radev': 4.717503838617861, 'prager': 6.9619757934252275, 'valerie': 7.666519909899057, 'samn': 10.836444911341369, '150': 3.3695187561571474, 'message': 3.3756477506798417, 'classification': 1.3405898844541972, 'call': 1.7307546127140419, 'center': 2.1319648363741925, 'stephan': 4.115345722634183, 'busemann': 7.866818560384886, 'sven': 6.653223087285598, 'schmeier': 9.7775512222878, 'roman': 10.332757684487591, 'arens': 8.231582853182507, '158': 4.696893558942574, 'question': 1.2834556773471657, 'answering': 2.780710574492431, 'rohini': 15.441935387842864, 'srihari': 14.463165706365015, 'wei': 6.908362306453062, 'li': 4.358252923520162, '166': 4.7261791205667745, 'categorizing': 6.382488422770223, 'unknown': 2.6602717622338785, 'words': 0.32132003194712555, 'decision': 1.9010342757228, 'trees': 1.827082808137677, 'identify': 1.3279898476745637, 'names': 2.0921716703287245, 'misspellings': 6.17347989861894, 'janine': 9.7775512222878, 'toole': 10.514516816454007, '173': 4.696893558942574, 'examining': 3.7596293142905375, 'role': 1.381802894108766, 'statistical': 1.8636205793464353, 'linguistic': 1.654165616275354, 'knowledge': 2.154133554295034, 'sources': 1.6044237888071435, 'generalknowledge': 11.836444911341369, 'questionanswering': 4.919570227160228, 'claire': 4.138319059038523, 'cardie': 4.510015424219065, 'vincent': 3.58377947889112, 'ng': 2.6577800603348964, 'pierce': 7.41018015663927, 'chris': 2.5665390288022882, 'buckley': 6.2414983220475895, '180': 4.1534503276596855, 'extracting': 2.1764490189113905, 'molecular': 5.7419273125570784, 'binding': 4.74016900028472, 'relationships': 2.596448671534002, 'biomedical': 3.973291106653061, 'thomas': 2.781614518783212, 'rindflesch': 7.7920507919829145, 'jayant': 7.9136127718638285, 'v': 1.0646803545978945, 'lawrence': 7.562324951680357, 'hunter': 5.803021909803918, '188': 4.752522427796277, 'compound': 3.4405538056827893, 'noun': 1.3948839695427588, 'segmentation': 2.646208683164539, 'lexical': 1.5328716160488107, 'extracted': 1.2532054058252555, 'juntae': 10.099479317175163, 'yoon': 6.727920454563199, '196': 4.834567624100698, 'corpusbased': 3.2034497141984106, 'lfg': 5.1663788422437955, 'specialization': 6.349945049505901, 'nicola': 4.3168086584981555, 'cancedda': 7.012016475924823, 'christer': 8.514516816454005, 'samuelsson': 7.653223087285598, '204': 4.766771383534557, 'tool': 1.9313086571782858, 'automated': 2.649917942561926, 'revision': 4.698599880892978, 'grammars': 2.253048684838638, 'nanda': 7.4327227252903585, 'kambhatla': 6.978463916213797, 'wlodek': 9.514516816454005, 'zadrozny': 8.251482410620213, '210': 4.61405249000492, 'anlpiv': 14.421407412062525, 'aggressive': 5.745450379120776, 'morphology': 3.02052797578034, 'robust': 2.0125469760377643, 'coverage': 2.105691754040323, 'william': 5.5959662443852265, 'woods': 11.56956558303775, '218': 4.8066975679473165, 'tnta': 12.421407412062523, 'partofspeech': 2.0240001676871455, 'tagger': 2.7003082233553397, 'thorsten': 4.375647750679842, 'brants': 4.750751162944084, '224': 4.788412214919567, 'independent': 1.7060888597311756, 'morphological': 2.3112503931084585, 'analysis': 0.46312658689350256, 'tatsuo': 9.720967693921432, 'yamashita': 8.514516816454005, 'yuji': 4.878375591807287, 'matsumoto': 4.48624236245883, '232': 4.513014791288774, 'divideandconquer': 8.720967693921432, 'strategy': 2.003027692697568, 'shallow': 3.016798014224813, 'parsing': 1.2028441758722535, 'german': 2.402511790940875, 'free': 2.1196254500114193, 'texts': 1.328320021618307, 'giinter': 10.720967693921432, 'neumann': 6.136005193200277, 'christian': 3.614857790076563, 'braun': 8.154620871367623, 'jakub': 6.563426416934953, 'piskorski': 8.192588721566644, '239': 5.0035548971766275, 'hybrid': 3.1522807329131064, 'approach': 0.3370972784647693, 'named': 7.998422206370607, 'entity': 5.798442234681346, 'subtype': 5.6799404256613775, 'tagging': 1.8632258922024971, 'cheng': 5.026944717452207, 'niu': 6.39350141549264, '247': 5.187787735302822, 'spelling': 3.475232171338966, 'grammar': 1.4884545198678099, 'correction': 3.5010545566474436, 'danish': 5.756071494877348, 'scarrie': 12.099479317175163, 'patrizia': 9.029089989283765, 'paggio': 9.514516816454005, '255': 5.02052797578034, 'can': 2.2748387363217386, 'improve': 1.0140068579938792, 'bookman': 10.099479317175163, 'houston': 7.514516816454005, 'kuhns': 9.211954046433576, 'stephen': 2.7993555926061484, 'green': 3.9867791844258003, '262': 5.183002672737445, 'domainspecific': 3.5782715009511747, 'acquisition': 2.4267004168044237, 'dan': 2.123631349168378, 'moldovan': 5.374283499948499, 'roxana': 6.377013292704071, 'girju': 5.995142657360426, 'vasile': 7.478892906723285, 'rus': 5.623745886208765, '268': 5.152280732913107, 'controlled': 3.6573651947559376, 'vocabulary': 2.2244979695213876, 'indexing': 3.575133500712618, 'entities': 2.023198150595673, 'wasson': 10.099479317175163, '276': 5.294702939219334, 'unsupervised': 2.1383190590385226, 'discovery': 2.839265430403747, 'scenariolevel': 10.961975793425228, 'patterns': 1.6924240420744758, 'yangarber': 7.202238891600364, 'pasi': 7.377013292704071, 'tapanainen': 6.882248600954493, 'silja': 7.851551803731578, 'huttunen': 8.897845456005513, '282': 5.183002672737445, 'corpusderived': 7.626991545712419, 'name': 1.7982117766095889, 'lists': 1.9782048347655128, 'recognition': 1.4453803718726896, '290': 5.320745073057326, 'answer': 2.2396339732537167, 'abney': 5.14994438415815, 'collins': 3.0567255561979647, 'singhal': 7.55104269247912, '296': 5.368839361258371, 'automatically': 1.1237746229200112, 'identified': 1.7011631537749587, 'index': 2.612443237143263, 'terms': 0.8049734371112472, 'browsing': 5.505528033226751, 'electronic': 3.0715733206052773, 'documents': 1.806927607817195, 'nina': 6.532664163164266, 'wacholder': 7.748982070091029, 'judith': 5.279300354759974, 'klavans': 5.773948985607604, 'k': 0.7588502912233406, '302': 5.194995219273739, 'reduction': 2.7093103609749787, 'summarization': 3.0328519080799636, 'hongyan': 6.39350141549264, 'jing': 3.7454503791207756, '310': 5.074893678896889, 'noisy': 2.8944192066305177, 'input': 0.9562236680681242, 'speech': 0.9918699028130465, 'ocr': 6.557221267408244, 'miller': 3.427761352045928, 'sean': 6.168741979612276, 'boisen': 9.614052490004921, 'schwartz': 4.057272757054473, 'rebecca': 4.1925887215666435, 'stone': 4.797525922049066, '316': 5.312882955284356, 'improving': 2.008837565257315, 'testsuites': 10.029089989283765, 'via': 1.373601858054935, 'instrumentation': 9.211954046433576, 'br6ker': 11.251482410620213, '325': 5.046367980715599, 'efficiency': 2.9512569767881653, 'multimodal': 4.131388565129907, 'interaction': 2.3322880141473568, 'mapbased': 9.614052490004921, 'task': 0.48697937143572056, 'philip': 3.3620629512380997, 'cohen': 3.614857790076563, 'mcgee': 8.099479317175161, 'josh': 6.099479317175162, 'clow': 10.720967693921432, '331': 4.635137784414059, 'anlpv': 14.421407412062525}}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2d562",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>Task 7</font>\n",
    "Write a code for VSM and run the following queries, you must show the top 5 documents ranked according to the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d26ef",
   "metadata": {},
   "source": [
    "### **Creating query list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88f1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'LDA'\n",
    "q2 = 'Topic modelling'\n",
    "q3 = 'Generative models'\n",
    "q4 = 'Semantic relationships between terms'\n",
    "q5 = 'Natural Language Processing'\n",
    "q6 = 'Text Mining'\n",
    "q7 = 'Translation model'\n",
    "q8 = 'Learning procedures for the lexicon'\n",
    "q9 = 'Semantic evaluations'\n",
    "q10 = 'System results and combination'\n",
    "\n",
    "query_list = [q1,q2,q3,q4,q5,q6,q7,q8,q9,q10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca59e8c",
   "metadata": {},
   "source": [
    "### **Vector Space Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b937395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorSpaceModel(query):\n",
    "    refined_query = termFrequencyInDoc(removePuncs([i for i in query.split()]))\n",
    "    \n",
    "                                       \n",
    "    relevance_scores = {} # a dictionary that will store the relevance score for each doc\n",
    "\n",
    "    # doc_id will be the key and relevance score the value for this dictionary\n",
    "    for dic in tfidf:\n",
    "        document= list(dic.keys())[0]\n",
    "        score=0\n",
    "        for k,v in refined_query.items():\n",
    "            if k in dic[document].keys():\n",
    "                score+= v*dic[document][k]\n",
    "            else: \n",
    "                score += 0\n",
    "        relevance_scores[document]= score\n",
    "    return relevance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1a4d8",
   "metadata": {},
   "source": [
    "### Qurery 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5845e465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.06087374687194824\n",
      "Top 5 retrived documents\n",
      "[('J14-2003.pdf.txt', 381.27005902102184), ('D09-1026.pdf.txt', 351.9415929424817), ('D11-1050.pdf.txt', 342.1654375829683), ('N10-1070.pdf.txt', 312.83697150442816), ('P10-1117.pdf.txt', 298.1727384651581)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q1)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dcb86",
   "metadata": {},
   "source": [
    "### Qurery 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4e1576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.13167047500610352\n",
      "Top 5 retrived documents\n",
      "[('J14-2003.pdf.txt', 379.22781112486547), ('P12-1079.pdf.txt', 338.5301923700018), ('Q15-1004.pdf.txt', 303.3822488998924), ('N15-1074.pdf.txt', 301.53235713830765), ('W10-4104.pdf.txt', 281.18354776087585)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q2)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274338c",
   "metadata": {},
   "source": [
    "### Qurery 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af3ebbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.11243247985839844\n",
      "Top 5 retrived documents\n",
      "[('W06-1668.pdf.txt', 187.87886349014082), ('W11-0100.pdf.txt', 176.69366973180556), ('J03-4003.pdf.txt', 150.26038946212753), ('D09-1111.pdf.txt', 133.05540026404435), ('D09-1058.pdf.txt', 132.4047558949034)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q3)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c85ff",
   "metadata": {},
   "source": [
    "### Qurery 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf9276e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorSpaceModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_39112/1497613350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorSpaceModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msorted_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time for document retrival: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorSpaceModel' is not defined"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q4)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30033cbf",
   "metadata": {},
   "source": [
    "### Qurery 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a361e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.14489054679870605\n",
      "Top 5 retrived documents\n",
      "[('W11-0100.pdf.txt', 154.1065359645849), ('J14-1005.pdf.txt', 89.59718548138927), ('J87-1020.pdf.txt', 86.19050944918523), ('W14-55.x.pdf.txt', 61.88699873009019), ('J86-2001.pdf.txt', 55.8927425043754)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q5)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d7cea",
   "metadata": {},
   "source": [
    "### Qurery 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d567c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.12449812889099121\n",
      "Top 5 retrived documents\n",
      "[('D09-1162.pdf.txt', 163.2383315006005), ('P06-1062.pdf.txt', 162.88065703010744), ('P12-1062.pdf.txt', 123.66089922209758), ('W09-2609.pdf.txt', 120.34787676261845), ('P09-1098.pdf.txt', 111.27268562038182)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q6)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469f0a5",
   "metadata": {},
   "source": [
    "### Qurery 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "774a0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.10837364196777344\n",
      "Top 5 retrived documents\n",
      "[('J85-2006.pdf.txt', 448.7720868713976), ('J03-3003.pdf.txt', 418.0406072933456), ('J06-4004.pdf.txt', 336.1709175930765), ('W14-3302.pdf.txt', 328.1599415579916), ('J03-3004.pdf.txt', 326.07082990813876)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q7)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13059d",
   "metadata": {},
   "source": [
    "### Qurery 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e871cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.15552377700805664\n",
      "Top 5 retrived documents\n",
      "[('W11-0100.pdf.txt', 238.18865159013257), ('J87-3007.pdf.txt', 212.77547878914234), ('W06-1647.pdf.txt', 200.10600795236127), ('W10-2505.pdf.txt', 195.67948216102045), ('J87-3009.pdf.txt', 180.18493253003498)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q8)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40915736",
   "metadata": {},
   "source": [
    "### Qurery 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aab9a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.11362266540527344\n",
      "Top 5 retrived documents\n",
      "[('W11-0100.pdf.txt', 867.1454637828948), ('J08-2004.pdf.txt', 199.42036732788452), ('J09-4008.pdf.txt', 194.3027742065038), ('J09-2003.pdf.txt', 160.8463546695711), ('J91-1003.pdf.txt', 124.45577669003012)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q9)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bca4a4",
   "metadata": {},
   "source": [
    "### Qurery 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cb11be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for document retrival: 0.17290925979614258\n",
      "Top 5 retrived documents\n",
      "[('W11-0100.pdf.txt', 215.44217282417776), ('J01-2002.pdf.txt', 154.23594278357413), ('N10-1141.pdf.txt', 152.32584973748556), ('P11-1127.pdf.txt', 129.21511739642608), ('J11-3003.pdf.txt', 123.72142094227992)]\n"
     ]
    }
   ],
   "source": [
    "start  = time.time()\n",
    "result = vectorSpaceModel(q10)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for document retrival: {}\".format(end-start))\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d6f5c",
   "metadata": {},
   "source": [
    "# <font color='red'>Part 3: Complete Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfa8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files\n",
      "Done!\n",
      "Time for TF-IDF calculations: 404.17116713523865\n",
      "Time for ranking documents: 0.04665994644165039\n",
      "Top 5 retrived documents\n",
      "[('J14-2003.pdf.txt', 381.27005902102184), ('D09-1026.pdf.txt', 351.9415929424817), ('D11-1050.pdf.txt', 342.1654375829683), ('N10-1070.pdf.txt', 312.83697150442816), ('P10-1117.pdf.txt', 298.1727384651581)]\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "\n",
    "# specify zip file name \n",
    "file_name = \"ACL txt.zip\"\n",
    "\n",
    "# specify query\n",
    "query = \"LDA\"\n",
    "  \n",
    "\n",
    "punctuations =  '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "              'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "              'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "              'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "              'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "              'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "              'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should',\n",
    "              \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n",
    "              'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    #zip.printdir()\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files')\n",
    "    zip.extractall()\n",
    "    print('Done!')\n",
    "\n",
    "# make list of all documnets \n",
    "path = \"./\"+file_name[:-4]\n",
    "docs_name_list = os.listdir(path)\n",
    "\n",
    "# calculate tfidf for each document \n",
    "start = time.time()\n",
    "tfidf = tfidf(docs_name_list)\n",
    "end = time.time()\n",
    "print(\"Time for TF-IDF calculations: {}\".format(end-start))\n",
    "\n",
    "# calculate document ranking \n",
    "start = time.time()\n",
    "result = vectorSpaceModel(query)\n",
    "sorted_result = sorted(result.items(), key=lambda item: item[1],reverse=True)\n",
    "end = time.time()\n",
    "print(\"Time for ranking documents: {}\".format(end-start))\n",
    "\n",
    "print(\"Top 5 retrived documents\")\n",
    "print(list(sorted_result)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc4ea7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
