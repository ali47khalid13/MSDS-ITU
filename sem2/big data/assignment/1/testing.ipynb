{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3247f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libaries\n",
    "import psycopg2\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# create an Empty DataFrame object\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc87e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "6820\n"
     ]
    }
   ],
   "source": [
    "def process_youtubedata_file(cur, conn, filepath,df1): \n",
    "    \"\"\"\n",
    "        This function reads one JSON file and read information of videos and youtuber data and saves into video_data and youtuber_data\n",
    "        Arguments:\n",
    "        cur: Database Cursor\n",
    "        filepath: location of JSON files\n",
    "        Return: None\n",
    "    \"\"\"\n",
    "    # open JSON file\n",
    "    #print(filepath)\n",
    "   # df1 = pd.read_json(filepath)\n",
    "    #df.fillna('')\n",
    "    \n",
    "    # ---------insert youtuber record----------\n",
    "    # write your code here that reads youtuber data from JSON file and insert it into Youtubers_dim table \n",
    "\n",
    "    # write your code here\n",
    "    # youtuber_data = df1[['youtuber_id', 'youtuber_name', 'youtuber_location', 'youtuber_latitude', 'youtuber_longitude']]\n",
    "    #youtuber_data = youtuber_data.drop_duplicates()\n",
    "    # df2 = df.append(youtuber_data)\n",
    "    #df2 = df2.drop_duplicates()\n",
    "    \n",
    "#     video_data = df1[['video_id', 'title', 'youtuber_id', 'year', 'duration']]\n",
    "#     df2 = df.append(video_data)\n",
    "#     df2 = df2.drop_duplicates()\n",
    "#     video_data = video_data.drop_duplicates(keep='first')\n",
    "#     video_data = video_data.values.tolist()[0]\n",
    "    \n",
    "#     try: \n",
    "#         cur.execute(Videos_table_insert, video_data)\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(e)\n",
    "\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "    # filter by NextVideo action\n",
    "    df = df[(df['page'] == 'NextVideo')]\n",
    "\n",
    "    # convert timestamp column to datetime with 'pd.to_datetime(df[\"ts\"], unit=\"ms\")' \n",
    "    df['ts'] = pd.to_datetime(df[\"ts\"], unit=\"ms\")\n",
    "    # insert time data records to Time_dim table\n",
    "    \n",
    "        # write your code here\n",
    "    df['year'] =  pd.to_datetime(df['ts']).dt.year\n",
    "    df['month'] =  pd.to_datetime(df['ts']).dt.month\n",
    "    df['day'] =  pd.to_datetime(df['ts']).dt.day\n",
    "    df['hour'] =  pd.to_datetime(df['ts']).dt.hour\n",
    "    df['week'] =  pd.to_datetime(df['ts']).dt.week\n",
    "    df['weekday'] =  pd.to_datetime(df['ts']).dt.weekday\n",
    "    df['start_time'] =  pd.to_datetime(df['ts'])\n",
    "    \n",
    "    user_data = df[['userId', 'firstName', 'lastName', 'gender', 'location']]\n",
    "    df2 = df1.append(user_data)\n",
    "    #df2 = df2.drop_duplicates()\n",
    "\n",
    "    return df2\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles \n",
    "\n",
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"\n",
    "        This function get all JSON files in given directory by exploring all sub directories, and process all files that were found using the given function.\n",
    "        Example: if I give it the path to youtube_data directory which resides in data folder of this assignment,\n",
    "        and func given is process_youtubedata_file it should get all JSON files in this directories and process each file using process_youtubedata_file function. \n",
    "        Arguments:\n",
    "        cur: Database Cursor\n",
    "        conn: Database\n",
    "        filepath: location of JSON files\n",
    "        func: function to process all files in the directory\n",
    "        Return: None\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    file_list = getListOfFiles(filepath)\n",
    "    print(len(file_list))\n",
    "    for file in file_list:\n",
    "        df = func(cur,conn,file,df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 dbname=youtubedb user=postgres password=ali123ali\")\n",
    "    cur = conn.cursor()\n",
    "    conn.set_session(autocommit=True)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #df3 = process_data(cur, conn, filepath='data/youtube_data', func=process_youtubedata_file)\n",
    "    df3 = process_data(cur, conn, filepath='data/log_data', func=process_youtubedata_file)\n",
    "\n",
    "    conn.close()\n",
    "    #print(len(df3))\n",
    "    return df3\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad0db68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>Summers</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Sylvie</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>F</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>Smith</td>\n",
       "      <td>M</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>Smith</td>\n",
       "      <td>M</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>New Orleans-Metairie, LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>New Orleans-Metairie, LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>Stefany</td>\n",
       "      <td>White</td>\n",
       "      <td>F</td>\n",
       "      <td>Lubbock, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>Stefany</td>\n",
       "      <td>White</td>\n",
       "      <td>F</td>\n",
       "      <td>Lubbock, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Arellano</td>\n",
       "      <td>M</td>\n",
       "      <td>Harrisburg-Carlisle, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48</td>\n",
       "      <td>Marina</td>\n",
       "      <td>Sutton</td>\n",
       "      <td>F</td>\n",
       "      <td>Salinas, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>86</td>\n",
       "      <td>Aiden</td>\n",
       "      <td>Hess</td>\n",
       "      <td>M</td>\n",
       "      <td>La Crosse-Onalaska, WI-MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>Makinley</td>\n",
       "      <td>Jones</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>66</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Arellano</td>\n",
       "      <td>M</td>\n",
       "      <td>Harrisburg-Carlisle, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>89</td>\n",
       "      <td>Kynnedi</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>F</td>\n",
       "      <td>Cedar Rapids, IA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Koch</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId firstName  lastName gender  \\\n",
       "2       8    Kaylee   Summers      F   \n",
       "4       8    Kaylee   Summers      F   \n",
       "5       8    Kaylee   Summers      F   \n",
       "6       8    Kaylee   Summers      F   \n",
       "7       8    Kaylee   Summers      F   \n",
       "8       8    Kaylee   Summers      F   \n",
       "9       8    Kaylee   Summers      F   \n",
       "10     10    Sylvie      Cruz      F   \n",
       "12     26      Ryan     Smith      M   \n",
       "13     26      Ryan     Smith      M   \n",
       "14    101    Jayden       Fox      M   \n",
       "0     101    Jayden       Fox      M   \n",
       "2      83   Stefany     White      F   \n",
       "3      83   Stefany     White      F   \n",
       "4      66     Kevin  Arellano      M   \n",
       "5      48    Marina    Sutton      F   \n",
       "7      86     Aiden      Hess      M   \n",
       "8      17  Makinley     Jones      F   \n",
       "9      66     Kevin  Arellano      M   \n",
       "10     15      Lily      Koch      F   \n",
       "11     15      Lily      Koch      F   \n",
       "13     15      Lily      Koch      F   \n",
       "14     15      Lily      Koch      F   \n",
       "15     89   Kynnedi   Sanchez      F   \n",
       "16     15      Lily      Koch      F   \n",
       "17     15      Lily      Koch      F   \n",
       "18     15      Lily      Koch      F   \n",
       "19     15      Lily      Koch      F   \n",
       "20     15      Lily      Koch      F   \n",
       "21     15      Lily      Koch      F   \n",
       "\n",
       "                                        location  \n",
       "2                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "4                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "5                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "6                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "7                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "8                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "9                    Phoenix-Mesa-Scottsdale, AZ  \n",
       "10  Washington-Arlington-Alexandria, DC-VA-MD-WV  \n",
       "12            San Jose-Sunnyvale-Santa Clara, CA  \n",
       "13            San Jose-Sunnyvale-Santa Clara, CA  \n",
       "14                      New Orleans-Metairie, LA  \n",
       "0                       New Orleans-Metairie, LA  \n",
       "2                                    Lubbock, TX  \n",
       "3                                    Lubbock, TX  \n",
       "4                        Harrisburg-Carlisle, PA  \n",
       "5                                    Salinas, CA  \n",
       "7                      La Crosse-Onalaska, WI-MN  \n",
       "8             Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "9                        Harrisburg-Carlisle, PA  \n",
       "10            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "11            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "13            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "14            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "15                              Cedar Rapids, IA  \n",
       "16            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "17            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "18            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "19            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "20            Chicago-Naperville-Elgin, IL-IN-WI  \n",
       "21            Chicago-Naperville-Elgin, IL-IN-WI  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.sort_values(by=['userId'], inplace=True)\n",
    "df.head(30)\n",
    "#df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa1ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 0.05823040008544922\n"
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "try: \n",
    "    t = time.time()\n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 user=postgres password=ali123ali\")\n",
    "    print('Time Taken:', time.time()-t)\n",
    "    \n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the Postgres database\")\n",
    "    print(e)\n",
    "    \n",
    "# get a cursor    \n",
    "try: \n",
    "    cur = conn.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get curser to the Database\")\n",
    "    print(e)\n",
    "    print('Time Taken:',time.time()-t)    \n",
    "    \n",
    "# set the autocommit to true    \n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe046f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database \"youtubedb\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    t = time.time()\n",
    "    cur.execute(\"create database youtubedb\")\n",
    "    print('Time Taken:',time.time()-t)\n",
    "    \n",
    "except psycopg2.Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c4dbb",
   "metadata": {},
   "source": [
    "### Create_Table_queries.py ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f19606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABLES\n",
    "# Write queries to drop each table, please don't change variable names,\n",
    "# You should write respective queries against each varibale\n",
    "\n",
    "Videoplay_table_drop = \"DROP TABLE IF EXISTS Videoplay_fact\"\n",
    "Users_table_drop = \"DROP TABLE IF EXISTS Users_dim\"\n",
    "Videos_table_drop = \"DROP TABLE IF EXISTS Videos_dim\"\n",
    "Youtubers_table_drop = \"DROP TABLE IF EXISTS Youtubers_dim\"\n",
    "Time_table_drop = \"DROP TABLE IF EXISTS Time_dim\"\n",
    "\n",
    "# CREATE TABLES\n",
    "# Write queries to create each table, please don't change variable names, you can refer to star schema table\n",
    "# You should write respective queries against each varibale\n",
    "\n",
    "Videoplay_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS Videoplay_fact (videoplay_id text, start_time TIME, \n",
    "                            user_id text, level VARCHAR, video_id text, \n",
    "                            youtuber_id text, session_id text, location text, user_agent text, \n",
    "                            PRIMARY KEY (videoplay_id), \n",
    "                            FOREIGN KEY(start_time) REFERENCES Time_dim(start_time),\n",
    "                            FOREIGN KEY(user_id) REFERENCES Users_dim(user_id), \n",
    "                            FOREIGN KEY(video_id) REFERENCES Videos_dim(video_id),\n",
    "                            FOREIGN KEY(youtuber_id) REFERENCES Youtubers_dim(youtuber_id));\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da214ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Users_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS Users_dim (user_id text, first_name text, \n",
    "                        last_name text, gender text, level VARCHAR, \n",
    "                        PRIMARY KEY (user_id));\"\"\")\n",
    "\n",
    "Videos_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS Videos_dim (Video_id text, title text, \n",
    "                        youtuber_id text, year text, duration VARCHAR, \n",
    "                        PRIMARY KEY (video_id),\n",
    "                        FOREIGN KEY(youtuber_id) REFERENCES Youtubers_dim(youtuber_id));\"\"\")\n",
    "\n",
    "\n",
    "Youtubers_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS Youtubers_dim (youtuber_id text, name text, \n",
    "                        location text, latitude VARCHAR, longitude VARCHAR, \n",
    "                        PRIMARY KEY (youtuber_id));\"\"\")\n",
    "\n",
    "Time_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS Time_dim (start_time TIME, hour int, \n",
    "                        day int, week int, month int, year int, weekday int, \n",
    "                        PRIMARY KEY (start_time));\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c265f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT RECORDS\n",
    "# Write queries to insert record to each table, please don't change variable names, you can refer to star schema table\n",
    "# You should write respective queries against each varibale\n",
    "\n",
    "Youtubers_table_insert = (\"\"\"INSERT INTO Youtubers_dim (youtuber_id, name, location, latitude, longitude)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\"\"\")\n",
    "\n",
    "Videos_table_insert = (\"\"\"INSERT INTO Videos_dim (Video_id, title, youtuber_id, year, duration)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\"\"\")\n",
    "\n",
    "Users_table_insert = (\"\"\"INSERT INTO Users_dim (user_id, first_name, last_name, gender, level)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\"\"\")\n",
    "\n",
    "Time_table_insert = (\"\"\"INSERT INTO Time_dim (start_time, hour, day, week, month, year, weekday)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s)\"\"\")\n",
    "\n",
    "Videoplay_table_insert = (\"\"\"INSERT INTO Youtubers_dim (youtuber_id, name, location, latitude, longitude )\n",
    "                        VALUES (%s, %s, %s, %s, %s)\"\"\")\n",
    "\n",
    "# youtuber_record = (\"\"\"INSERT INTO Youtubers_dim (youtuber_id, name, location, latitude, longitude)\n",
    "# SELECT youtuber_id, youtuber_name, youtuber_location, youtuber_latitude, youtuber_longitude FROM youtube_data\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [Users_table_create, Time_table_create, Youtubers_table_create, Videos_table_create, Videoplay_table_create]\n",
    "drop_table_queries = [Videoplay_table_drop, Users_table_drop, Videos_table_drop, Youtubers_table_drop, Time_table_drop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858917d",
   "metadata": {},
   "source": [
    "### Create Tables.py file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f701da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# from Create_Table_queries import create_table_queries, drop_table_queries\n",
    "\n",
    "\n",
    "def create_database():\n",
    "    '''Creates and connects to youtube  database. Returns cursor and connection to DB'''\n",
    "    \n",
    "    # connect to default database\n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 user=postgres password = ali123ali\")\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # create youtube database with UTF8 encoding\n",
    "    cur.execute(\"DROP DATABASE IF EXISTS youtubedb\")\n",
    "    cur.execute(\"CREATE DATABASE Youtubedb WITH ENCODING 'utf8' TEMPLATE template0\")\n",
    "\n",
    "    # close connection to default database\n",
    "    conn.close()    \n",
    "    \n",
    "    # connect to youtubedb database\n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 dbname=youtubedb user=postgres password=ali123ali\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    return cur, conn\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    '''Drops all tables created on the database'''\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    '''Created tables defined on the Create_Table_queries script'''\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Function to drop and re create youtube database and all related tables.\n",
    "        Usage: python Create_Table_queries.py\n",
    "    \"\"\"\n",
    "    cur, conn = create_database()\n",
    "    \n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "945feb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    duration             title  year         youtuber_id  youtuber_latitude  \\\n",
      "0  218.93179  I Didn't Mean To     0  ARD7TVE1187B99BFB1                NaN   \n",
      "\n",
      "  youtuber_location  youtuber_longitude youtuber_name  num_videos  \\\n",
      "0   California - LA                 NaN        Casual           1   \n",
      "\n",
      "             video_id  \n",
      "0  SOMZWCG12A8C13C480  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ARD7TVE1187B99BFB1', 'Casual', 'California - LA', nan, nan]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filepath = \"data\\youtube_data\\A\\A\\A\\TRAAAAW128F429D538.json\"\n",
    "df = pd.read_json(filepath, orient ='columns')\n",
    "youtuber_data = df[['youtuber_id', 'youtuber_name', 'youtuber_location', 'youtuber_latitude', 'youtuber_longitude']]\n",
    "\n",
    "youtuber_data.values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a1b667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\TEMP/ipykernel_25032/576111745.py:16: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['week'] =  pd.to_datetime(df['ts']).dt.week\n",
      "C:\\Windows\\TEMP/ipykernel_25032/576111745.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  videoplay_data['videoplay_id'] = list(map(str,list(range(0, len(videoplay_data)))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "youtuber                 object\n",
       "auth                     object\n",
       "firstName                object\n",
       "gender                   object\n",
       "itemInSession             int64\n",
       "lastName                 object\n",
       "length                  float64\n",
       "level                    object\n",
       "location                 object\n",
       "method                   object\n",
       "page                     object\n",
       "registration              int64\n",
       "sessionId                 int64\n",
       "video                    object\n",
       "status                    int64\n",
       "ts               datetime64[ns]\n",
       "userAgent                object\n",
       "userId                    int64\n",
       "year                      int64\n",
       "month                     int64\n",
       "day                       int64\n",
       "hour                      int64\n",
       "week                      int64\n",
       "weekday                   int64\n",
       "start_time               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"data\\log_data\\\\2018\\\\11\\\\2018-11-02-events.json\"\n",
    "df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "# filter by NextVideo action\n",
    "df = df[(df['page'] == 'NextVideo')]\n",
    "\n",
    "# convert timestamp column to datetime with 'pd.to_datetime(df[\"ts\"], unit=\"ms\")' \n",
    "df['ts'] = pd.to_datetime(df[\"ts\"], unit=\"ms\")\n",
    "# insert time data records to Time_dim table\n",
    "\n",
    "    # write your code here\n",
    "df['year'] =  pd.to_datetime(df['ts']).dt.year\n",
    "df['month'] =  pd.to_datetime(df['ts']).dt.month\n",
    "df['day'] =  pd.to_datetime(df['ts']).dt.day\n",
    "df['hour'] =  pd.to_datetime(df['ts']).dt.hour\n",
    "df['week'] =  pd.to_datetime(df['ts']).dt.week\n",
    "df['weekday'] =  pd.to_datetime(df['ts']).dt.weekday\n",
    "df['start_time'] =  pd.to_datetime(df['ts']).dt.time\n",
    "\n",
    "videoplay_data = df[['start_time', 'userId', 'level','video', 'youtuber', 'sessionId', 'location','userAgent']]\n",
    "videoplay_data['videoplay_id'] = list(map(str,list(range(0, len(videoplay_data)))))\n",
    "videoplay_data = videoplay_data.drop_duplicates(keep='first')\n",
    "videoplay_data = videoplay_data[['videoplay_id','start_time', 'userId', 'level','video', 'youtuber', 'sessionId', 'location','userAgent']]\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7fd449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:25:41.121934\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath = \"data\\log_data\\\\2018\\\\11\\\\2018-11-02-events.json\"\n",
    "#filepath = \"data\\youtube_data\\A\\A\\A\\TRAAAAW128F429D538.json\"\n",
    "df = pd.read_json(filepath, lines=True)\n",
    "df['start_time'] = pd.to_datetime(df['ts']).dt.time\n",
    "#df['start_time'] = df['start_time'].astype('str')\n",
    "#df['videoplay_id'] = range(0, len(df))\n",
    "df.columns\n",
    "print(df['start_time'][0])\n",
    "#df.dtypes\n",
    "#youtuber_data = df[['youtuber_id', 'youtuber_name', 'youtuber_location', 'youtuber_latitude', 'youtuber_longitude']]\n",
    "\n",
    "#df.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70d667",
   "metadata": {},
   "source": [
    "### Test_tables notebook ### \n",
    "Test your tables (Don't change anything in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72484521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from Create_Table_queries import *\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "def process_youtubedata_file(cur, filepath):\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "    \n",
    "    \"\"\"\n",
    "        This function reads one JSON file and read information of videos and youtuber data and saves into video_data and youtuber_data\n",
    "        Arguments:\n",
    "        cur: Database Cursor\n",
    "        filepath: location of JSON files\n",
    "        Return: None\n",
    "    \"\"\"\n",
    "    # open JSON file\n",
    "    youtuber_data = df[['youtuber_id', 'youtuber_name', 'youtuber_location', 'youtuber_latitude', 'youtuber_longitude']]\n",
    "    #print(\"Youtuber dataframe\\n\", youtuber_data)\n",
    "\n",
    "    # ---------insert youtuber record----------\n",
    "    # write your code here that reads youtuber data from JSON file and insert it into Youtubers_dim table \n",
    "    # write your code here\n",
    "    \n",
    "    \n",
    "    youtuber_record = []\n",
    "    for pair in youtuber_data.values[:,:]: \n",
    "        print(\"pair:\", pair)\n",
    "        for value in pair:\n",
    "            print(\"value:\", value)\n",
    "            for i in value.values() :\n",
    "                youtuber_record.append(i)\n",
    "    youtuber_data.append(youtuber_record)\n",
    "    print(\"\\n\\nYoutuber record to be added:\", youtuber_record)\n",
    "    \n",
    "    \n",
    "    cur.execute(Youtubers_table_insert, youtuber_record)\n",
    "    \n",
    "    # Check for data in the Youtuber_dim Table\n",
    "#     query = \"\"\"SELECT * FROM Youtubers_dim LIMIT 5;\"\"\"\n",
    "#     cur.execute(query)\n",
    "#     row = cur.fetchone()\n",
    "#     while row:\n",
    "#         print(\"Checking data in the table:\", row)\n",
    "#         row = cur.fetchone()\n",
    "    \n",
    "    print(\"Youtuber Dataframe\\n\", youtuber_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------insert video record--------------\n",
    "    # write your code here that reads youtube videos data from JSON file and insert it into Videos_dim table \n",
    "    # write your code here\n",
    "    \n",
    "    video_data = df[['video_id', 'title', 'youtuber_id', 'year', 'duration']]\n",
    "    print(\"\\n\\n\\nVideo dataframe\\n\", video_data)\n",
    "    \n",
    "    video_record = []\n",
    "    for pair in video_data.values[:,:] : \n",
    "        for value in pair:\n",
    "            for i in value.values() :\n",
    "                video_record.append(i)\n",
    "    print(\"\\n\\nVideo record to be added:\", video_record)\n",
    "    \n",
    "    cur.execute(Videos_table_insert, video_record)\n",
    "    \n",
    "    query = \"\"\"SELECT * FROM Videos_dim LIMIT 5;\"\"\"\n",
    "    cur.execute(query)\n",
    "    row = cur.fetchone()\n",
    "    while row:\n",
    "        print(\"Checking data in the table:\", row)\n",
    "        row = cur.fetchone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a135f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_22524/2832222818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_22524/2832222818.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/youtube_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocess_youtubedata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;31m#     process_data(cur, conn, filepath='data/log_data', func=process_log_file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_22524/2832222818.py\u001b[0m in \u001b[0;36mprocess_data\u001b[1;34m(cur, conn, filepath, func)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mReturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"   \n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\TEMP/ipykernel_22524/4069930209.py\u001b[0m in \u001b[0;36mprocess_youtubedata_file\u001b[1;34m(cur, filepath)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_youtubedata_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \"\"\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m             )\n\u001b[0;32m   1142\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "def process_log_file(cur, filepath):\n",
    "    \"\"\"\n",
    "        This function reads Log files and reads information of time, user and videoplay data and saves into time, user, videoplay\n",
    "        Arguments:\n",
    "        cur: Database Cursor\n",
    "        filepath: location of Log files\n",
    "        Return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # open log file\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "    print(\"Log files data:\\n\", df)\n",
    "    # filter by NextVideo action\n",
    "    \n",
    "#     df = df[(df['page'] == 'NextVideo')]\n",
    "#     print(\"Dataframe:\", df)\n",
    "\n",
    "    # convert timestamp column to datetime with 'pd.to_datetime(df[\"ts\"], unit=\"ms\")'   \n",
    "    # insert time data records to Time_dim table\n",
    "\n",
    "    \n",
    "        # write your code here\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # load user table\n",
    "    # insert user records into Users_dim table\n",
    "\n",
    "    \n",
    "        # write your code here\n",
    "\n",
    "    \n",
    "    # insert Videoplay records in Videoplay_fact table\n",
    "\n",
    "    \n",
    "        # write your code here\n",
    "\n",
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"\n",
    "        This function get all JSON files in given directory by exploring all sub directories, and process all files that were found using the given function.\n",
    "        Example: if I give it the path to youtube_data directory which resides in data folder of this assignment,\n",
    "        and func given is process_youtubedata_file it should get all JSON files in this directories and process each file using process_youtubedata_file function. \n",
    "        Arguments:\n",
    "        cur: Database Cursor\n",
    "        conn: Database\n",
    "        filepath: location of JSON files\n",
    "        func: function to process all files in the directory\n",
    "        Return: None\n",
    "    \"\"\"   \n",
    "    func(cur,filepath)\n",
    "        \n",
    "def main():\n",
    "    \n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 dbname=youtubedb user=postgres password=ali123ali\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    process_data(cur, conn, filepath='data/youtube_data', func=process_youtubedata_file)\n",
    "#     process_data(cur, conn, filepath='data/log_data', func=process_log_file)\n",
    "\n",
    "#     conn.close()\n",
    "\n",
    "#     conn = psycopg2.connect(\"host=127.0.0.1 dbname=youtubedb user=postgres password=ali123ali\")\n",
    "#     cur = conn.cursor()\n",
    "\n",
    "#     for filename in glob.glob('data\\youtube_data\\A\\A\\A\\*.json'):\n",
    "#         with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "#             text = f.read()\n",
    "#     process_youtubedata_file (cur, text)\n",
    "    \n",
    "    \n",
    "#     for filename in glob.glob('data\\log_data\\2018\\11*.json'):\n",
    "#         with open(os.path.join(os.getcwd(), filename), 'r') as file_log:\n",
    "#             text_log = file_log.read()\n",
    "    \n",
    "#     process_log_file (cur, text_log)\n",
    "#     process_log_file (cur, 'data\\log_data\\2018\\11\\*.json')\n",
    "    \n",
    "#     process_data(cur, conn, filepath='data/youtube_data', func=process_youtubedata_file)\n",
    "#     process_data(cur, conn, filepath='data/log_data', func=process_log_file)\n",
    "    \n",
    "#     conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aab1025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "['data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCEI128F424C983.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCFL128F149BB0D.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCIX128F4265903.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCKL128F423A778.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCPZ128F4275C32.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCRU128F423F449.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCTK128F934B224.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCUQ128E0783E2B.json', 'data\\\\youtube_data\\\\A\\\\B\\\\C\\\\TRABCXB128F4286BD3.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "    For the given path, get the List of all files in the directory tree \n",
    "'''\n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles \n",
    "\n",
    "l = getListOfFiles('data\\youtube_data')\n",
    "print(len(l))\n",
    "print(l[-10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3423367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
