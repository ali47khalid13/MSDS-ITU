{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-5VWruMtjA4"
   },
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3133,
     "status": "ok",
     "timestamp": 1651937482560,
     "user": {
      "displayName": "ALI KHALID KHALID MAHMOOD AHMAD",
      "userId": "04961537570896709275"
     },
     "user_tz": -300
    },
    "id": "YOrsQs3przvi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1651936475942,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "D8ZOVQkkdMvS",
    "outputId": "c247195d-6d45-4e82-870a-ee0e2a59e56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLBCZshD33oh"
   },
   "source": [
    "## **Preparing Dataset for task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYMwW4MD6MMy"
   },
   "source": [
    "**Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NF3M2PUc6Lh3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\TEMP/ipykernel_21412/3358446840.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['image']=new_data.index\n"
     ]
    }
   ],
   "source": [
    "# specify paths \n",
    "label_file = 'CelebA\\Anno\\list_attr_celeba.txt'\n",
    "img_dir = 'CelebA\\Img\\img_align_celeba\\img_align_celeba'\n",
    "output_dir = 'task3_data'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# load labels file\n",
    "data= pd.read_csv(label_file, sep='\\s+',skiprows=[0], header=0) #on_bad_lines='skip'\n",
    "data = data[['Male','Black_Hair','Wearing_Earrings','Eyeglasses','Straight_Hair','Smiling','Wearing_Necktie']]\n",
    "#data = data.reset_index()\n",
    "label_map = {1 : 1, -1 : 0}\n",
    "data = data.applymap(label_map.get)\n",
    "\n",
    "# choosing combinmation such that label for each category are almost equal\n",
    "new_data = data[ (data['Wearing_Earrings'] == 1) |(data['Eyeglasses'] == 1) |(data['Wearing_Necktie'] == 1)]\n",
    "new_data['image']=new_data.index\n",
    "\n",
    "# saving data to csv file\n",
    "new_data.to_csv(os.path.join(output_dir,'task3.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xImxLDAU-XOm"
   },
   "source": [
    "**Data Loading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651937487589,
     "user": {
      "displayName": "ALI KHALID KHALID MAHMOOD AHMAD",
      "userId": "04961537570896709275"
     },
     "user_tz": -300
    },
    "id": "VywF9NyEDj4d"
   },
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None): \n",
    "        # Run once\n",
    "        self.img_labels = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the number of samples in dataset\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # loads and returns a sample from the dataset at the given index\n",
    "        img_path = os.path.join(self.img_dir ,self.img_labels.iloc[idx, -1])\n",
    "        image = PIL.Image.open(img_path)\n",
    "        label = torch.tensor(self.img_labels.iloc[idx][0:-1])\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "o7JasRaVQLBW"
   },
   "outputs": [],
   "source": [
    "def load_dataset(csv_file, img_dir,train_size, validation_size, test_size, batch_size):\n",
    "\n",
    "    transform = transforms.Compose([#transforms.Grayscale(), \n",
    "                                     transforms.ToTensor(),\n",
    "                                     #transforms.Resize((28,28)), \n",
    "                                     #transforms.Normalize(0,0.5)\n",
    "                                     ])\n",
    "    # read dataset\n",
    "    dataset = Custom_Dataset(csv_file=csv_file, img_dir=img_dir, transform = transform)\n",
    "\n",
    "    # specify sizes\n",
    "    train_set_size = int(len(dataset) * train_size)\n",
    "    test_set_size = int(len(dataset) * test_size)\n",
    "    val_set_size = len(dataset) - train_set_size - test_set_size\n",
    "\n",
    "    # split data\n",
    "    train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_set_size,val_set_size,test_set_size])\n",
    "\n",
    "    # create dataloader for each data (test,train,val)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #return\n",
    "    return train_data_loader, val_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "error",
     "timestamp": 1651918321207,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "yiTxJ9SLYaa0",
    "outputId": "48ac96f1-68bf-4f1e-b6df-685fa9453ed8"
   },
   "outputs": [],
   "source": [
    "img_dir = 'CelebA\\Img\\img_align_celeba\\img_align_celeba'\n",
    "csv_file = 'task3_data/task3.csv'\n",
    "train_data_loader, val_data_loader, test_data_loader = load_dataset(csv_file,img_dir,0.7,0.15,0.15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKBtkDmfaolk"
   },
   "source": [
    "## **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ys55GWVAatBt"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*218*178, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 800)\n",
    "        self.fc3 = nn.Linear(800, 400)\n",
    "        self.fc4 = nn.Linear(400, 200)\n",
    "        self.fc5 = nn.Linear(200, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# model\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPhzrua7hZrv"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ovdn0w0qhfSW"
   },
   "outputs": [],
   "source": [
    "def train(epochs, train_data_loader, val_data_loader, loss_func, optimizer, learning_rate):\n",
    "    train_step = len(train_data_loader)\n",
    "    val_step = len(val_data_loader)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    for epoch in range(epochs): # iterate over epochs\n",
    "        t_loss = 0\n",
    "        t_acc = 0 \n",
    "        for i, data in enumerate(train_data_loader): # iterate over batches\n",
    "            \n",
    "          # get image and labels data is in tuple form (inputs, label)\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "         \n",
    "            # Zero-out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(image)\n",
    "            pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_loss += loss.item()\n",
    "            t_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, epochs, i+1, train_step, loss.item()))\n",
    "    \n",
    "        v_loss = 0\n",
    "        v_acc = 0\n",
    "        for i, data in enumerate(val_data_loader): # iterate over batches\n",
    "            # get image and labels data is in tuple form (inputs, label)\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = net(image)\n",
    "            #pred = np.squeeze(outputs)\n",
    "            pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "            loss = loss_func(outputs, labels)\n",
    "            v_loss += loss.item()\n",
    "            v_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "            v_acc += torch.sum(pred == labels)/len(labels)\n",
    "     \n",
    "        train_loss.append(t_loss/train_step)\n",
    "        train_acc.append(t_acc/train_step)\n",
    "        val_loss.append(v_loss/val_step)\n",
    "        val_acc.append(v_acc/val_step)\n",
    "        print ('Epoch [{}/{}], train_loss: {:.4f}, val_loss: {:.4f}' .format(epoch+1, epochs, train_loss[-1], val_loss[-1]))\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 106559,
     "status": "error",
     "timestamp": 1651881065083,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "pN0Nkf8Fh8fg",
    "outputId": "2f748680-34a9-46cf-e93f-370e7bcc611a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/2737], Loss: 0.6347\n",
      "Epoch [1/10], Step [20/2737], Loss: 0.6412\n",
      "Epoch [1/10], Step [30/2737], Loss: 0.5690\n",
      "Epoch [1/10], Step [40/2737], Loss: 0.5582\n",
      "Epoch [1/10], Step [50/2737], Loss: 0.8029\n",
      "Epoch [1/10], Step [60/2737], Loss: 0.5839\n",
      "Epoch [1/10], Step [70/2737], Loss: 0.5659\n",
      "Epoch [1/10], Step [80/2737], Loss: 0.5461\n",
      "Epoch [1/10], Step [90/2737], Loss: 0.6123\n",
      "Epoch [1/10], Step [100/2737], Loss: 0.5203\n",
      "Epoch [1/10], Step [110/2737], Loss: 0.5736\n",
      "Epoch [1/10], Step [120/2737], Loss: 0.4318\n",
      "Epoch [1/10], Step [130/2737], Loss: 0.4522\n",
      "Epoch [1/10], Step [140/2737], Loss: 0.4945\n",
      "Epoch [1/10], Step [150/2737], Loss: 0.4027\n",
      "Epoch [1/10], Step [160/2737], Loss: 0.4849\n",
      "Epoch [1/10], Step [170/2737], Loss: 0.4991\n",
      "Epoch [1/10], Step [180/2737], Loss: 0.4220\n",
      "Epoch [1/10], Step [190/2737], Loss: 0.3319\n",
      "Epoch [1/10], Step [200/2737], Loss: 0.5126\n",
      "Epoch [1/10], Step [210/2737], Loss: 0.3898\n",
      "Epoch [1/10], Step [220/2737], Loss: 0.4510\n",
      "Epoch [1/10], Step [230/2737], Loss: 0.4299\n",
      "Epoch [1/10], Step [240/2737], Loss: 0.4866\n",
      "Epoch [1/10], Step [250/2737], Loss: 0.3114\n",
      "Epoch [1/10], Step [260/2737], Loss: 0.4400\n",
      "Epoch [1/10], Step [270/2737], Loss: 0.6014\n",
      "Epoch [1/10], Step [280/2737], Loss: 0.5712\n",
      "Epoch [1/10], Step [290/2737], Loss: 0.4391\n",
      "Epoch [1/10], Step [300/2737], Loss: 0.5079\n",
      "Epoch [1/10], Step [310/2737], Loss: 0.4629\n",
      "Epoch [1/10], Step [320/2737], Loss: 0.3793\n",
      "Epoch [1/10], Step [330/2737], Loss: 0.6005\n",
      "Epoch [1/10], Step [340/2737], Loss: 0.4013\n",
      "Epoch [1/10], Step [350/2737], Loss: 0.4369\n",
      "Epoch [1/10], Step [360/2737], Loss: 0.4248\n",
      "Epoch [1/10], Step [370/2737], Loss: 0.3532\n",
      "Epoch [1/10], Step [380/2737], Loss: 0.3814\n",
      "Epoch [1/10], Step [390/2737], Loss: 0.4543\n",
      "Epoch [1/10], Step [400/2737], Loss: 0.4438\n",
      "Epoch [1/10], Step [410/2737], Loss: 0.3316\n",
      "Epoch [1/10], Step [420/2737], Loss: 0.3299\n",
      "Epoch [1/10], Step [430/2737], Loss: 0.3133\n",
      "Epoch [1/10], Step [440/2737], Loss: 0.4480\n",
      "Epoch [1/10], Step [450/2737], Loss: 0.3360\n",
      "Epoch [1/10], Step [460/2737], Loss: 0.4705\n",
      "Epoch [1/10], Step [470/2737], Loss: 0.5510\n",
      "Epoch [1/10], Step [480/2737], Loss: 0.3737\n",
      "Epoch [1/10], Step [490/2737], Loss: 0.5420\n",
      "Epoch [1/10], Step [500/2737], Loss: 0.3433\n",
      "Epoch [1/10], Step [510/2737], Loss: 0.3490\n",
      "Epoch [1/10], Step [520/2737], Loss: 0.3436\n",
      "Epoch [1/10], Step [530/2737], Loss: 0.3966\n",
      "Epoch [1/10], Step [540/2737], Loss: 0.3438\n",
      "Epoch [1/10], Step [550/2737], Loss: 0.3192\n",
      "Epoch [1/10], Step [560/2737], Loss: 0.3798\n",
      "Epoch [1/10], Step [570/2737], Loss: 0.3034\n",
      "Epoch [1/10], Step [580/2737], Loss: 0.4945\n",
      "Epoch [1/10], Step [590/2737], Loss: 0.3745\n",
      "Epoch [1/10], Step [600/2737], Loss: 0.4667\n",
      "Epoch [1/10], Step [610/2737], Loss: 0.4449\n",
      "Epoch [1/10], Step [620/2737], Loss: 0.3391\n",
      "Epoch [1/10], Step [630/2737], Loss: 0.3831\n",
      "Epoch [1/10], Step [640/2737], Loss: 0.4574\n",
      "Epoch [1/10], Step [650/2737], Loss: 0.3528\n",
      "Epoch [1/10], Step [660/2737], Loss: 0.6742\n",
      "Epoch [1/10], Step [670/2737], Loss: 0.3640\n",
      "Epoch [1/10], Step [680/2737], Loss: 0.4181\n",
      "Epoch [1/10], Step [690/2737], Loss: 0.3586\n",
      "Epoch [1/10], Step [700/2737], Loss: 0.3501\n",
      "Epoch [1/10], Step [710/2737], Loss: 0.3959\n",
      "Epoch [1/10], Step [720/2737], Loss: 0.3788\n",
      "Epoch [1/10], Step [730/2737], Loss: 0.2791\n",
      "Epoch [1/10], Step [740/2737], Loss: 0.3418\n",
      "Epoch [1/10], Step [750/2737], Loss: 0.3650\n",
      "Epoch [1/10], Step [760/2737], Loss: 0.4469\n",
      "Epoch [1/10], Step [770/2737], Loss: 0.4107\n",
      "Epoch [1/10], Step [780/2737], Loss: 0.4397\n",
      "Epoch [1/10], Step [790/2737], Loss: 0.3722\n",
      "Epoch [1/10], Step [800/2737], Loss: 0.3678\n",
      "Epoch [1/10], Step [810/2737], Loss: 0.3397\n",
      "Epoch [1/10], Step [820/2737], Loss: 0.4227\n",
      "Epoch [1/10], Step [830/2737], Loss: 0.3558\n",
      "Epoch [1/10], Step [840/2737], Loss: 0.2607\n",
      "Epoch [1/10], Step [850/2737], Loss: 0.4413\n",
      "Epoch [1/10], Step [860/2737], Loss: 0.2922\n",
      "Epoch [1/10], Step [870/2737], Loss: 0.3331\n",
      "Epoch [1/10], Step [880/2737], Loss: 0.4401\n",
      "Epoch [1/10], Step [890/2737], Loss: 0.2690\n",
      "Epoch [1/10], Step [900/2737], Loss: 0.2908\n",
      "Epoch [1/10], Step [910/2737], Loss: 0.4645\n",
      "Epoch [1/10], Step [920/2737], Loss: 0.3796\n",
      "Epoch [1/10], Step [930/2737], Loss: 0.3020\n",
      "Epoch [1/10], Step [940/2737], Loss: 0.4379\n",
      "Epoch [1/10], Step [950/2737], Loss: 0.4690\n",
      "Epoch [1/10], Step [960/2737], Loss: 0.1816\n",
      "Epoch [1/10], Step [970/2737], Loss: 0.4188\n",
      "Epoch [1/10], Step [980/2737], Loss: 0.2630\n",
      "Epoch [1/10], Step [990/2737], Loss: 0.5335\n",
      "Epoch [1/10], Step [1000/2737], Loss: 0.4984\n",
      "Epoch [1/10], Step [1010/2737], Loss: 0.3785\n",
      "Epoch [1/10], Step [1020/2737], Loss: 0.2460\n",
      "Epoch [1/10], Step [1030/2737], Loss: 0.3400\n",
      "Epoch [1/10], Step [1040/2737], Loss: 0.3245\n",
      "Epoch [1/10], Step [1050/2737], Loss: 0.4029\n",
      "Epoch [1/10], Step [1060/2737], Loss: 0.2900\n",
      "Epoch [1/10], Step [1070/2737], Loss: 0.3600\n",
      "Epoch [1/10], Step [1080/2737], Loss: 0.4334\n",
      "Epoch [1/10], Step [1090/2737], Loss: 0.2401\n",
      "Epoch [1/10], Step [1100/2737], Loss: 0.3983\n",
      "Epoch [1/10], Step [1110/2737], Loss: 0.4932\n",
      "Epoch [1/10], Step [1120/2737], Loss: 0.2846\n",
      "Epoch [1/10], Step [1130/2737], Loss: 0.3737\n",
      "Epoch [1/10], Step [1140/2737], Loss: 0.3397\n",
      "Epoch [1/10], Step [1150/2737], Loss: 0.2977\n",
      "Epoch [1/10], Step [1160/2737], Loss: 0.5110\n",
      "Epoch [1/10], Step [1170/2737], Loss: 0.2850\n",
      "Epoch [1/10], Step [1180/2737], Loss: 0.2504\n",
      "Epoch [1/10], Step [1190/2737], Loss: 0.3118\n",
      "Epoch [1/10], Step [1200/2737], Loss: 0.3650\n",
      "Epoch [1/10], Step [1210/2737], Loss: 0.3575\n",
      "Epoch [1/10], Step [1220/2737], Loss: 0.4839\n",
      "Epoch [1/10], Step [1230/2737], Loss: 0.4914\n",
      "Epoch [1/10], Step [1240/2737], Loss: 0.4012\n",
      "Epoch [1/10], Step [1250/2737], Loss: 0.4397\n",
      "Epoch [1/10], Step [1260/2737], Loss: 0.4079\n",
      "Epoch [1/10], Step [1270/2737], Loss: 0.3430\n",
      "Epoch [1/10], Step [1280/2737], Loss: 0.2449\n",
      "Epoch [1/10], Step [1290/2737], Loss: 0.3647\n",
      "Epoch [1/10], Step [1300/2737], Loss: 0.3539\n",
      "Epoch [1/10], Step [1310/2737], Loss: 0.2687\n",
      "Epoch [1/10], Step [1320/2737], Loss: 0.4603\n",
      "Epoch [1/10], Step [1330/2737], Loss: 0.3444\n",
      "Epoch [1/10], Step [1340/2737], Loss: 0.4164\n",
      "Epoch [1/10], Step [1350/2737], Loss: 0.4203\n",
      "Epoch [1/10], Step [1360/2737], Loss: 0.4069\n",
      "Epoch [1/10], Step [1370/2737], Loss: 0.3241\n",
      "Epoch [1/10], Step [1380/2737], Loss: 0.2936\n",
      "Epoch [1/10], Step [1390/2737], Loss: 0.2853\n",
      "Epoch [1/10], Step [1400/2737], Loss: 0.2112\n",
      "Epoch [1/10], Step [1410/2737], Loss: 0.2518\n",
      "Epoch [1/10], Step [1420/2737], Loss: 0.2984\n",
      "Epoch [1/10], Step [1430/2737], Loss: 0.3444\n",
      "Epoch [1/10], Step [1440/2737], Loss: 0.2562\n",
      "Epoch [1/10], Step [1450/2737], Loss: 0.2882\n",
      "Epoch [1/10], Step [1460/2737], Loss: 0.3764\n",
      "Epoch [1/10], Step [1470/2737], Loss: 0.3236\n",
      "Epoch [1/10], Step [1480/2737], Loss: 0.2801\n",
      "Epoch [1/10], Step [1490/2737], Loss: 0.3630\n",
      "Epoch [1/10], Step [1500/2737], Loss: 0.2848\n",
      "Epoch [1/10], Step [1510/2737], Loss: 0.2013\n",
      "Epoch [1/10], Step [1520/2737], Loss: 0.2342\n",
      "Epoch [1/10], Step [1530/2737], Loss: 0.4347\n",
      "Epoch [1/10], Step [1540/2737], Loss: 0.2487\n",
      "Epoch [1/10], Step [1550/2737], Loss: 0.3629\n",
      "Epoch [1/10], Step [1560/2737], Loss: 0.3145\n",
      "Epoch [1/10], Step [1570/2737], Loss: 0.3211\n",
      "Epoch [1/10], Step [1580/2737], Loss: 0.2647\n",
      "Epoch [1/10], Step [1590/2737], Loss: 0.2772\n",
      "Epoch [1/10], Step [1600/2737], Loss: 0.3572\n",
      "Epoch [1/10], Step [1610/2737], Loss: 0.2498\n",
      "Epoch [1/10], Step [1620/2737], Loss: 0.2926\n",
      "Epoch [1/10], Step [1630/2737], Loss: 0.3137\n",
      "Epoch [1/10], Step [1640/2737], Loss: 0.3792\n",
      "Epoch [1/10], Step [1650/2737], Loss: 0.3304\n",
      "Epoch [1/10], Step [1660/2737], Loss: 0.2395\n",
      "Epoch [1/10], Step [1670/2737], Loss: 0.1984\n",
      "Epoch [1/10], Step [1680/2737], Loss: 0.2898\n",
      "Epoch [1/10], Step [1690/2737], Loss: 0.2663\n",
      "Epoch [1/10], Step [1700/2737], Loss: 0.4083\n",
      "Epoch [1/10], Step [1710/2737], Loss: 0.3004\n",
      "Epoch [1/10], Step [1720/2737], Loss: 0.3836\n",
      "Epoch [1/10], Step [1730/2737], Loss: 0.3114\n",
      "Epoch [1/10], Step [1740/2737], Loss: 0.3434\n",
      "Epoch [1/10], Step [1750/2737], Loss: 0.3468\n",
      "Epoch [1/10], Step [1760/2737], Loss: 0.3810\n",
      "Epoch [1/10], Step [1770/2737], Loss: 0.3296\n",
      "Epoch [1/10], Step [1780/2737], Loss: 0.2838\n",
      "Epoch [1/10], Step [1790/2737], Loss: 0.2718\n",
      "Epoch [1/10], Step [1800/2737], Loss: 0.2558\n",
      "Epoch [1/10], Step [1810/2737], Loss: 0.3266\n",
      "Epoch [1/10], Step [1820/2737], Loss: 0.3492\n",
      "Epoch [1/10], Step [1830/2737], Loss: 0.2857\n",
      "Epoch [1/10], Step [1840/2737], Loss: 0.2076\n",
      "Epoch [1/10], Step [1850/2737], Loss: 0.2512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1860/2737], Loss: 0.6161\n",
      "Epoch [1/10], Step [1870/2737], Loss: 0.4435\n",
      "Epoch [1/10], Step [1880/2737], Loss: 0.3535\n",
      "Epoch [1/10], Step [1890/2737], Loss: 0.2830\n",
      "Epoch [1/10], Step [1900/2737], Loss: 0.3222\n",
      "Epoch [1/10], Step [1910/2737], Loss: 0.2536\n",
      "Epoch [1/10], Step [1920/2737], Loss: 0.3628\n",
      "Epoch [1/10], Step [1930/2737], Loss: 0.2671\n",
      "Epoch [1/10], Step [1940/2737], Loss: 0.4350\n",
      "Epoch [1/10], Step [1950/2737], Loss: 0.2387\n",
      "Epoch [1/10], Step [1960/2737], Loss: 0.3143\n",
      "Epoch [1/10], Step [1970/2737], Loss: 0.2704\n",
      "Epoch [1/10], Step [1980/2737], Loss: 0.3894\n",
      "Epoch [1/10], Step [1990/2737], Loss: 0.3281\n",
      "Epoch [1/10], Step [2000/2737], Loss: 0.3664\n",
      "Epoch [1/10], Step [2010/2737], Loss: 0.1877\n",
      "Epoch [1/10], Step [2020/2737], Loss: 0.2870\n",
      "Epoch [1/10], Step [2030/2737], Loss: 0.4393\n",
      "Epoch [1/10], Step [2040/2737], Loss: 0.3046\n",
      "Epoch [1/10], Step [2050/2737], Loss: 0.2980\n",
      "Epoch [1/10], Step [2060/2737], Loss: 0.4119\n",
      "Epoch [1/10], Step [2070/2737], Loss: 0.3236\n",
      "Epoch [1/10], Step [2080/2737], Loss: 0.4050\n",
      "Epoch [1/10], Step [2090/2737], Loss: 0.2724\n",
      "Epoch [1/10], Step [2100/2737], Loss: 0.4611\n",
      "Epoch [1/10], Step [2110/2737], Loss: 0.2472\n",
      "Epoch [1/10], Step [2120/2737], Loss: 0.3386\n",
      "Epoch [1/10], Step [2130/2737], Loss: 0.3515\n",
      "Epoch [1/10], Step [2140/2737], Loss: 0.2965\n",
      "Epoch [1/10], Step [2150/2737], Loss: 0.3455\n",
      "Epoch [1/10], Step [2160/2737], Loss: 0.2966\n",
      "Epoch [1/10], Step [2170/2737], Loss: 0.2532\n",
      "Epoch [1/10], Step [2180/2737], Loss: 0.2742\n",
      "Epoch [1/10], Step [2190/2737], Loss: 0.2973\n",
      "Epoch [1/10], Step [2200/2737], Loss: 0.1870\n",
      "Epoch [1/10], Step [2210/2737], Loss: 0.3272\n",
      "Epoch [1/10], Step [2220/2737], Loss: 0.3214\n",
      "Epoch [1/10], Step [2230/2737], Loss: 0.2485\n",
      "Epoch [1/10], Step [2240/2737], Loss: 0.1989\n",
      "Epoch [1/10], Step [2250/2737], Loss: 0.1922\n",
      "Epoch [1/10], Step [2260/2737], Loss: 0.2426\n",
      "Epoch [1/10], Step [2270/2737], Loss: 0.2566\n",
      "Epoch [1/10], Step [2280/2737], Loss: 0.2392\n",
      "Epoch [1/10], Step [2290/2737], Loss: 0.2705\n",
      "Epoch [1/10], Step [2300/2737], Loss: 0.2951\n",
      "Epoch [1/10], Step [2310/2737], Loss: 0.2953\n",
      "Epoch [1/10], Step [2320/2737], Loss: 0.3213\n",
      "Epoch [1/10], Step [2330/2737], Loss: 0.2589\n",
      "Epoch [1/10], Step [2340/2737], Loss: 0.2620\n",
      "Epoch [1/10], Step [2350/2737], Loss: 0.4424\n",
      "Epoch [1/10], Step [2360/2737], Loss: 0.4139\n",
      "Epoch [1/10], Step [2370/2737], Loss: 0.3125\n",
      "Epoch [1/10], Step [2380/2737], Loss: 0.3646\n",
      "Epoch [1/10], Step [2390/2737], Loss: 0.3448\n",
      "Epoch [1/10], Step [2400/2737], Loss: 0.3325\n",
      "Epoch [1/10], Step [2410/2737], Loss: 0.2897\n",
      "Epoch [1/10], Step [2420/2737], Loss: 0.4350\n",
      "Epoch [1/10], Step [2430/2737], Loss: 0.2779\n",
      "Epoch [1/10], Step [2440/2737], Loss: 0.3315\n",
      "Epoch [1/10], Step [2450/2737], Loss: 0.4091\n",
      "Epoch [1/10], Step [2460/2737], Loss: 0.3135\n",
      "Epoch [1/10], Step [2470/2737], Loss: 0.2908\n",
      "Epoch [1/10], Step [2480/2737], Loss: 0.4593\n",
      "Epoch [1/10], Step [2490/2737], Loss: 0.2502\n",
      "Epoch [1/10], Step [2500/2737], Loss: 0.5488\n",
      "Epoch [1/10], Step [2510/2737], Loss: 0.2763\n",
      "Epoch [1/10], Step [2520/2737], Loss: 0.2806\n",
      "Epoch [1/10], Step [2530/2737], Loss: 0.2376\n",
      "Epoch [1/10], Step [2540/2737], Loss: 0.3657\n",
      "Epoch [1/10], Step [2550/2737], Loss: 0.3551\n",
      "Epoch [1/10], Step [2560/2737], Loss: 0.2763\n",
      "Epoch [1/10], Step [2570/2737], Loss: 0.2515\n",
      "Epoch [1/10], Step [2580/2737], Loss: 0.3588\n",
      "Epoch [1/10], Step [2590/2737], Loss: 0.2913\n",
      "Epoch [1/10], Step [2600/2737], Loss: 0.2884\n",
      "Epoch [1/10], Step [2610/2737], Loss: 0.4628\n",
      "Epoch [1/10], Step [2620/2737], Loss: 0.2482\n",
      "Epoch [1/10], Step [2630/2737], Loss: 0.2961\n",
      "Epoch [1/10], Step [2640/2737], Loss: 0.2562\n",
      "Epoch [1/10], Step [2650/2737], Loss: 0.3311\n",
      "Epoch [1/10], Step [2660/2737], Loss: 0.2540\n",
      "Epoch [1/10], Step [2670/2737], Loss: 0.3189\n",
      "Epoch [1/10], Step [2680/2737], Loss: 0.2783\n",
      "Epoch [1/10], Step [2690/2737], Loss: 0.2649\n",
      "Epoch [1/10], Step [2700/2737], Loss: 0.3987\n",
      "Epoch [1/10], Step [2710/2737], Loss: 0.2813\n",
      "Epoch [1/10], Step [2720/2737], Loss: 0.3455\n",
      "Epoch [1/10], Step [2730/2737], Loss: 0.3220\n",
      "Epoch [1/10], train_loss: 0.3532, val_loss: 0.2786\n",
      "Epoch [2/10], Step [10/2737], Loss: 0.1948\n",
      "Epoch [2/10], Step [20/2737], Loss: 0.2215\n",
      "Epoch [2/10], Step [30/2737], Loss: 0.3229\n",
      "Epoch [2/10], Step [40/2737], Loss: 0.2095\n",
      "Epoch [2/10], Step [50/2737], Loss: 0.2611\n",
      "Epoch [2/10], Step [60/2737], Loss: 0.3777\n",
      "Epoch [2/10], Step [70/2737], Loss: 0.1583\n",
      "Epoch [2/10], Step [80/2737], Loss: 0.2646\n",
      "Epoch [2/10], Step [90/2737], Loss: 0.4058\n",
      "Epoch [2/10], Step [100/2737], Loss: 0.2776\n",
      "Epoch [2/10], Step [110/2737], Loss: 0.3061\n",
      "Epoch [2/10], Step [120/2737], Loss: 0.2384\n",
      "Epoch [2/10], Step [130/2737], Loss: 0.3859\n",
      "Epoch [2/10], Step [140/2737], Loss: 0.4587\n",
      "Epoch [2/10], Step [150/2737], Loss: 0.5029\n",
      "Epoch [2/10], Step [160/2737], Loss: 0.2766\n",
      "Epoch [2/10], Step [170/2737], Loss: 0.2518\n",
      "Epoch [2/10], Step [180/2737], Loss: 0.1587\n",
      "Epoch [2/10], Step [190/2737], Loss: 0.4281\n",
      "Epoch [2/10], Step [200/2737], Loss: 0.2839\n",
      "Epoch [2/10], Step [210/2737], Loss: 0.2122\n",
      "Epoch [2/10], Step [220/2737], Loss: 0.2592\n",
      "Epoch [2/10], Step [230/2737], Loss: 0.3165\n",
      "Epoch [2/10], Step [240/2737], Loss: 0.2765\n",
      "Epoch [2/10], Step [250/2737], Loss: 0.2689\n",
      "Epoch [2/10], Step [260/2737], Loss: 0.2827\n",
      "Epoch [2/10], Step [270/2737], Loss: 0.1556\n",
      "Epoch [2/10], Step [280/2737], Loss: 0.2998\n",
      "Epoch [2/10], Step [290/2737], Loss: 0.2944\n",
      "Epoch [2/10], Step [300/2737], Loss: 0.3321\n",
      "Epoch [2/10], Step [310/2737], Loss: 0.1481\n",
      "Epoch [2/10], Step [320/2737], Loss: 0.3072\n",
      "Epoch [2/10], Step [330/2737], Loss: 0.3709\n",
      "Epoch [2/10], Step [340/2737], Loss: 0.3433\n",
      "Epoch [2/10], Step [350/2737], Loss: 0.3453\n",
      "Epoch [2/10], Step [360/2737], Loss: 0.2309\n",
      "Epoch [2/10], Step [370/2737], Loss: 0.2981\n",
      "Epoch [2/10], Step [380/2737], Loss: 0.2936\n",
      "Epoch [2/10], Step [390/2737], Loss: 0.2490\n",
      "Epoch [2/10], Step [400/2737], Loss: 0.2518\n",
      "Epoch [2/10], Step [410/2737], Loss: 0.3046\n",
      "Epoch [2/10], Step [420/2737], Loss: 0.3520\n",
      "Epoch [2/10], Step [430/2737], Loss: 0.2988\n",
      "Epoch [2/10], Step [440/2737], Loss: 0.3078\n",
      "Epoch [2/10], Step [450/2737], Loss: 0.3054\n",
      "Epoch [2/10], Step [460/2737], Loss: 0.3102\n",
      "Epoch [2/10], Step [470/2737], Loss: 0.2127\n",
      "Epoch [2/10], Step [480/2737], Loss: 0.2896\n",
      "Epoch [2/10], Step [490/2737], Loss: 0.2803\n",
      "Epoch [2/10], Step [500/2737], Loss: 0.2311\n",
      "Epoch [2/10], Step [510/2737], Loss: 0.2661\n",
      "Epoch [2/10], Step [520/2737], Loss: 0.1799\n",
      "Epoch [2/10], Step [530/2737], Loss: 0.3493\n",
      "Epoch [2/10], Step [540/2737], Loss: 0.3496\n",
      "Epoch [2/10], Step [550/2737], Loss: 0.3174\n",
      "Epoch [2/10], Step [560/2737], Loss: 0.2696\n",
      "Epoch [2/10], Step [570/2737], Loss: 0.3591\n",
      "Epoch [2/10], Step [580/2737], Loss: 0.1832\n",
      "Epoch [2/10], Step [590/2737], Loss: 0.2550\n",
      "Epoch [2/10], Step [600/2737], Loss: 0.1712\n",
      "Epoch [2/10], Step [610/2737], Loss: 0.2624\n",
      "Epoch [2/10], Step [620/2737], Loss: 0.4196\n",
      "Epoch [2/10], Step [630/2737], Loss: 0.3019\n",
      "Epoch [2/10], Step [640/2737], Loss: 0.3208\n",
      "Epoch [2/10], Step [650/2737], Loss: 0.2418\n",
      "Epoch [2/10], Step [660/2737], Loss: 0.2263\n",
      "Epoch [2/10], Step [670/2737], Loss: 0.2366\n",
      "Epoch [2/10], Step [680/2737], Loss: 0.2537\n",
      "Epoch [2/10], Step [690/2737], Loss: 0.3575\n",
      "Epoch [2/10], Step [700/2737], Loss: 0.3000\n",
      "Epoch [2/10], Step [710/2737], Loss: 0.2911\n",
      "Epoch [2/10], Step [720/2737], Loss: 0.2991\n",
      "Epoch [2/10], Step [730/2737], Loss: 0.3500\n",
      "Epoch [2/10], Step [740/2737], Loss: 0.1960\n",
      "Epoch [2/10], Step [750/2737], Loss: 0.2184\n",
      "Epoch [2/10], Step [760/2737], Loss: 0.2928\n",
      "Epoch [2/10], Step [770/2737], Loss: 0.1464\n",
      "Epoch [2/10], Step [780/2737], Loss: 0.1584\n",
      "Epoch [2/10], Step [790/2737], Loss: 0.4475\n",
      "Epoch [2/10], Step [800/2737], Loss: 0.2794\n",
      "Epoch [2/10], Step [810/2737], Loss: 0.2365\n",
      "Epoch [2/10], Step [820/2737], Loss: 0.3158\n",
      "Epoch [2/10], Step [830/2737], Loss: 0.3801\n",
      "Epoch [2/10], Step [840/2737], Loss: 0.2477\n",
      "Epoch [2/10], Step [850/2737], Loss: 0.3431\n",
      "Epoch [2/10], Step [860/2737], Loss: 0.2801\n",
      "Epoch [2/10], Step [870/2737], Loss: 0.3080\n",
      "Epoch [2/10], Step [880/2737], Loss: 0.3751\n",
      "Epoch [2/10], Step [890/2737], Loss: 0.3612\n",
      "Epoch [2/10], Step [900/2737], Loss: 0.3367\n",
      "Epoch [2/10], Step [910/2737], Loss: 0.3230\n",
      "Epoch [2/10], Step [920/2737], Loss: 0.2491\n",
      "Epoch [2/10], Step [930/2737], Loss: 0.2393\n",
      "Epoch [2/10], Step [940/2737], Loss: 0.3161\n",
      "Epoch [2/10], Step [950/2737], Loss: 0.2386\n",
      "Epoch [2/10], Step [960/2737], Loss: 0.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [970/2737], Loss: 0.2124\n",
      "Epoch [2/10], Step [980/2737], Loss: 0.2613\n",
      "Epoch [2/10], Step [990/2737], Loss: 0.3166\n",
      "Epoch [2/10], Step [1000/2737], Loss: 0.3213\n",
      "Epoch [2/10], Step [1010/2737], Loss: 0.2558\n",
      "Epoch [2/10], Step [1020/2737], Loss: 0.2922\n",
      "Epoch [2/10], Step [1030/2737], Loss: 0.3712\n",
      "Epoch [2/10], Step [1040/2737], Loss: 0.2259\n",
      "Epoch [2/10], Step [1050/2737], Loss: 0.4103\n",
      "Epoch [2/10], Step [1060/2737], Loss: 0.3622\n",
      "Epoch [2/10], Step [1070/2737], Loss: 0.3085\n",
      "Epoch [2/10], Step [1080/2737], Loss: 0.2129\n",
      "Epoch [2/10], Step [1090/2737], Loss: 0.3451\n",
      "Epoch [2/10], Step [1100/2737], Loss: 0.3445\n",
      "Epoch [2/10], Step [1110/2737], Loss: 0.2603\n",
      "Epoch [2/10], Step [1120/2737], Loss: 0.2954\n",
      "Epoch [2/10], Step [1130/2737], Loss: 0.3169\n",
      "Epoch [2/10], Step [1140/2737], Loss: 0.2361\n",
      "Epoch [2/10], Step [1150/2737], Loss: 0.3008\n",
      "Epoch [2/10], Step [1160/2737], Loss: 0.2211\n",
      "Epoch [2/10], Step [1170/2737], Loss: 0.3245\n",
      "Epoch [2/10], Step [1180/2737], Loss: 0.2422\n",
      "Epoch [2/10], Step [1190/2737], Loss: 0.3583\n",
      "Epoch [2/10], Step [1200/2737], Loss: 0.3922\n",
      "Epoch [2/10], Step [1210/2737], Loss: 0.3436\n",
      "Epoch [2/10], Step [1220/2737], Loss: 0.2441\n",
      "Epoch [2/10], Step [1230/2737], Loss: 0.3068\n",
      "Epoch [2/10], Step [1240/2737], Loss: 0.3584\n",
      "Epoch [2/10], Step [1250/2737], Loss: 0.4079\n",
      "Epoch [2/10], Step [1260/2737], Loss: 0.3259\n",
      "Epoch [2/10], Step [1270/2737], Loss: 0.3404\n",
      "Epoch [2/10], Step [1280/2737], Loss: 0.2780\n",
      "Epoch [2/10], Step [1290/2737], Loss: 0.1993\n",
      "Epoch [2/10], Step [1300/2737], Loss: 0.2764\n",
      "Epoch [2/10], Step [1310/2737], Loss: 0.3581\n",
      "Epoch [2/10], Step [1320/2737], Loss: 0.2305\n",
      "Epoch [2/10], Step [1330/2737], Loss: 0.3435\n",
      "Epoch [2/10], Step [1340/2737], Loss: 0.2182\n",
      "Epoch [2/10], Step [1350/2737], Loss: 0.3003\n",
      "Epoch [2/10], Step [1360/2737], Loss: 0.2957\n",
      "Epoch [2/10], Step [1370/2737], Loss: 0.2346\n",
      "Epoch [2/10], Step [1380/2737], Loss: 0.3530\n",
      "Epoch [2/10], Step [1390/2737], Loss: 0.3209\n",
      "Epoch [2/10], Step [1400/2737], Loss: 0.1644\n",
      "Epoch [2/10], Step [1410/2737], Loss: 0.2684\n",
      "Epoch [2/10], Step [1420/2737], Loss: 0.2453\n",
      "Epoch [2/10], Step [1430/2737], Loss: 0.1491\n",
      "Epoch [2/10], Step [1440/2737], Loss: 0.3047\n",
      "Epoch [2/10], Step [1450/2737], Loss: 0.2769\n",
      "Epoch [2/10], Step [1460/2737], Loss: 0.2697\n",
      "Epoch [2/10], Step [1470/2737], Loss: 0.4954\n",
      "Epoch [2/10], Step [1480/2737], Loss: 0.2432\n",
      "Epoch [2/10], Step [1490/2737], Loss: 0.3405\n",
      "Epoch [2/10], Step [1500/2737], Loss: 0.2512\n",
      "Epoch [2/10], Step [1510/2737], Loss: 0.3152\n",
      "Epoch [2/10], Step [1520/2737], Loss: 0.2453\n",
      "Epoch [2/10], Step [1530/2737], Loss: 0.3315\n",
      "Epoch [2/10], Step [1540/2737], Loss: 0.2620\n",
      "Epoch [2/10], Step [1550/2737], Loss: 0.2538\n",
      "Epoch [2/10], Step [1560/2737], Loss: 0.3547\n",
      "Epoch [2/10], Step [1570/2737], Loss: 0.2355\n",
      "Epoch [2/10], Step [1580/2737], Loss: 0.4180\n",
      "Epoch [2/10], Step [1590/2737], Loss: 0.2346\n",
      "Epoch [2/10], Step [1600/2737], Loss: 0.2810\n",
      "Epoch [2/10], Step [1610/2737], Loss: 0.2074\n",
      "Epoch [2/10], Step [1620/2737], Loss: 0.2465\n",
      "Epoch [2/10], Step [1630/2737], Loss: 0.2084\n",
      "Epoch [2/10], Step [1640/2737], Loss: 0.3350\n",
      "Epoch [2/10], Step [1650/2737], Loss: 0.2591\n",
      "Epoch [2/10], Step [1660/2737], Loss: 0.3070\n",
      "Epoch [2/10], Step [1670/2737], Loss: 0.2975\n",
      "Epoch [2/10], Step [1680/2737], Loss: 0.3409\n",
      "Epoch [2/10], Step [1690/2737], Loss: 0.2204\n",
      "Epoch [2/10], Step [1700/2737], Loss: 0.4147\n",
      "Epoch [2/10], Step [1710/2737], Loss: 0.2604\n",
      "Epoch [2/10], Step [1720/2737], Loss: 0.2999\n",
      "Epoch [2/10], Step [1730/2737], Loss: 0.3302\n",
      "Epoch [2/10], Step [1740/2737], Loss: 0.2229\n",
      "Epoch [2/10], Step [1750/2737], Loss: 0.2419\n",
      "Epoch [2/10], Step [1760/2737], Loss: 0.2602\n",
      "Epoch [2/10], Step [1770/2737], Loss: 0.2119\n",
      "Epoch [2/10], Step [1780/2737], Loss: 0.2618\n",
      "Epoch [2/10], Step [1790/2737], Loss: 0.2381\n",
      "Epoch [2/10], Step [1800/2737], Loss: 0.2910\n",
      "Epoch [2/10], Step [1810/2737], Loss: 0.5176\n",
      "Epoch [2/10], Step [1820/2737], Loss: 0.2684\n",
      "Epoch [2/10], Step [1830/2737], Loss: 0.3837\n",
      "Epoch [2/10], Step [1840/2737], Loss: 0.3372\n",
      "Epoch [2/10], Step [1850/2737], Loss: 0.3257\n",
      "Epoch [2/10], Step [1860/2737], Loss: 0.1877\n",
      "Epoch [2/10], Step [1870/2737], Loss: 0.3112\n",
      "Epoch [2/10], Step [1880/2737], Loss: 0.3301\n",
      "Epoch [2/10], Step [1890/2737], Loss: 0.1941\n",
      "Epoch [2/10], Step [1900/2737], Loss: 0.2181\n",
      "Epoch [2/10], Step [1910/2737], Loss: 0.3129\n",
      "Epoch [2/10], Step [1920/2737], Loss: 0.1620\n",
      "Epoch [2/10], Step [1930/2737], Loss: 0.2935\n",
      "Epoch [2/10], Step [1940/2737], Loss: 0.3706\n",
      "Epoch [2/10], Step [1950/2737], Loss: 0.1351\n",
      "Epoch [2/10], Step [1960/2737], Loss: 0.2350\n",
      "Epoch [2/10], Step [1970/2737], Loss: 0.2348\n",
      "Epoch [2/10], Step [1980/2737], Loss: 0.2610\n",
      "Epoch [2/10], Step [1990/2737], Loss: 0.3222\n",
      "Epoch [2/10], Step [2000/2737], Loss: 0.2907\n",
      "Epoch [2/10], Step [2010/2737], Loss: 0.1779\n",
      "Epoch [2/10], Step [2020/2737], Loss: 0.3463\n",
      "Epoch [2/10], Step [2030/2737], Loss: 0.2590\n",
      "Epoch [2/10], Step [2040/2737], Loss: 0.3325\n",
      "Epoch [2/10], Step [2050/2737], Loss: 0.3321\n",
      "Epoch [2/10], Step [2060/2737], Loss: 0.2468\n",
      "Epoch [2/10], Step [2070/2737], Loss: 0.2338\n",
      "Epoch [2/10], Step [2080/2737], Loss: 0.2036\n",
      "Epoch [2/10], Step [2090/2737], Loss: 0.2898\n",
      "Epoch [2/10], Step [2100/2737], Loss: 0.3981\n",
      "Epoch [2/10], Step [2110/2737], Loss: 0.2381\n",
      "Epoch [2/10], Step [2120/2737], Loss: 0.3260\n",
      "Epoch [2/10], Step [2130/2737], Loss: 0.2183\n",
      "Epoch [2/10], Step [2140/2737], Loss: 0.3172\n",
      "Epoch [2/10], Step [2150/2737], Loss: 0.3130\n",
      "Epoch [2/10], Step [2160/2737], Loss: 0.2582\n",
      "Epoch [2/10], Step [2170/2737], Loss: 0.4021\n",
      "Epoch [2/10], Step [2180/2737], Loss: 0.1514\n",
      "Epoch [2/10], Step [2190/2737], Loss: 0.3189\n",
      "Epoch [2/10], Step [2200/2737], Loss: 0.2601\n",
      "Epoch [2/10], Step [2210/2737], Loss: 0.2901\n",
      "Epoch [2/10], Step [2220/2737], Loss: 0.1829\n",
      "Epoch [2/10], Step [2230/2737], Loss: 0.1691\n",
      "Epoch [2/10], Step [2240/2737], Loss: 0.2785\n",
      "Epoch [2/10], Step [2250/2737], Loss: 0.2332\n",
      "Epoch [2/10], Step [2260/2737], Loss: 0.3538\n",
      "Epoch [2/10], Step [2270/2737], Loss: 0.1607\n",
      "Epoch [2/10], Step [2280/2737], Loss: 0.3564\n",
      "Epoch [2/10], Step [2290/2737], Loss: 0.1898\n",
      "Epoch [2/10], Step [2300/2737], Loss: 0.2706\n",
      "Epoch [2/10], Step [2310/2737], Loss: 0.4054\n",
      "Epoch [2/10], Step [2320/2737], Loss: 0.2776\n",
      "Epoch [2/10], Step [2330/2737], Loss: 0.2940\n",
      "Epoch [2/10], Step [2340/2737], Loss: 0.1618\n",
      "Epoch [2/10], Step [2350/2737], Loss: 0.3026\n",
      "Epoch [2/10], Step [2360/2737], Loss: 0.3643\n",
      "Epoch [2/10], Step [2370/2737], Loss: 0.2739\n",
      "Epoch [2/10], Step [2380/2737], Loss: 0.3018\n",
      "Epoch [2/10], Step [2390/2737], Loss: 0.2869\n",
      "Epoch [2/10], Step [2400/2737], Loss: 0.2411\n",
      "Epoch [2/10], Step [2410/2737], Loss: 0.1805\n",
      "Epoch [2/10], Step [2420/2737], Loss: 0.3725\n",
      "Epoch [2/10], Step [2430/2737], Loss: 0.1753\n",
      "Epoch [2/10], Step [2440/2737], Loss: 0.2352\n",
      "Epoch [2/10], Step [2450/2737], Loss: 0.3161\n",
      "Epoch [2/10], Step [2460/2737], Loss: 0.2505\n",
      "Epoch [2/10], Step [2470/2737], Loss: 0.2641\n",
      "Epoch [2/10], Step [2480/2737], Loss: 0.2509\n",
      "Epoch [2/10], Step [2490/2737], Loss: 0.3292\n",
      "Epoch [2/10], Step [2500/2737], Loss: 0.1844\n",
      "Epoch [2/10], Step [2510/2737], Loss: 0.3627\n",
      "Epoch [2/10], Step [2520/2737], Loss: 0.1846\n",
      "Epoch [2/10], Step [2530/2737], Loss: 0.3564\n",
      "Epoch [2/10], Step [2540/2737], Loss: 0.2420\n",
      "Epoch [2/10], Step [2550/2737], Loss: 0.5439\n",
      "Epoch [2/10], Step [2560/2737], Loss: 0.1925\n",
      "Epoch [2/10], Step [2570/2737], Loss: 0.3371\n",
      "Epoch [2/10], Step [2580/2737], Loss: 0.3004\n",
      "Epoch [2/10], Step [2590/2737], Loss: 0.2354\n",
      "Epoch [2/10], Step [2600/2737], Loss: 0.2764\n",
      "Epoch [2/10], Step [2610/2737], Loss: 0.2076\n",
      "Epoch [2/10], Step [2620/2737], Loss: 0.3045\n",
      "Epoch [2/10], Step [2630/2737], Loss: 0.3923\n",
      "Epoch [2/10], Step [2640/2737], Loss: 0.1564\n",
      "Epoch [2/10], Step [2650/2737], Loss: 0.2854\n",
      "Epoch [2/10], Step [2660/2737], Loss: 0.3522\n",
      "Epoch [2/10], Step [2670/2737], Loss: 0.3407\n",
      "Epoch [2/10], Step [2680/2737], Loss: 0.2616\n",
      "Epoch [2/10], Step [2690/2737], Loss: 0.2534\n",
      "Epoch [2/10], Step [2700/2737], Loss: 0.2740\n",
      "Epoch [2/10], Step [2710/2737], Loss: 0.2691\n",
      "Epoch [2/10], Step [2720/2737], Loss: 0.2865\n",
      "Epoch [2/10], Step [2730/2737], Loss: 0.3158\n",
      "Epoch [2/10], train_loss: 0.2824, val_loss: 0.3078\n",
      "Epoch [3/10], Step [10/2737], Loss: 0.2317\n",
      "Epoch [3/10], Step [20/2737], Loss: 0.1558\n",
      "Epoch [3/10], Step [30/2737], Loss: 0.2794\n",
      "Epoch [3/10], Step [40/2737], Loss: 0.4031\n",
      "Epoch [3/10], Step [50/2737], Loss: 0.2167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [60/2737], Loss: 0.2720\n",
      "Epoch [3/10], Step [70/2737], Loss: 0.3092\n",
      "Epoch [3/10], Step [80/2737], Loss: 0.2218\n",
      "Epoch [3/10], Step [90/2737], Loss: 0.2028\n",
      "Epoch [3/10], Step [100/2737], Loss: 0.1771\n",
      "Epoch [3/10], Step [110/2737], Loss: 0.3466\n",
      "Epoch [3/10], Step [120/2737], Loss: 0.3289\n",
      "Epoch [3/10], Step [130/2737], Loss: 0.2625\n",
      "Epoch [3/10], Step [140/2737], Loss: 0.3588\n",
      "Epoch [3/10], Step [150/2737], Loss: 0.1895\n",
      "Epoch [3/10], Step [160/2737], Loss: 0.1968\n",
      "Epoch [3/10], Step [170/2737], Loss: 0.3151\n",
      "Epoch [3/10], Step [180/2737], Loss: 0.2710\n",
      "Epoch [3/10], Step [190/2737], Loss: 0.2775\n",
      "Epoch [3/10], Step [200/2737], Loss: 0.2901\n",
      "Epoch [3/10], Step [210/2737], Loss: 0.3863\n",
      "Epoch [3/10], Step [220/2737], Loss: 0.2597\n",
      "Epoch [3/10], Step [230/2737], Loss: 0.3538\n",
      "Epoch [3/10], Step [240/2737], Loss: 0.2628\n",
      "Epoch [3/10], Step [250/2737], Loss: 0.1670\n",
      "Epoch [3/10], Step [260/2737], Loss: 0.2480\n",
      "Epoch [3/10], Step [270/2737], Loss: 0.3016\n",
      "Epoch [3/10], Step [280/2737], Loss: 0.1674\n",
      "Epoch [3/10], Step [290/2737], Loss: 0.3411\n",
      "Epoch [3/10], Step [300/2737], Loss: 0.2228\n",
      "Epoch [3/10], Step [310/2737], Loss: 0.2793\n",
      "Epoch [3/10], Step [320/2737], Loss: 0.3700\n",
      "Epoch [3/10], Step [330/2737], Loss: 0.3613\n",
      "Epoch [3/10], Step [340/2737], Loss: 0.2663\n",
      "Epoch [3/10], Step [350/2737], Loss: 0.2242\n",
      "Epoch [3/10], Step [360/2737], Loss: 0.2019\n",
      "Epoch [3/10], Step [370/2737], Loss: 0.2584\n",
      "Epoch [3/10], Step [380/2737], Loss: 0.3040\n",
      "Epoch [3/10], Step [390/2737], Loss: 0.2409\n",
      "Epoch [3/10], Step [400/2737], Loss: 0.1970\n",
      "Epoch [3/10], Step [410/2737], Loss: 0.2446\n",
      "Epoch [3/10], Step [420/2737], Loss: 0.2835\n",
      "Epoch [3/10], Step [430/2737], Loss: 0.2459\n",
      "Epoch [3/10], Step [440/2737], Loss: 0.2970\n",
      "Epoch [3/10], Step [450/2737], Loss: 0.2386\n",
      "Epoch [3/10], Step [460/2737], Loss: 0.2277\n",
      "Epoch [3/10], Step [470/2737], Loss: 0.3009\n",
      "Epoch [3/10], Step [480/2737], Loss: 0.3189\n",
      "Epoch [3/10], Step [490/2737], Loss: 0.1994\n",
      "Epoch [3/10], Step [500/2737], Loss: 0.2308\n",
      "Epoch [3/10], Step [510/2737], Loss: 0.2709\n",
      "Epoch [3/10], Step [520/2737], Loss: 0.2246\n",
      "Epoch [3/10], Step [530/2737], Loss: 0.3098\n",
      "Epoch [3/10], Step [540/2737], Loss: 0.2940\n",
      "Epoch [3/10], Step [550/2737], Loss: 0.2674\n",
      "Epoch [3/10], Step [560/2737], Loss: 0.2537\n",
      "Epoch [3/10], Step [570/2737], Loss: 0.3596\n",
      "Epoch [3/10], Step [580/2737], Loss: 0.2253\n",
      "Epoch [3/10], Step [590/2737], Loss: 0.2109\n",
      "Epoch [3/10], Step [600/2737], Loss: 0.2317\n",
      "Epoch [3/10], Step [610/2737], Loss: 0.2280\n",
      "Epoch [3/10], Step [620/2737], Loss: 0.2214\n",
      "Epoch [3/10], Step [630/2737], Loss: 0.3029\n",
      "Epoch [3/10], Step [640/2737], Loss: 0.2049\n",
      "Epoch [3/10], Step [650/2737], Loss: 0.1446\n",
      "Epoch [3/10], Step [660/2737], Loss: 0.1979\n",
      "Epoch [3/10], Step [670/2737], Loss: 0.2963\n",
      "Epoch [3/10], Step [680/2737], Loss: 0.1833\n",
      "Epoch [3/10], Step [690/2737], Loss: 0.1684\n",
      "Epoch [3/10], Step [700/2737], Loss: 0.2137\n",
      "Epoch [3/10], Step [710/2737], Loss: 0.3383\n",
      "Epoch [3/10], Step [720/2737], Loss: 0.2879\n",
      "Epoch [3/10], Step [730/2737], Loss: 0.1672\n",
      "Epoch [3/10], Step [740/2737], Loss: 0.2809\n",
      "Epoch [3/10], Step [750/2737], Loss: 0.2125\n",
      "Epoch [3/10], Step [760/2737], Loss: 0.1673\n",
      "Epoch [3/10], Step [770/2737], Loss: 0.2944\n",
      "Epoch [3/10], Step [780/2737], Loss: 0.2802\n",
      "Epoch [3/10], Step [790/2737], Loss: 0.2727\n",
      "Epoch [3/10], Step [800/2737], Loss: 0.3669\n",
      "Epoch [3/10], Step [810/2737], Loss: 0.3047\n",
      "Epoch [3/10], Step [820/2737], Loss: 0.3704\n",
      "Epoch [3/10], Step [830/2737], Loss: 0.2843\n",
      "Epoch [3/10], Step [840/2737], Loss: 0.3158\n",
      "Epoch [3/10], Step [850/2737], Loss: 0.2904\n",
      "Epoch [3/10], Step [860/2737], Loss: 0.2652\n",
      "Epoch [3/10], Step [870/2737], Loss: 0.3986\n",
      "Epoch [3/10], Step [880/2737], Loss: 0.2665\n",
      "Epoch [3/10], Step [890/2737], Loss: 0.3601\n",
      "Epoch [3/10], Step [900/2737], Loss: 0.2153\n",
      "Epoch [3/10], Step [910/2737], Loss: 0.2086\n",
      "Epoch [3/10], Step [920/2737], Loss: 0.2053\n",
      "Epoch [3/10], Step [930/2737], Loss: 0.1952\n",
      "Epoch [3/10], Step [940/2737], Loss: 0.0790\n",
      "Epoch [3/10], Step [950/2737], Loss: 0.3329\n",
      "Epoch [3/10], Step [960/2737], Loss: 0.3079\n",
      "Epoch [3/10], Step [970/2737], Loss: 0.3667\n",
      "Epoch [3/10], Step [980/2737], Loss: 0.2105\n",
      "Epoch [3/10], Step [990/2737], Loss: 0.2128\n",
      "Epoch [3/10], Step [1000/2737], Loss: 0.2424\n",
      "Epoch [3/10], Step [1010/2737], Loss: 0.2388\n",
      "Epoch [3/10], Step [1020/2737], Loss: 0.3508\n",
      "Epoch [3/10], Step [1030/2737], Loss: 0.3070\n",
      "Epoch [3/10], Step [1040/2737], Loss: 0.3469\n",
      "Epoch [3/10], Step [1050/2737], Loss: 0.3466\n",
      "Epoch [3/10], Step [1060/2737], Loss: 0.1860\n",
      "Epoch [3/10], Step [1070/2737], Loss: 0.3836\n",
      "Epoch [3/10], Step [1080/2737], Loss: 0.2766\n",
      "Epoch [3/10], Step [1090/2737], Loss: 0.1683\n",
      "Epoch [3/10], Step [1100/2737], Loss: 0.3776\n",
      "Epoch [3/10], Step [1110/2737], Loss: 0.2600\n",
      "Epoch [3/10], Step [1120/2737], Loss: 0.2017\n",
      "Epoch [3/10], Step [1130/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1140/2737], Loss: 0.1950\n",
      "Epoch [3/10], Step [1150/2737], Loss: 0.1993\n",
      "Epoch [3/10], Step [1160/2737], Loss: 0.1515\n",
      "Epoch [3/10], Step [1170/2737], Loss: 0.2963\n",
      "Epoch [3/10], Step [1180/2737], Loss: 0.3108\n",
      "Epoch [3/10], Step [1190/2737], Loss: 0.3467\n",
      "Epoch [3/10], Step [1200/2737], Loss: 0.3132\n",
      "Epoch [3/10], Step [1210/2737], Loss: 0.2472\n",
      "Epoch [3/10], Step [1220/2737], Loss: 0.2408\n",
      "Epoch [3/10], Step [1230/2737], Loss: 0.1885\n",
      "Epoch [3/10], Step [1240/2737], Loss: 0.2948\n",
      "Epoch [3/10], Step [1250/2737], Loss: 0.1945\n",
      "Epoch [3/10], Step [1260/2737], Loss: 0.3484\n",
      "Epoch [3/10], Step [1270/2737], Loss: 0.2122\n",
      "Epoch [3/10], Step [1280/2737], Loss: 0.2272\n",
      "Epoch [3/10], Step [1290/2737], Loss: 0.1439\n",
      "Epoch [3/10], Step [1300/2737], Loss: 0.4767\n",
      "Epoch [3/10], Step [1310/2737], Loss: 0.4168\n",
      "Epoch [3/10], Step [1320/2737], Loss: 0.3483\n",
      "Epoch [3/10], Step [1330/2737], Loss: 0.2428\n",
      "Epoch [3/10], Step [1340/2737], Loss: 0.1845\n",
      "Epoch [3/10], Step [1350/2737], Loss: 0.2360\n",
      "Epoch [3/10], Step [1360/2737], Loss: 0.2660\n",
      "Epoch [3/10], Step [1370/2737], Loss: 0.2900\n",
      "Epoch [3/10], Step [1380/2737], Loss: 0.2304\n",
      "Epoch [3/10], Step [1390/2737], Loss: 0.3351\n",
      "Epoch [3/10], Step [1400/2737], Loss: 0.1782\n",
      "Epoch [3/10], Step [1410/2737], Loss: 0.2238\n",
      "Epoch [3/10], Step [1420/2737], Loss: 0.2844\n",
      "Epoch [3/10], Step [1430/2737], Loss: 0.2154\n",
      "Epoch [3/10], Step [1440/2737], Loss: 0.3239\n",
      "Epoch [3/10], Step [1450/2737], Loss: 0.3079\n",
      "Epoch [3/10], Step [1460/2737], Loss: 0.4107\n",
      "Epoch [3/10], Step [1470/2737], Loss: 0.2216\n",
      "Epoch [3/10], Step [1480/2737], Loss: 0.3415\n",
      "Epoch [3/10], Step [1490/2737], Loss: 0.2917\n",
      "Epoch [3/10], Step [1500/2737], Loss: 0.2102\n",
      "Epoch [3/10], Step [1510/2737], Loss: 0.3804\n",
      "Epoch [3/10], Step [1520/2737], Loss: 0.3440\n",
      "Epoch [3/10], Step [1530/2737], Loss: 0.2813\n",
      "Epoch [3/10], Step [1540/2737], Loss: 0.2343\n",
      "Epoch [3/10], Step [1550/2737], Loss: 0.1991\n",
      "Epoch [3/10], Step [1560/2737], Loss: 0.2651\n",
      "Epoch [3/10], Step [1570/2737], Loss: 0.2403\n",
      "Epoch [3/10], Step [1580/2737], Loss: 0.1873\n",
      "Epoch [3/10], Step [1590/2737], Loss: 0.1672\n",
      "Epoch [3/10], Step [1600/2737], Loss: 0.3302\n",
      "Epoch [3/10], Step [1610/2737], Loss: 0.2131\n",
      "Epoch [3/10], Step [1620/2737], Loss: 0.2527\n",
      "Epoch [3/10], Step [1630/2737], Loss: 0.1473\n",
      "Epoch [3/10], Step [1640/2737], Loss: 0.2585\n",
      "Epoch [3/10], Step [1650/2737], Loss: 0.2901\n",
      "Epoch [3/10], Step [1660/2737], Loss: 0.1559\n",
      "Epoch [3/10], Step [1670/2737], Loss: 0.2076\n",
      "Epoch [3/10], Step [1680/2737], Loss: 0.2081\n",
      "Epoch [3/10], Step [1690/2737], Loss: 0.2003\n",
      "Epoch [3/10], Step [1700/2737], Loss: 0.2249\n",
      "Epoch [3/10], Step [1710/2737], Loss: 0.3678\n",
      "Epoch [3/10], Step [1720/2737], Loss: 0.2119\n",
      "Epoch [3/10], Step [1730/2737], Loss: 0.2572\n",
      "Epoch [3/10], Step [1740/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1750/2737], Loss: 0.2143\n",
      "Epoch [3/10], Step [1760/2737], Loss: 0.3421\n",
      "Epoch [3/10], Step [1770/2737], Loss: 0.2232\n",
      "Epoch [3/10], Step [1780/2737], Loss: 0.2711\n",
      "Epoch [3/10], Step [1790/2737], Loss: 0.3006\n",
      "Epoch [3/10], Step [1800/2737], Loss: 0.2784\n",
      "Epoch [3/10], Step [1810/2737], Loss: 0.3540\n",
      "Epoch [3/10], Step [1820/2737], Loss: 0.3274\n",
      "Epoch [3/10], Step [1830/2737], Loss: 0.1926\n",
      "Epoch [3/10], Step [1840/2737], Loss: 0.2195\n",
      "Epoch [3/10], Step [1850/2737], Loss: 0.3931\n",
      "Epoch [3/10], Step [1860/2737], Loss: 0.2293\n",
      "Epoch [3/10], Step [1870/2737], Loss: 0.3100\n",
      "Epoch [3/10], Step [1880/2737], Loss: 0.1571\n",
      "Epoch [3/10], Step [1890/2737], Loss: 0.3826\n",
      "Epoch [3/10], Step [1900/2737], Loss: 0.3410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [1910/2737], Loss: 0.2496\n",
      "Epoch [3/10], Step [1920/2737], Loss: 0.3327\n",
      "Epoch [3/10], Step [1930/2737], Loss: 0.2223\n",
      "Epoch [3/10], Step [1940/2737], Loss: 0.1943\n",
      "Epoch [3/10], Step [1950/2737], Loss: 0.2297\n",
      "Epoch [3/10], Step [1960/2737], Loss: 0.2605\n",
      "Epoch [3/10], Step [1970/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1980/2737], Loss: 0.1267\n",
      "Epoch [3/10], Step [1990/2737], Loss: 0.2843\n",
      "Epoch [3/10], Step [2000/2737], Loss: 0.2725\n",
      "Epoch [3/10], Step [2010/2737], Loss: 0.2230\n",
      "Epoch [3/10], Step [2020/2737], Loss: 0.3268\n",
      "Epoch [3/10], Step [2030/2737], Loss: 0.2588\n",
      "Epoch [3/10], Step [2040/2737], Loss: 0.2651\n",
      "Epoch [3/10], Step [2050/2737], Loss: 0.2929\n",
      "Epoch [3/10], Step [2060/2737], Loss: 0.2330\n",
      "Epoch [3/10], Step [2070/2737], Loss: 0.1868\n",
      "Epoch [3/10], Step [2080/2737], Loss: 0.1871\n",
      "Epoch [3/10], Step [2090/2737], Loss: 0.2948\n",
      "Epoch [3/10], Step [2100/2737], Loss: 0.2231\n",
      "Epoch [3/10], Step [2110/2737], Loss: 0.3068\n",
      "Epoch [3/10], Step [2120/2737], Loss: 0.2099\n",
      "Epoch [3/10], Step [2130/2737], Loss: 0.1532\n",
      "Epoch [3/10], Step [2140/2737], Loss: 0.3230\n",
      "Epoch [3/10], Step [2150/2737], Loss: 0.2237\n",
      "Epoch [3/10], Step [2160/2737], Loss: 0.2665\n",
      "Epoch [3/10], Step [2170/2737], Loss: 0.2069\n",
      "Epoch [3/10], Step [2180/2737], Loss: 0.1532\n",
      "Epoch [3/10], Step [2190/2737], Loss: 0.3616\n",
      "Epoch [3/10], Step [2200/2737], Loss: 0.2379\n",
      "Epoch [3/10], Step [2210/2737], Loss: 0.3390\n",
      "Epoch [3/10], Step [2220/2737], Loss: 0.5115\n",
      "Epoch [3/10], Step [2230/2737], Loss: 0.2745\n",
      "Epoch [3/10], Step [2240/2737], Loss: 0.2779\n",
      "Epoch [3/10], Step [2250/2737], Loss: 0.2110\n",
      "Epoch [3/10], Step [2260/2737], Loss: 0.2473\n",
      "Epoch [3/10], Step [2270/2737], Loss: 0.3654\n",
      "Epoch [3/10], Step [2280/2737], Loss: 0.2325\n",
      "Epoch [3/10], Step [2290/2737], Loss: 0.1744\n",
      "Epoch [3/10], Step [2300/2737], Loss: 0.2851\n",
      "Epoch [3/10], Step [2310/2737], Loss: 0.2434\n",
      "Epoch [3/10], Step [2320/2737], Loss: 0.1793\n",
      "Epoch [3/10], Step [2330/2737], Loss: 0.3277\n",
      "Epoch [3/10], Step [2340/2737], Loss: 0.3020\n",
      "Epoch [3/10], Step [2350/2737], Loss: 0.2339\n",
      "Epoch [3/10], Step [2360/2737], Loss: 0.2239\n",
      "Epoch [3/10], Step [2370/2737], Loss: 0.2578\n",
      "Epoch [3/10], Step [2380/2737], Loss: 0.1928\n",
      "Epoch [3/10], Step [2390/2737], Loss: 0.3699\n",
      "Epoch [3/10], Step [2400/2737], Loss: 0.3122\n",
      "Epoch [3/10], Step [2410/2737], Loss: 0.1850\n",
      "Epoch [3/10], Step [2420/2737], Loss: 0.3510\n",
      "Epoch [3/10], Step [2430/2737], Loss: 0.2469\n",
      "Epoch [3/10], Step [2440/2737], Loss: 0.3292\n",
      "Epoch [3/10], Step [2450/2737], Loss: 0.3481\n",
      "Epoch [3/10], Step [2460/2737], Loss: 0.3791\n",
      "Epoch [3/10], Step [2470/2737], Loss: 0.3122\n",
      "Epoch [3/10], Step [2480/2737], Loss: 0.2950\n",
      "Epoch [3/10], Step [2490/2737], Loss: 0.2983\n",
      "Epoch [3/10], Step [2500/2737], Loss: 0.3030\n",
      "Epoch [3/10], Step [2510/2737], Loss: 0.3164\n",
      "Epoch [3/10], Step [2520/2737], Loss: 0.2375\n",
      "Epoch [3/10], Step [2530/2737], Loss: 0.2933\n",
      "Epoch [3/10], Step [2540/2737], Loss: 0.2120\n",
      "Epoch [3/10], Step [2550/2737], Loss: 0.2050\n",
      "Epoch [3/10], Step [2560/2737], Loss: 0.2469\n",
      "Epoch [3/10], Step [2570/2737], Loss: 0.4300\n",
      "Epoch [3/10], Step [2580/2737], Loss: 0.2299\n",
      "Epoch [3/10], Step [2590/2737], Loss: 0.2560\n",
      "Epoch [3/10], Step [2600/2737], Loss: 0.2268\n",
      "Epoch [3/10], Step [2610/2737], Loss: 0.3580\n",
      "Epoch [3/10], Step [2620/2737], Loss: 0.2245\n",
      "Epoch [3/10], Step [2630/2737], Loss: 0.2408\n",
      "Epoch [3/10], Step [2640/2737], Loss: 0.2720\n",
      "Epoch [3/10], Step [2650/2737], Loss: 0.2562\n",
      "Epoch [3/10], Step [2660/2737], Loss: 0.2422\n",
      "Epoch [3/10], Step [2670/2737], Loss: 0.2215\n",
      "Epoch [3/10], Step [2680/2737], Loss: 0.1819\n",
      "Epoch [3/10], Step [2690/2737], Loss: 0.2772\n",
      "Epoch [3/10], Step [2700/2737], Loss: 0.3313\n",
      "Epoch [3/10], Step [2710/2737], Loss: 0.2622\n",
      "Epoch [3/10], Step [2720/2737], Loss: 0.3202\n",
      "Epoch [3/10], Step [2730/2737], Loss: 0.1857\n",
      "Epoch [3/10], train_loss: 0.2641, val_loss: 0.2491\n",
      "Epoch [4/10], Step [10/2737], Loss: 0.2554\n",
      "Epoch [4/10], Step [20/2737], Loss: 0.2359\n",
      "Epoch [4/10], Step [30/2737], Loss: 0.2434\n",
      "Epoch [4/10], Step [40/2737], Loss: 0.2190\n",
      "Epoch [4/10], Step [50/2737], Loss: 0.1885\n",
      "Epoch [4/10], Step [60/2737], Loss: 0.2695\n",
      "Epoch [4/10], Step [70/2737], Loss: 0.1699\n",
      "Epoch [4/10], Step [80/2737], Loss: 0.1467\n",
      "Epoch [4/10], Step [90/2737], Loss: 0.3995\n",
      "Epoch [4/10], Step [100/2737], Loss: 0.2748\n",
      "Epoch [4/10], Step [110/2737], Loss: 0.2573\n",
      "Epoch [4/10], Step [120/2737], Loss: 0.2871\n",
      "Epoch [4/10], Step [130/2737], Loss: 0.3748\n",
      "Epoch [4/10], Step [140/2737], Loss: 0.2859\n",
      "Epoch [4/10], Step [150/2737], Loss: 0.2657\n",
      "Epoch [4/10], Step [160/2737], Loss: 0.1993\n",
      "Epoch [4/10], Step [170/2737], Loss: 0.1865\n",
      "Epoch [4/10], Step [180/2737], Loss: 0.2115\n",
      "Epoch [4/10], Step [190/2737], Loss: 0.2330\n",
      "Epoch [4/10], Step [200/2737], Loss: 0.2117\n",
      "Epoch [4/10], Step [210/2737], Loss: 0.2022\n",
      "Epoch [4/10], Step [220/2737], Loss: 0.2166\n",
      "Epoch [4/10], Step [230/2737], Loss: 0.1876\n",
      "Epoch [4/10], Step [240/2737], Loss: 0.2339\n",
      "Epoch [4/10], Step [250/2737], Loss: 0.1915\n",
      "Epoch [4/10], Step [260/2737], Loss: 0.2235\n",
      "Epoch [4/10], Step [270/2737], Loss: 0.2271\n",
      "Epoch [4/10], Step [280/2737], Loss: 0.2586\n",
      "Epoch [4/10], Step [290/2737], Loss: 0.2522\n",
      "Epoch [4/10], Step [300/2737], Loss: 0.3513\n",
      "Epoch [4/10], Step [310/2737], Loss: 0.2835\n",
      "Epoch [4/10], Step [320/2737], Loss: 0.1705\n",
      "Epoch [4/10], Step [330/2737], Loss: 0.2028\n",
      "Epoch [4/10], Step [340/2737], Loss: 0.2862\n",
      "Epoch [4/10], Step [350/2737], Loss: 0.2339\n",
      "Epoch [4/10], Step [360/2737], Loss: 0.2466\n",
      "Epoch [4/10], Step [370/2737], Loss: 0.1813\n",
      "Epoch [4/10], Step [380/2737], Loss: 0.2844\n",
      "Epoch [4/10], Step [390/2737], Loss: 0.3292\n",
      "Epoch [4/10], Step [400/2737], Loss: 0.3282\n",
      "Epoch [4/10], Step [410/2737], Loss: 0.3365\n",
      "Epoch [4/10], Step [420/2737], Loss: 0.1699\n",
      "Epoch [4/10], Step [430/2737], Loss: 0.2450\n",
      "Epoch [4/10], Step [440/2737], Loss: 0.1856\n",
      "Epoch [4/10], Step [450/2737], Loss: 0.2593\n",
      "Epoch [4/10], Step [460/2737], Loss: 0.3263\n",
      "Epoch [4/10], Step [470/2737], Loss: 0.3748\n",
      "Epoch [4/10], Step [480/2737], Loss: 0.2041\n",
      "Epoch [4/10], Step [490/2737], Loss: 0.1654\n",
      "Epoch [4/10], Step [500/2737], Loss: 0.2834\n",
      "Epoch [4/10], Step [510/2737], Loss: 0.2629\n",
      "Epoch [4/10], Step [520/2737], Loss: 0.1769\n",
      "Epoch [4/10], Step [530/2737], Loss: 0.3588\n",
      "Epoch [4/10], Step [540/2737], Loss: 0.2211\n",
      "Epoch [4/10], Step [550/2737], Loss: 0.1943\n",
      "Epoch [4/10], Step [560/2737], Loss: 0.3445\n",
      "Epoch [4/10], Step [570/2737], Loss: 0.1793\n",
      "Epoch [4/10], Step [580/2737], Loss: 0.2438\n",
      "Epoch [4/10], Step [590/2737], Loss: 0.2776\n",
      "Epoch [4/10], Step [600/2737], Loss: 0.1618\n",
      "Epoch [4/10], Step [610/2737], Loss: 0.2334\n",
      "Epoch [4/10], Step [620/2737], Loss: 0.2615\n",
      "Epoch [4/10], Step [630/2737], Loss: 0.1887\n",
      "Epoch [4/10], Step [640/2737], Loss: 0.3008\n",
      "Epoch [4/10], Step [650/2737], Loss: 0.2543\n",
      "Epoch [4/10], Step [660/2737], Loss: 0.1566\n",
      "Epoch [4/10], Step [670/2737], Loss: 0.2719\n",
      "Epoch [4/10], Step [680/2737], Loss: 0.2484\n",
      "Epoch [4/10], Step [690/2737], Loss: 0.2107\n",
      "Epoch [4/10], Step [700/2737], Loss: 0.2611\n",
      "Epoch [4/10], Step [710/2737], Loss: 0.2708\n",
      "Epoch [4/10], Step [720/2737], Loss: 0.3045\n",
      "Epoch [4/10], Step [730/2737], Loss: 0.2660\n",
      "Epoch [4/10], Step [740/2737], Loss: 0.1867\n",
      "Epoch [4/10], Step [750/2737], Loss: 0.2346\n",
      "Epoch [4/10], Step [760/2737], Loss: 0.2035\n",
      "Epoch [4/10], Step [770/2737], Loss: 0.2539\n",
      "Epoch [4/10], Step [780/2737], Loss: 0.2323\n",
      "Epoch [4/10], Step [790/2737], Loss: 0.1811\n",
      "Epoch [4/10], Step [800/2737], Loss: 0.2130\n",
      "Epoch [4/10], Step [810/2737], Loss: 0.2779\n",
      "Epoch [4/10], Step [820/2737], Loss: 0.2106\n",
      "Epoch [4/10], Step [830/2737], Loss: 0.2814\n",
      "Epoch [4/10], Step [840/2737], Loss: 0.1882\n",
      "Epoch [4/10], Step [850/2737], Loss: 0.2407\n",
      "Epoch [4/10], Step [860/2737], Loss: 0.2799\n",
      "Epoch [4/10], Step [870/2737], Loss: 0.1944\n",
      "Epoch [4/10], Step [880/2737], Loss: 0.3110\n",
      "Epoch [4/10], Step [890/2737], Loss: 0.2944\n",
      "Epoch [4/10], Step [900/2737], Loss: 0.1750\n",
      "Epoch [4/10], Step [910/2737], Loss: 0.2288\n",
      "Epoch [4/10], Step [920/2737], Loss: 0.3645\n",
      "Epoch [4/10], Step [930/2737], Loss: 0.2696\n",
      "Epoch [4/10], Step [940/2737], Loss: 0.2800\n",
      "Epoch [4/10], Step [950/2737], Loss: 0.1528\n",
      "Epoch [4/10], Step [960/2737], Loss: 0.1900\n",
      "Epoch [4/10], Step [970/2737], Loss: 0.2992\n",
      "Epoch [4/10], Step [980/2737], Loss: 0.2601\n",
      "Epoch [4/10], Step [990/2737], Loss: 0.2658\n",
      "Epoch [4/10], Step [1000/2737], Loss: 0.3921\n",
      "Epoch [4/10], Step [1010/2737], Loss: 0.2342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [1020/2737], Loss: 0.2035\n",
      "Epoch [4/10], Step [1030/2737], Loss: 0.2174\n",
      "Epoch [4/10], Step [1040/2737], Loss: 0.2355\n",
      "Epoch [4/10], Step [1050/2737], Loss: 0.2862\n",
      "Epoch [4/10], Step [1060/2737], Loss: 0.2165\n",
      "Epoch [4/10], Step [1070/2737], Loss: 0.2105\n",
      "Epoch [4/10], Step [1080/2737], Loss: 0.1819\n",
      "Epoch [4/10], Step [1090/2737], Loss: 0.1999\n",
      "Epoch [4/10], Step [1100/2737], Loss: 0.2083\n",
      "Epoch [4/10], Step [1110/2737], Loss: 0.2109\n",
      "Epoch [4/10], Step [1120/2737], Loss: 0.2611\n",
      "Epoch [4/10], Step [1130/2737], Loss: 0.3400\n",
      "Epoch [4/10], Step [1140/2737], Loss: 0.1697\n",
      "Epoch [4/10], Step [1150/2737], Loss: 0.2786\n",
      "Epoch [4/10], Step [1160/2737], Loss: 0.2589\n",
      "Epoch [4/10], Step [1170/2737], Loss: 0.2785\n",
      "Epoch [4/10], Step [1180/2737], Loss: 0.2205\n",
      "Epoch [4/10], Step [1190/2737], Loss: 0.2235\n",
      "Epoch [4/10], Step [1200/2737], Loss: 0.3073\n",
      "Epoch [4/10], Step [1210/2737], Loss: 0.2322\n",
      "Epoch [4/10], Step [1220/2737], Loss: 0.2220\n",
      "Epoch [4/10], Step [1230/2737], Loss: 0.2056\n",
      "Epoch [4/10], Step [1240/2737], Loss: 0.1298\n",
      "Epoch [4/10], Step [1250/2737], Loss: 0.2443\n",
      "Epoch [4/10], Step [1260/2737], Loss: 0.1538\n",
      "Epoch [4/10], Step [1270/2737], Loss: 0.2436\n",
      "Epoch [4/10], Step [1280/2737], Loss: 0.3210\n",
      "Epoch [4/10], Step [1290/2737], Loss: 0.2475\n",
      "Epoch [4/10], Step [1300/2737], Loss: 0.2402\n",
      "Epoch [4/10], Step [1310/2737], Loss: 0.3219\n",
      "Epoch [4/10], Step [1320/2737], Loss: 0.2993\n",
      "Epoch [4/10], Step [1330/2737], Loss: 0.2356\n",
      "Epoch [4/10], Step [1340/2737], Loss: 0.1945\n",
      "Epoch [4/10], Step [1350/2737], Loss: 0.2716\n",
      "Epoch [4/10], Step [1360/2737], Loss: 0.3257\n",
      "Epoch [4/10], Step [1370/2737], Loss: 0.1692\n",
      "Epoch [4/10], Step [1380/2737], Loss: 0.2602\n",
      "Epoch [4/10], Step [1390/2737], Loss: 0.2240\n",
      "Epoch [4/10], Step [1400/2737], Loss: 0.1166\n",
      "Epoch [4/10], Step [1410/2737], Loss: 0.3073\n",
      "Epoch [4/10], Step [1420/2737], Loss: 0.2177\n",
      "Epoch [4/10], Step [1430/2737], Loss: 0.2914\n",
      "Epoch [4/10], Step [1440/2737], Loss: 0.2204\n",
      "Epoch [4/10], Step [1450/2737], Loss: 0.2941\n",
      "Epoch [4/10], Step [1460/2737], Loss: 0.1786\n",
      "Epoch [4/10], Step [1470/2737], Loss: 0.3096\n",
      "Epoch [4/10], Step [1480/2737], Loss: 0.2193\n",
      "Epoch [4/10], Step [1490/2737], Loss: 0.1734\n",
      "Epoch [4/10], Step [1500/2737], Loss: 0.3091\n",
      "Epoch [4/10], Step [1510/2737], Loss: 0.1922\n",
      "Epoch [4/10], Step [1520/2737], Loss: 0.2198\n",
      "Epoch [4/10], Step [1530/2737], Loss: 0.1598\n",
      "Epoch [4/10], Step [1540/2737], Loss: 0.3466\n",
      "Epoch [4/10], Step [1550/2737], Loss: 0.1269\n",
      "Epoch [4/10], Step [1560/2737], Loss: 0.3752\n",
      "Epoch [4/10], Step [1570/2737], Loss: 0.3126\n",
      "Epoch [4/10], Step [1580/2737], Loss: 0.3225\n",
      "Epoch [4/10], Step [1590/2737], Loss: 0.2842\n",
      "Epoch [4/10], Step [1600/2737], Loss: 0.1766\n",
      "Epoch [4/10], Step [1610/2737], Loss: 0.3023\n",
      "Epoch [4/10], Step [1620/2737], Loss: 0.2315\n",
      "Epoch [4/10], Step [1630/2737], Loss: 0.3884\n",
      "Epoch [4/10], Step [1640/2737], Loss: 0.1848\n",
      "Epoch [4/10], Step [1650/2737], Loss: 0.3060\n",
      "Epoch [4/10], Step [1660/2737], Loss: 0.2333\n",
      "Epoch [4/10], Step [1670/2737], Loss: 0.1670\n",
      "Epoch [4/10], Step [1680/2737], Loss: 0.2592\n",
      "Epoch [4/10], Step [1690/2737], Loss: 0.2547\n",
      "Epoch [4/10], Step [1700/2737], Loss: 0.1882\n",
      "Epoch [4/10], Step [1710/2737], Loss: 0.2744\n",
      "Epoch [4/10], Step [1720/2737], Loss: 0.3068\n",
      "Epoch [4/10], Step [1730/2737], Loss: 0.3498\n",
      "Epoch [4/10], Step [1740/2737], Loss: 0.3035\n",
      "Epoch [4/10], Step [1750/2737], Loss: 0.1611\n",
      "Epoch [4/10], Step [1760/2737], Loss: 0.2349\n",
      "Epoch [4/10], Step [1770/2737], Loss: 0.2413\n",
      "Epoch [4/10], Step [1780/2737], Loss: 0.1678\n",
      "Epoch [4/10], Step [1790/2737], Loss: 0.3053\n",
      "Epoch [4/10], Step [1800/2737], Loss: 0.2489\n",
      "Epoch [4/10], Step [1810/2737], Loss: 0.3003\n",
      "Epoch [4/10], Step [1820/2737], Loss: 0.2067\n",
      "Epoch [4/10], Step [1830/2737], Loss: 0.2178\n",
      "Epoch [4/10], Step [1840/2737], Loss: 0.2557\n",
      "Epoch [4/10], Step [1850/2737], Loss: 0.2291\n",
      "Epoch [4/10], Step [1860/2737], Loss: 0.1497\n",
      "Epoch [4/10], Step [1870/2737], Loss: 0.2739\n",
      "Epoch [4/10], Step [1880/2737], Loss: 0.2822\n",
      "Epoch [4/10], Step [1890/2737], Loss: 0.2359\n",
      "Epoch [4/10], Step [1900/2737], Loss: 0.2004\n",
      "Epoch [4/10], Step [1910/2737], Loss: 0.1270\n",
      "Epoch [4/10], Step [1920/2737], Loss: 0.2289\n",
      "Epoch [4/10], Step [1930/2737], Loss: 0.2782\n",
      "Epoch [4/10], Step [1940/2737], Loss: 0.2841\n",
      "Epoch [4/10], Step [1950/2737], Loss: 0.2231\n",
      "Epoch [4/10], Step [1960/2737], Loss: 0.2614\n",
      "Epoch [4/10], Step [1970/2737], Loss: 0.2762\n",
      "Epoch [4/10], Step [1980/2737], Loss: 0.2729\n",
      "Epoch [4/10], Step [1990/2737], Loss: 0.3491\n",
      "Epoch [4/10], Step [2000/2737], Loss: 0.2688\n",
      "Epoch [4/10], Step [2010/2737], Loss: 0.1739\n",
      "Epoch [4/10], Step [2020/2737], Loss: 0.1481\n",
      "Epoch [4/10], Step [2030/2737], Loss: 0.2758\n",
      "Epoch [4/10], Step [2040/2737], Loss: 0.3145\n",
      "Epoch [4/10], Step [2050/2737], Loss: 0.2169\n",
      "Epoch [4/10], Step [2060/2737], Loss: 0.3111\n",
      "Epoch [4/10], Step [2070/2737], Loss: 0.3245\n",
      "Epoch [4/10], Step [2080/2737], Loss: 0.2453\n",
      "Epoch [4/10], Step [2090/2737], Loss: 0.2302\n",
      "Epoch [4/10], Step [2100/2737], Loss: 0.5206\n",
      "Epoch [4/10], Step [2110/2737], Loss: 0.3612\n",
      "Epoch [4/10], Step [2120/2737], Loss: 0.3326\n",
      "Epoch [4/10], Step [2130/2737], Loss: 0.2513\n",
      "Epoch [4/10], Step [2140/2737], Loss: 0.2297\n",
      "Epoch [4/10], Step [2150/2737], Loss: 0.2209\n",
      "Epoch [4/10], Step [2160/2737], Loss: 0.1177\n",
      "Epoch [4/10], Step [2170/2737], Loss: 0.2822\n",
      "Epoch [4/10], Step [2180/2737], Loss: 0.2089\n",
      "Epoch [4/10], Step [2190/2737], Loss: 0.2206\n",
      "Epoch [4/10], Step [2200/2737], Loss: 0.4167\n",
      "Epoch [4/10], Step [2210/2737], Loss: 0.2203\n",
      "Epoch [4/10], Step [2220/2737], Loss: 0.2796\n",
      "Epoch [4/10], Step [2230/2737], Loss: 0.2547\n",
      "Epoch [4/10], Step [2240/2737], Loss: 0.4792\n",
      "Epoch [4/10], Step [2250/2737], Loss: 0.1460\n",
      "Epoch [4/10], Step [2260/2737], Loss: 0.2228\n",
      "Epoch [4/10], Step [2270/2737], Loss: 0.2047\n",
      "Epoch [4/10], Step [2280/2737], Loss: 0.4087\n",
      "Epoch [4/10], Step [2290/2737], Loss: 0.3161\n",
      "Epoch [4/10], Step [2300/2737], Loss: 0.1940\n",
      "Epoch [4/10], Step [2310/2737], Loss: 0.2777\n",
      "Epoch [4/10], Step [2320/2737], Loss: 0.2468\n",
      "Epoch [4/10], Step [2330/2737], Loss: 0.2603\n",
      "Epoch [4/10], Step [2340/2737], Loss: 0.1447\n",
      "Epoch [4/10], Step [2350/2737], Loss: 0.1757\n",
      "Epoch [4/10], Step [2360/2737], Loss: 0.2234\n",
      "Epoch [4/10], Step [2370/2737], Loss: 0.3188\n",
      "Epoch [4/10], Step [2380/2737], Loss: 0.2005\n",
      "Epoch [4/10], Step [2390/2737], Loss: 0.2826\n",
      "Epoch [4/10], Step [2400/2737], Loss: 0.1750\n",
      "Epoch [4/10], Step [2410/2737], Loss: 0.3469\n",
      "Epoch [4/10], Step [2420/2737], Loss: 0.1581\n",
      "Epoch [4/10], Step [2430/2737], Loss: 0.1807\n",
      "Epoch [4/10], Step [2440/2737], Loss: 0.1911\n",
      "Epoch [4/10], Step [2450/2737], Loss: 0.2965\n",
      "Epoch [4/10], Step [2460/2737], Loss: 0.2809\n",
      "Epoch [4/10], Step [2470/2737], Loss: 0.2230\n",
      "Epoch [4/10], Step [2480/2737], Loss: 0.2592\n",
      "Epoch [4/10], Step [2490/2737], Loss: 0.2327\n",
      "Epoch [4/10], Step [2500/2737], Loss: 0.1557\n",
      "Epoch [4/10], Step [2510/2737], Loss: 0.2340\n",
      "Epoch [4/10], Step [2520/2737], Loss: 0.2390\n",
      "Epoch [4/10], Step [2530/2737], Loss: 0.3193\n",
      "Epoch [4/10], Step [2540/2737], Loss: 0.2763\n",
      "Epoch [4/10], Step [2550/2737], Loss: 0.1961\n",
      "Epoch [4/10], Step [2560/2737], Loss: 0.1762\n",
      "Epoch [4/10], Step [2570/2737], Loss: 0.1822\n",
      "Epoch [4/10], Step [2580/2737], Loss: 0.3754\n",
      "Epoch [4/10], Step [2590/2737], Loss: 0.3234\n",
      "Epoch [4/10], Step [2600/2737], Loss: 0.3397\n",
      "Epoch [4/10], Step [2610/2737], Loss: 0.2288\n",
      "Epoch [4/10], Step [2620/2737], Loss: 0.1669\n",
      "Epoch [4/10], Step [2630/2737], Loss: 0.1332\n",
      "Epoch [4/10], Step [2640/2737], Loss: 0.2457\n",
      "Epoch [4/10], Step [2650/2737], Loss: 0.2364\n",
      "Epoch [4/10], Step [2660/2737], Loss: 0.1919\n",
      "Epoch [4/10], Step [2670/2737], Loss: 0.4178\n",
      "Epoch [4/10], Step [2680/2737], Loss: 0.1555\n",
      "Epoch [4/10], Step [2690/2737], Loss: 0.0864\n",
      "Epoch [4/10], Step [2700/2737], Loss: 0.3297\n",
      "Epoch [4/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [4/10], Step [2720/2737], Loss: 0.2807\n",
      "Epoch [4/10], Step [2730/2737], Loss: 0.2513\n",
      "Epoch [4/10], train_loss: 0.2542, val_loss: 0.2580\n",
      "Epoch [5/10], Step [10/2737], Loss: 0.2138\n",
      "Epoch [5/10], Step [20/2737], Loss: 0.3143\n",
      "Epoch [5/10], Step [30/2737], Loss: 0.3769\n",
      "Epoch [5/10], Step [40/2737], Loss: 0.1672\n",
      "Epoch [5/10], Step [50/2737], Loss: 0.3064\n",
      "Epoch [5/10], Step [60/2737], Loss: 0.2459\n",
      "Epoch [5/10], Step [70/2737], Loss: 0.1879\n",
      "Epoch [5/10], Step [80/2737], Loss: 0.2945\n",
      "Epoch [5/10], Step [90/2737], Loss: 0.1855\n",
      "Epoch [5/10], Step [100/2737], Loss: 0.2786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [110/2737], Loss: 0.2059\n",
      "Epoch [5/10], Step [120/2737], Loss: 0.2566\n",
      "Epoch [5/10], Step [130/2737], Loss: 0.3254\n",
      "Epoch [5/10], Step [140/2737], Loss: 0.2996\n",
      "Epoch [5/10], Step [150/2737], Loss: 0.1725\n",
      "Epoch [5/10], Step [160/2737], Loss: 0.3611\n",
      "Epoch [5/10], Step [170/2737], Loss: 0.2006\n",
      "Epoch [5/10], Step [180/2737], Loss: 0.1974\n",
      "Epoch [5/10], Step [190/2737], Loss: 0.2512\n",
      "Epoch [5/10], Step [200/2737], Loss: 0.2975\n",
      "Epoch [5/10], Step [210/2737], Loss: 0.3245\n",
      "Epoch [5/10], Step [220/2737], Loss: 0.1999\n",
      "Epoch [5/10], Step [230/2737], Loss: 0.1557\n",
      "Epoch [5/10], Step [240/2737], Loss: 0.2195\n",
      "Epoch [5/10], Step [250/2737], Loss: 0.3012\n",
      "Epoch [5/10], Step [260/2737], Loss: 0.2492\n",
      "Epoch [5/10], Step [270/2737], Loss: 0.2987\n",
      "Epoch [5/10], Step [280/2737], Loss: 0.2235\n",
      "Epoch [5/10], Step [290/2737], Loss: 0.1898\n",
      "Epoch [5/10], Step [300/2737], Loss: 0.3154\n",
      "Epoch [5/10], Step [310/2737], Loss: 0.1528\n",
      "Epoch [5/10], Step [320/2737], Loss: 0.3160\n",
      "Epoch [5/10], Step [330/2737], Loss: 0.1782\n",
      "Epoch [5/10], Step [340/2737], Loss: 0.3089\n",
      "Epoch [5/10], Step [350/2737], Loss: 0.1910\n",
      "Epoch [5/10], Step [360/2737], Loss: 0.2240\n",
      "Epoch [5/10], Step [370/2737], Loss: 0.3146\n",
      "Epoch [5/10], Step [380/2737], Loss: 0.2761\n",
      "Epoch [5/10], Step [390/2737], Loss: 0.1950\n",
      "Epoch [5/10], Step [400/2737], Loss: 0.3579\n",
      "Epoch [5/10], Step [410/2737], Loss: 0.2278\n",
      "Epoch [5/10], Step [420/2737], Loss: 0.2221\n",
      "Epoch [5/10], Step [430/2737], Loss: 0.1270\n",
      "Epoch [5/10], Step [440/2737], Loss: 0.1526\n",
      "Epoch [5/10], Step [450/2737], Loss: 0.2150\n",
      "Epoch [5/10], Step [460/2737], Loss: 0.2334\n",
      "Epoch [5/10], Step [470/2737], Loss: 0.3908\n",
      "Epoch [5/10], Step [480/2737], Loss: 0.2114\n",
      "Epoch [5/10], Step [490/2737], Loss: 0.1848\n",
      "Epoch [5/10], Step [500/2737], Loss: 0.2933\n",
      "Epoch [5/10], Step [510/2737], Loss: 0.2520\n",
      "Epoch [5/10], Step [520/2737], Loss: 0.1979\n",
      "Epoch [5/10], Step [530/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [540/2737], Loss: 0.2786\n",
      "Epoch [5/10], Step [550/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [560/2737], Loss: 0.1831\n",
      "Epoch [5/10], Step [570/2737], Loss: 0.1742\n",
      "Epoch [5/10], Step [580/2737], Loss: 0.1743\n",
      "Epoch [5/10], Step [590/2737], Loss: 0.2465\n",
      "Epoch [5/10], Step [600/2737], Loss: 0.3461\n",
      "Epoch [5/10], Step [610/2737], Loss: 0.2334\n",
      "Epoch [5/10], Step [620/2737], Loss: 0.2697\n",
      "Epoch [5/10], Step [630/2737], Loss: 0.2714\n",
      "Epoch [5/10], Step [640/2737], Loss: 0.3149\n",
      "Epoch [5/10], Step [650/2737], Loss: 0.2563\n",
      "Epoch [5/10], Step [660/2737], Loss: 0.1851\n",
      "Epoch [5/10], Step [670/2737], Loss: 0.2095\n",
      "Epoch [5/10], Step [680/2737], Loss: 0.2314\n",
      "Epoch [5/10], Step [690/2737], Loss: 0.2697\n",
      "Epoch [5/10], Step [700/2737], Loss: 0.2157\n",
      "Epoch [5/10], Step [710/2737], Loss: 0.1976\n",
      "Epoch [5/10], Step [720/2737], Loss: 0.1553\n",
      "Epoch [5/10], Step [730/2737], Loss: 0.1741\n",
      "Epoch [5/10], Step [740/2737], Loss: 0.2149\n",
      "Epoch [5/10], Step [750/2737], Loss: 0.2458\n",
      "Epoch [5/10], Step [760/2737], Loss: 0.2404\n",
      "Epoch [5/10], Step [770/2737], Loss: 0.1647\n",
      "Epoch [5/10], Step [780/2737], Loss: 0.2524\n",
      "Epoch [5/10], Step [790/2737], Loss: 0.2297\n",
      "Epoch [5/10], Step [800/2737], Loss: 0.2642\n",
      "Epoch [5/10], Step [810/2737], Loss: 0.3220\n",
      "Epoch [5/10], Step [820/2737], Loss: 0.1634\n",
      "Epoch [5/10], Step [830/2737], Loss: 0.2018\n",
      "Epoch [5/10], Step [840/2737], Loss: 0.3159\n",
      "Epoch [5/10], Step [850/2737], Loss: 0.2041\n",
      "Epoch [5/10], Step [860/2737], Loss: 0.2400\n",
      "Epoch [5/10], Step [870/2737], Loss: 0.3107\n",
      "Epoch [5/10], Step [880/2737], Loss: 0.3802\n",
      "Epoch [5/10], Step [890/2737], Loss: 0.2965\n",
      "Epoch [5/10], Step [900/2737], Loss: 0.2049\n",
      "Epoch [5/10], Step [910/2737], Loss: 0.2369\n",
      "Epoch [5/10], Step [920/2737], Loss: 0.3160\n",
      "Epoch [5/10], Step [930/2737], Loss: 0.1780\n",
      "Epoch [5/10], Step [940/2737], Loss: 0.1988\n",
      "Epoch [5/10], Step [950/2737], Loss: 0.2191\n",
      "Epoch [5/10], Step [960/2737], Loss: 0.3614\n",
      "Epoch [5/10], Step [970/2737], Loss: 0.1545\n",
      "Epoch [5/10], Step [980/2737], Loss: 0.2005\n",
      "Epoch [5/10], Step [990/2737], Loss: 0.3368\n",
      "Epoch [5/10], Step [1000/2737], Loss: 0.1860\n",
      "Epoch [5/10], Step [1010/2737], Loss: 0.2706\n",
      "Epoch [5/10], Step [1020/2737], Loss: 0.2504\n",
      "Epoch [5/10], Step [1030/2737], Loss: 0.2169\n",
      "Epoch [5/10], Step [1040/2737], Loss: 0.2674\n",
      "Epoch [5/10], Step [1050/2737], Loss: 0.2996\n",
      "Epoch [5/10], Step [1060/2737], Loss: 0.2125\n",
      "Epoch [5/10], Step [1070/2737], Loss: 0.2466\n",
      "Epoch [5/10], Step [1080/2737], Loss: 0.1739\n",
      "Epoch [5/10], Step [1090/2737], Loss: 0.2635\n",
      "Epoch [5/10], Step [1100/2737], Loss: 0.2433\n",
      "Epoch [5/10], Step [1110/2737], Loss: 0.2189\n",
      "Epoch [5/10], Step [1120/2737], Loss: 0.2007\n",
      "Epoch [5/10], Step [1130/2737], Loss: 0.1910\n",
      "Epoch [5/10], Step [1140/2737], Loss: 0.2203\n",
      "Epoch [5/10], Step [1150/2737], Loss: 0.1879\n",
      "Epoch [5/10], Step [1160/2737], Loss: 0.3532\n",
      "Epoch [5/10], Step [1170/2737], Loss: 0.4066\n",
      "Epoch [5/10], Step [1180/2737], Loss: 0.2145\n",
      "Epoch [5/10], Step [1190/2737], Loss: 0.2938\n",
      "Epoch [5/10], Step [1200/2737], Loss: 0.3332\n",
      "Epoch [5/10], Step [1210/2737], Loss: 0.3104\n",
      "Epoch [5/10], Step [1220/2737], Loss: 0.2097\n",
      "Epoch [5/10], Step [1230/2737], Loss: 0.2497\n",
      "Epoch [5/10], Step [1240/2737], Loss: 0.1822\n",
      "Epoch [5/10], Step [1250/2737], Loss: 0.2364\n",
      "Epoch [5/10], Step [1260/2737], Loss: 0.2834\n",
      "Epoch [5/10], Step [1270/2737], Loss: 0.2551\n",
      "Epoch [5/10], Step [1280/2737], Loss: 0.2480\n",
      "Epoch [5/10], Step [1290/2737], Loss: 0.2307\n",
      "Epoch [5/10], Step [1300/2737], Loss: 0.2255\n",
      "Epoch [5/10], Step [1310/2737], Loss: 0.1816\n",
      "Epoch [5/10], Step [1320/2737], Loss: 0.3270\n",
      "Epoch [5/10], Step [1330/2737], Loss: 0.2127\n",
      "Epoch [5/10], Step [1340/2737], Loss: 0.2097\n",
      "Epoch [5/10], Step [1350/2737], Loss: 0.2475\n",
      "Epoch [5/10], Step [1360/2737], Loss: 0.2003\n",
      "Epoch [5/10], Step [1370/2737], Loss: 0.1421\n",
      "Epoch [5/10], Step [1380/2737], Loss: 0.3890\n",
      "Epoch [5/10], Step [1390/2737], Loss: 0.3063\n",
      "Epoch [5/10], Step [1400/2737], Loss: 0.1367\n",
      "Epoch [5/10], Step [1410/2737], Loss: 0.2023\n",
      "Epoch [5/10], Step [1420/2737], Loss: 0.2761\n",
      "Epoch [5/10], Step [1430/2737], Loss: 0.2451\n",
      "Epoch [5/10], Step [1440/2737], Loss: 0.2532\n",
      "Epoch [5/10], Step [1450/2737], Loss: 0.2576\n",
      "Epoch [5/10], Step [1460/2737], Loss: 0.2775\n",
      "Epoch [5/10], Step [1470/2737], Loss: 0.3423\n",
      "Epoch [5/10], Step [1480/2737], Loss: 0.1840\n",
      "Epoch [5/10], Step [1490/2737], Loss: 0.2529\n",
      "Epoch [5/10], Step [1500/2737], Loss: 0.2180\n",
      "Epoch [5/10], Step [1510/2737], Loss: 0.2648\n",
      "Epoch [5/10], Step [1520/2737], Loss: 0.2676\n",
      "Epoch [5/10], Step [1530/2737], Loss: 0.3482\n",
      "Epoch [5/10], Step [1540/2737], Loss: 0.3686\n",
      "Epoch [5/10], Step [1550/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [1560/2737], Loss: 0.2308\n",
      "Epoch [5/10], Step [1570/2737], Loss: 0.2459\n",
      "Epoch [5/10], Step [1580/2737], Loss: 0.2255\n",
      "Epoch [5/10], Step [1590/2737], Loss: 0.3731\n",
      "Epoch [5/10], Step [1600/2737], Loss: 0.1635\n",
      "Epoch [5/10], Step [1610/2737], Loss: 0.1566\n",
      "Epoch [5/10], Step [1620/2737], Loss: 0.3457\n",
      "Epoch [5/10], Step [1630/2737], Loss: 0.2003\n",
      "Epoch [5/10], Step [1640/2737], Loss: 0.2055\n",
      "Epoch [5/10], Step [1650/2737], Loss: 0.2493\n",
      "Epoch [5/10], Step [1660/2737], Loss: 0.3560\n",
      "Epoch [5/10], Step [1670/2737], Loss: 0.2814\n",
      "Epoch [5/10], Step [1680/2737], Loss: 0.2269\n",
      "Epoch [5/10], Step [1690/2737], Loss: 0.3397\n",
      "Epoch [5/10], Step [1700/2737], Loss: 0.1368\n",
      "Epoch [5/10], Step [1710/2737], Loss: 0.2014\n",
      "Epoch [5/10], Step [1720/2737], Loss: 0.1770\n",
      "Epoch [5/10], Step [1730/2737], Loss: 0.2575\n",
      "Epoch [5/10], Step [1740/2737], Loss: 0.2006\n",
      "Epoch [5/10], Step [1750/2737], Loss: 0.3720\n",
      "Epoch [5/10], Step [1760/2737], Loss: 0.3870\n",
      "Epoch [5/10], Step [1770/2737], Loss: 0.3476\n",
      "Epoch [5/10], Step [1780/2737], Loss: 0.1513\n",
      "Epoch [5/10], Step [1790/2737], Loss: 0.3279\n",
      "Epoch [5/10], Step [1800/2737], Loss: 0.1369\n",
      "Epoch [5/10], Step [1810/2737], Loss: 0.3396\n",
      "Epoch [5/10], Step [1820/2737], Loss: 0.2507\n",
      "Epoch [5/10], Step [1830/2737], Loss: 0.2702\n",
      "Epoch [5/10], Step [1840/2737], Loss: 0.2985\n",
      "Epoch [5/10], Step [1850/2737], Loss: 0.2279\n",
      "Epoch [5/10], Step [1860/2737], Loss: 0.1906\n",
      "Epoch [5/10], Step [1870/2737], Loss: 0.3266\n",
      "Epoch [5/10], Step [1880/2737], Loss: 0.2345\n",
      "Epoch [5/10], Step [1890/2737], Loss: 0.3471\n",
      "Epoch [5/10], Step [1900/2737], Loss: 0.1794\n",
      "Epoch [5/10], Step [1910/2737], Loss: 0.2188\n",
      "Epoch [5/10], Step [1920/2737], Loss: 0.2596\n",
      "Epoch [5/10], Step [1930/2737], Loss: 0.2727\n",
      "Epoch [5/10], Step [1940/2737], Loss: 0.3056\n",
      "Epoch [5/10], Step [1950/2737], Loss: 0.2032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [1960/2737], Loss: 0.2734\n",
      "Epoch [5/10], Step [1970/2737], Loss: 0.1680\n",
      "Epoch [5/10], Step [1980/2737], Loss: 0.2803\n",
      "Epoch [5/10], Step [1990/2737], Loss: 0.1871\n",
      "Epoch [5/10], Step [2000/2737], Loss: 0.2070\n",
      "Epoch [5/10], Step [2010/2737], Loss: 0.3230\n",
      "Epoch [5/10], Step [2020/2737], Loss: 0.1803\n",
      "Epoch [5/10], Step [2030/2737], Loss: 0.1942\n",
      "Epoch [5/10], Step [2040/2737], Loss: 0.4921\n",
      "Epoch [5/10], Step [2050/2737], Loss: 0.2501\n",
      "Epoch [5/10], Step [2060/2737], Loss: 0.2593\n",
      "Epoch [5/10], Step [2070/2737], Loss: 0.2574\n",
      "Epoch [5/10], Step [2080/2737], Loss: 0.3038\n",
      "Epoch [5/10], Step [2090/2737], Loss: 0.2516\n",
      "Epoch [5/10], Step [2100/2737], Loss: 0.1678\n",
      "Epoch [5/10], Step [2110/2737], Loss: 0.2534\n",
      "Epoch [5/10], Step [2120/2737], Loss: 0.1811\n",
      "Epoch [5/10], Step [2130/2737], Loss: 0.1898\n",
      "Epoch [5/10], Step [2140/2737], Loss: 0.2184\n",
      "Epoch [5/10], Step [2150/2737], Loss: 0.2274\n",
      "Epoch [5/10], Step [2160/2737], Loss: 0.3393\n",
      "Epoch [5/10], Step [2170/2737], Loss: 0.2346\n",
      "Epoch [5/10], Step [2180/2737], Loss: 0.1343\n",
      "Epoch [5/10], Step [2190/2737], Loss: 0.4325\n",
      "Epoch [5/10], Step [2200/2737], Loss: 0.2500\n",
      "Epoch [5/10], Step [2210/2737], Loss: 0.2939\n",
      "Epoch [5/10], Step [2220/2737], Loss: 0.2062\n",
      "Epoch [5/10], Step [2230/2737], Loss: 0.4011\n",
      "Epoch [5/10], Step [2240/2737], Loss: 0.2823\n",
      "Epoch [5/10], Step [2250/2737], Loss: 0.2248\n",
      "Epoch [5/10], Step [2260/2737], Loss: 0.2756\n",
      "Epoch [5/10], Step [2270/2737], Loss: 0.2905\n",
      "Epoch [5/10], Step [2280/2737], Loss: 0.1613\n",
      "Epoch [5/10], Step [2290/2737], Loss: 0.2423\n",
      "Epoch [5/10], Step [2300/2737], Loss: 0.1463\n",
      "Epoch [5/10], Step [2310/2737], Loss: 0.3284\n",
      "Epoch [5/10], Step [2320/2737], Loss: 0.2030\n",
      "Epoch [5/10], Step [2330/2737], Loss: 0.2759\n",
      "Epoch [5/10], Step [2340/2737], Loss: 0.2804\n",
      "Epoch [5/10], Step [2350/2737], Loss: 0.1909\n",
      "Epoch [5/10], Step [2360/2737], Loss: 0.1859\n",
      "Epoch [5/10], Step [2370/2737], Loss: 0.1553\n",
      "Epoch [5/10], Step [2380/2737], Loss: 0.3481\n",
      "Epoch [5/10], Step [2390/2737], Loss: 0.2297\n",
      "Epoch [5/10], Step [2400/2737], Loss: 0.2233\n",
      "Epoch [5/10], Step [2410/2737], Loss: 0.2296\n",
      "Epoch [5/10], Step [2420/2737], Loss: 0.2759\n",
      "Epoch [5/10], Step [2430/2737], Loss: 0.3474\n",
      "Epoch [5/10], Step [2440/2737], Loss: 0.3373\n",
      "Epoch [5/10], Step [2450/2737], Loss: 0.2806\n",
      "Epoch [5/10], Step [2460/2737], Loss: 0.2096\n",
      "Epoch [5/10], Step [2470/2737], Loss: 0.2719\n",
      "Epoch [5/10], Step [2480/2737], Loss: 0.3741\n",
      "Epoch [5/10], Step [2490/2737], Loss: 0.1980\n",
      "Epoch [5/10], Step [2500/2737], Loss: 0.1809\n",
      "Epoch [5/10], Step [2510/2737], Loss: 0.1512\n",
      "Epoch [5/10], Step [2520/2737], Loss: 0.2237\n",
      "Epoch [5/10], Step [2530/2737], Loss: 0.1557\n",
      "Epoch [5/10], Step [2540/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2550/2737], Loss: 0.3314\n",
      "Epoch [5/10], Step [2560/2737], Loss: 0.2072\n",
      "Epoch [5/10], Step [2570/2737], Loss: 0.2738\n",
      "Epoch [5/10], Step [2580/2737], Loss: 0.1457\n",
      "Epoch [5/10], Step [2590/2737], Loss: 0.2146\n",
      "Epoch [5/10], Step [2600/2737], Loss: 0.1958\n",
      "Epoch [5/10], Step [2610/2737], Loss: 0.3227\n",
      "Epoch [5/10], Step [2620/2737], Loss: 0.1516\n",
      "Epoch [5/10], Step [2630/2737], Loss: 0.1751\n",
      "Epoch [5/10], Step [2640/2737], Loss: 0.2057\n",
      "Epoch [5/10], Step [2650/2737], Loss: 0.2331\n",
      "Epoch [5/10], Step [2660/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2670/2737], Loss: 0.1795\n",
      "Epoch [5/10], Step [2680/2737], Loss: 0.2770\n",
      "Epoch [5/10], Step [2690/2737], Loss: 0.1525\n",
      "Epoch [5/10], Step [2700/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [5/10], Step [2720/2737], Loss: 0.3222\n",
      "Epoch [5/10], Step [2730/2737], Loss: 0.3452\n",
      "Epoch [5/10], train_loss: 0.2452, val_loss: 0.2533\n",
      "Epoch [6/10], Step [10/2737], Loss: 0.2399\n",
      "Epoch [6/10], Step [20/2737], Loss: 0.3109\n",
      "Epoch [6/10], Step [30/2737], Loss: 0.2173\n",
      "Epoch [6/10], Step [40/2737], Loss: 0.1588\n",
      "Epoch [6/10], Step [50/2737], Loss: 0.3325\n",
      "Epoch [6/10], Step [60/2737], Loss: 0.2895\n",
      "Epoch [6/10], Step [70/2737], Loss: 0.2511\n",
      "Epoch [6/10], Step [80/2737], Loss: 0.2264\n",
      "Epoch [6/10], Step [90/2737], Loss: 0.2219\n",
      "Epoch [6/10], Step [100/2737], Loss: 0.2470\n",
      "Epoch [6/10], Step [110/2737], Loss: 0.2169\n",
      "Epoch [6/10], Step [120/2737], Loss: 0.3072\n",
      "Epoch [6/10], Step [130/2737], Loss: 0.2151\n",
      "Epoch [6/10], Step [140/2737], Loss: 0.2086\n",
      "Epoch [6/10], Step [150/2737], Loss: 0.3174\n",
      "Epoch [6/10], Step [160/2737], Loss: 0.3035\n",
      "Epoch [6/10], Step [170/2737], Loss: 0.1876\n",
      "Epoch [6/10], Step [180/2737], Loss: 0.1755\n",
      "Epoch [6/10], Step [190/2737], Loss: 0.1656\n",
      "Epoch [6/10], Step [200/2737], Loss: 0.2095\n",
      "Epoch [6/10], Step [210/2737], Loss: 0.3081\n",
      "Epoch [6/10], Step [220/2737], Loss: 0.3216\n",
      "Epoch [6/10], Step [230/2737], Loss: 0.1953\n",
      "Epoch [6/10], Step [240/2737], Loss: 0.1706\n",
      "Epoch [6/10], Step [250/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [260/2737], Loss: 0.2748\n",
      "Epoch [6/10], Step [270/2737], Loss: 0.2488\n",
      "Epoch [6/10], Step [280/2737], Loss: 0.1966\n",
      "Epoch [6/10], Step [290/2737], Loss: 0.4001\n",
      "Epoch [6/10], Step [300/2737], Loss: 0.2444\n",
      "Epoch [6/10], Step [310/2737], Loss: 0.2748\n",
      "Epoch [6/10], Step [320/2737], Loss: 0.3118\n",
      "Epoch [6/10], Step [330/2737], Loss: 0.1643\n",
      "Epoch [6/10], Step [340/2737], Loss: 0.2160\n",
      "Epoch [6/10], Step [350/2737], Loss: 0.1563\n",
      "Epoch [6/10], Step [360/2737], Loss: 0.1825\n",
      "Epoch [6/10], Step [370/2737], Loss: 0.4525\n",
      "Epoch [6/10], Step [380/2737], Loss: 0.2207\n",
      "Epoch [6/10], Step [390/2737], Loss: 0.2590\n",
      "Epoch [6/10], Step [400/2737], Loss: 0.2436\n",
      "Epoch [6/10], Step [410/2737], Loss: 0.3093\n",
      "Epoch [6/10], Step [420/2737], Loss: 0.2623\n",
      "Epoch [6/10], Step [430/2737], Loss: 0.2598\n",
      "Epoch [6/10], Step [440/2737], Loss: 0.2909\n",
      "Epoch [6/10], Step [450/2737], Loss: 0.3834\n",
      "Epoch [6/10], Step [460/2737], Loss: 0.4520\n",
      "Epoch [6/10], Step [470/2737], Loss: 0.1534\n",
      "Epoch [6/10], Step [480/2737], Loss: 0.1611\n",
      "Epoch [6/10], Step [490/2737], Loss: 0.1487\n",
      "Epoch [6/10], Step [500/2737], Loss: 0.2089\n",
      "Epoch [6/10], Step [510/2737], Loss: 0.1885\n",
      "Epoch [6/10], Step [520/2737], Loss: 0.2510\n",
      "Epoch [6/10], Step [530/2737], Loss: 0.2429\n",
      "Epoch [6/10], Step [540/2737], Loss: 0.2840\n",
      "Epoch [6/10], Step [550/2737], Loss: 0.1823\n",
      "Epoch [6/10], Step [560/2737], Loss: 0.3027\n",
      "Epoch [6/10], Step [570/2737], Loss: 0.1330\n",
      "Epoch [6/10], Step [580/2737], Loss: 0.1631\n",
      "Epoch [6/10], Step [590/2737], Loss: 0.2279\n",
      "Epoch [6/10], Step [600/2737], Loss: 0.3450\n",
      "Epoch [6/10], Step [610/2737], Loss: 0.1958\n",
      "Epoch [6/10], Step [620/2737], Loss: 0.3047\n",
      "Epoch [6/10], Step [630/2737], Loss: 0.1443\n",
      "Epoch [6/10], Step [640/2737], Loss: 0.1624\n",
      "Epoch [6/10], Step [650/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [660/2737], Loss: 0.1812\n",
      "Epoch [6/10], Step [670/2737], Loss: 0.2932\n",
      "Epoch [6/10], Step [680/2737], Loss: 0.1794\n",
      "Epoch [6/10], Step [690/2737], Loss: 0.1544\n",
      "Epoch [6/10], Step [700/2737], Loss: 0.2793\n",
      "Epoch [6/10], Step [710/2737], Loss: 0.2587\n",
      "Epoch [6/10], Step [720/2737], Loss: 0.2631\n",
      "Epoch [6/10], Step [730/2737], Loss: 0.1850\n",
      "Epoch [6/10], Step [740/2737], Loss: 0.2496\n",
      "Epoch [6/10], Step [750/2737], Loss: 0.1894\n",
      "Epoch [6/10], Step [760/2737], Loss: 0.2997\n",
      "Epoch [6/10], Step [770/2737], Loss: 0.2157\n",
      "Epoch [6/10], Step [780/2737], Loss: 0.1629\n",
      "Epoch [6/10], Step [790/2737], Loss: 0.2091\n",
      "Epoch [6/10], Step [800/2737], Loss: 0.3174\n",
      "Epoch [6/10], Step [810/2737], Loss: 0.3003\n",
      "Epoch [6/10], Step [820/2737], Loss: 0.1328\n",
      "Epoch [6/10], Step [830/2737], Loss: 0.2981\n",
      "Epoch [6/10], Step [840/2737], Loss: 0.2193\n",
      "Epoch [6/10], Step [850/2737], Loss: 0.2247\n",
      "Epoch [6/10], Step [860/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [870/2737], Loss: 0.1607\n",
      "Epoch [6/10], Step [880/2737], Loss: 0.2132\n",
      "Epoch [6/10], Step [890/2737], Loss: 0.2149\n",
      "Epoch [6/10], Step [900/2737], Loss: 0.3021\n",
      "Epoch [6/10], Step [910/2737], Loss: 0.2299\n",
      "Epoch [6/10], Step [920/2737], Loss: 0.2172\n",
      "Epoch [6/10], Step [930/2737], Loss: 0.2158\n",
      "Epoch [6/10], Step [940/2737], Loss: 0.2203\n",
      "Epoch [6/10], Step [950/2737], Loss: 0.3613\n",
      "Epoch [6/10], Step [960/2737], Loss: 0.2182\n",
      "Epoch [6/10], Step [970/2737], Loss: 0.3053\n",
      "Epoch [6/10], Step [980/2737], Loss: 0.2027\n",
      "Epoch [6/10], Step [990/2737], Loss: 0.4155\n",
      "Epoch [6/10], Step [1000/2737], Loss: 0.2440\n",
      "Epoch [6/10], Step [1010/2737], Loss: 0.1947\n",
      "Epoch [6/10], Step [1020/2737], Loss: 0.2665\n",
      "Epoch [6/10], Step [1030/2737], Loss: 0.2227\n",
      "Epoch [6/10], Step [1040/2737], Loss: 0.1396\n",
      "Epoch [6/10], Step [1050/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [1060/2737], Loss: 0.3280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1070/2737], Loss: 0.3962\n",
      "Epoch [6/10], Step [1080/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [1090/2737], Loss: 0.2299\n",
      "Epoch [6/10], Step [1100/2737], Loss: 0.1609\n",
      "Epoch [6/10], Step [1110/2737], Loss: 0.2819\n",
      "Epoch [6/10], Step [1120/2737], Loss: 0.2479\n",
      "Epoch [6/10], Step [1130/2737], Loss: 0.1879\n",
      "Epoch [6/10], Step [1140/2737], Loss: 0.3115\n",
      "Epoch [6/10], Step [1150/2737], Loss: 0.4137\n",
      "Epoch [6/10], Step [1160/2737], Loss: 0.2154\n",
      "Epoch [6/10], Step [1170/2737], Loss: 0.1494\n",
      "Epoch [6/10], Step [1180/2737], Loss: 0.2525\n",
      "Epoch [6/10], Step [1190/2737], Loss: 0.1396\n",
      "Epoch [6/10], Step [1200/2737], Loss: 0.2226\n",
      "Epoch [6/10], Step [1210/2737], Loss: 0.2488\n",
      "Epoch [6/10], Step [1220/2737], Loss: 0.2417\n",
      "Epoch [6/10], Step [1230/2737], Loss: 0.2195\n",
      "Epoch [6/10], Step [1240/2737], Loss: 0.1813\n",
      "Epoch [6/10], Step [1250/2737], Loss: 0.2161\n",
      "Epoch [6/10], Step [1260/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [1270/2737], Loss: 0.2535\n",
      "Epoch [6/10], Step [1280/2737], Loss: 0.1698\n",
      "Epoch [6/10], Step [1290/2737], Loss: 0.2793\n",
      "Epoch [6/10], Step [1300/2737], Loss: 0.2263\n",
      "Epoch [6/10], Step [1310/2737], Loss: 0.3163\n",
      "Epoch [6/10], Step [1320/2737], Loss: 0.2331\n",
      "Epoch [6/10], Step [1330/2737], Loss: 0.1535\n",
      "Epoch [6/10], Step [1340/2737], Loss: 0.1738\n",
      "Epoch [6/10], Step [1350/2737], Loss: 0.1811\n",
      "Epoch [6/10], Step [1360/2737], Loss: 0.2075\n",
      "Epoch [6/10], Step [1370/2737], Loss: 0.2942\n",
      "Epoch [6/10], Step [1380/2737], Loss: 0.2646\n",
      "Epoch [6/10], Step [1390/2737], Loss: 0.2357\n",
      "Epoch [6/10], Step [1400/2737], Loss: 0.2916\n",
      "Epoch [6/10], Step [1410/2737], Loss: 0.2435\n",
      "Epoch [6/10], Step [1420/2737], Loss: 0.2645\n",
      "Epoch [6/10], Step [1430/2737], Loss: 0.2057\n",
      "Epoch [6/10], Step [1440/2737], Loss: 0.1827\n",
      "Epoch [6/10], Step [1450/2737], Loss: 0.2126\n",
      "Epoch [6/10], Step [1460/2737], Loss: 0.1197\n",
      "Epoch [6/10], Step [1470/2737], Loss: 0.2231\n",
      "Epoch [6/10], Step [1480/2737], Loss: 0.2629\n",
      "Epoch [6/10], Step [1490/2737], Loss: 0.1988\n",
      "Epoch [6/10], Step [1500/2737], Loss: 0.2455\n",
      "Epoch [6/10], Step [1510/2737], Loss: 0.2607\n",
      "Epoch [6/10], Step [1520/2737], Loss: 0.1935\n",
      "Epoch [6/10], Step [1530/2737], Loss: 0.2826\n",
      "Epoch [6/10], Step [1540/2737], Loss: 0.2359\n",
      "Epoch [6/10], Step [1550/2737], Loss: 0.1852\n",
      "Epoch [6/10], Step [1560/2737], Loss: 0.2834\n",
      "Epoch [6/10], Step [1570/2737], Loss: 0.1566\n",
      "Epoch [6/10], Step [1580/2737], Loss: 0.2699\n",
      "Epoch [6/10], Step [1590/2737], Loss: 0.2292\n",
      "Epoch [6/10], Step [1600/2737], Loss: 0.1955\n",
      "Epoch [6/10], Step [1610/2737], Loss: 0.2322\n",
      "Epoch [6/10], Step [1620/2737], Loss: 0.1754\n",
      "Epoch [6/10], Step [1630/2737], Loss: 0.2235\n",
      "Epoch [6/10], Step [1640/2737], Loss: 0.2567\n",
      "Epoch [6/10], Step [1650/2737], Loss: 0.2731\n",
      "Epoch [6/10], Step [1660/2737], Loss: 0.2869\n",
      "Epoch [6/10], Step [1670/2737], Loss: 0.1806\n",
      "Epoch [6/10], Step [1680/2737], Loss: 0.2657\n",
      "Epoch [6/10], Step [1690/2737], Loss: 0.1818\n",
      "Epoch [6/10], Step [1700/2737], Loss: 0.2556\n",
      "Epoch [6/10], Step [1710/2737], Loss: 0.2027\n",
      "Epoch [6/10], Step [1720/2737], Loss: 0.2089\n",
      "Epoch [6/10], Step [1730/2737], Loss: 0.2058\n",
      "Epoch [6/10], Step [1740/2737], Loss: 0.2078\n",
      "Epoch [6/10], Step [1750/2737], Loss: 0.2312\n",
      "Epoch [6/10], Step [1760/2737], Loss: 0.2606\n",
      "Epoch [6/10], Step [1770/2737], Loss: 0.1989\n",
      "Epoch [6/10], Step [1780/2737], Loss: 0.2604\n",
      "Epoch [6/10], Step [1790/2737], Loss: 0.2495\n",
      "Epoch [6/10], Step [1800/2737], Loss: 0.1883\n",
      "Epoch [6/10], Step [1810/2737], Loss: 0.2228\n",
      "Epoch [6/10], Step [1820/2737], Loss: 0.1882\n",
      "Epoch [6/10], Step [1830/2737], Loss: 0.2804\n",
      "Epoch [6/10], Step [1840/2737], Loss: 0.2193\n",
      "Epoch [6/10], Step [1850/2737], Loss: 0.2181\n",
      "Epoch [6/10], Step [1860/2737], Loss: 0.1784\n",
      "Epoch [6/10], Step [1870/2737], Loss: 0.2031\n",
      "Epoch [6/10], Step [1880/2737], Loss: 0.1491\n",
      "Epoch [6/10], Step [1890/2737], Loss: 0.2264\n",
      "Epoch [6/10], Step [1900/2737], Loss: 0.3413\n",
      "Epoch [6/10], Step [1910/2737], Loss: 0.2449\n",
      "Epoch [6/10], Step [1920/2737], Loss: 0.1872\n",
      "Epoch [6/10], Step [1930/2737], Loss: 0.3247\n",
      "Epoch [6/10], Step [1940/2737], Loss: 0.1834\n",
      "Epoch [6/10], Step [1950/2737], Loss: 0.1835\n",
      "Epoch [6/10], Step [1960/2737], Loss: 0.2018\n",
      "Epoch [6/10], Step [1970/2737], Loss: 0.3102\n",
      "Epoch [6/10], Step [1980/2737], Loss: 0.1900\n",
      "Epoch [6/10], Step [1990/2737], Loss: 0.3024\n",
      "Epoch [6/10], Step [2000/2737], Loss: 0.2398\n",
      "Epoch [6/10], Step [2010/2737], Loss: 0.1952\n",
      "Epoch [6/10], Step [2020/2737], Loss: 0.2420\n",
      "Epoch [6/10], Step [2030/2737], Loss: 0.1913\n",
      "Epoch [6/10], Step [2040/2737], Loss: 0.2291\n",
      "Epoch [6/10], Step [2050/2737], Loss: 0.3700\n",
      "Epoch [6/10], Step [2060/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [2070/2737], Loss: 0.2278\n",
      "Epoch [6/10], Step [2080/2737], Loss: 0.2239\n",
      "Epoch [6/10], Step [2090/2737], Loss: 0.1725\n",
      "Epoch [6/10], Step [2100/2737], Loss: 0.1441\n",
      "Epoch [6/10], Step [2110/2737], Loss: 0.2016\n",
      "Epoch [6/10], Step [2120/2737], Loss: 0.1823\n",
      "Epoch [6/10], Step [2130/2737], Loss: 0.3051\n",
      "Epoch [6/10], Step [2140/2737], Loss: 0.2536\n",
      "Epoch [6/10], Step [2150/2737], Loss: 0.2268\n",
      "Epoch [6/10], Step [2160/2737], Loss: 0.1682\n",
      "Epoch [6/10], Step [2170/2737], Loss: 0.1671\n",
      "Epoch [6/10], Step [2180/2737], Loss: 0.1806\n",
      "Epoch [6/10], Step [2190/2737], Loss: 0.2460\n",
      "Epoch [6/10], Step [2200/2737], Loss: 0.3287\n",
      "Epoch [6/10], Step [2210/2737], Loss: 0.2584\n",
      "Epoch [6/10], Step [2220/2737], Loss: 0.3187\n",
      "Epoch [6/10], Step [2230/2737], Loss: 0.1651\n",
      "Epoch [6/10], Step [2240/2737], Loss: 0.2865\n",
      "Epoch [6/10], Step [2250/2737], Loss: 0.2187\n",
      "Epoch [6/10], Step [2260/2737], Loss: 0.2387\n",
      "Epoch [6/10], Step [2270/2737], Loss: 0.2493\n",
      "Epoch [6/10], Step [2280/2737], Loss: 0.1984\n",
      "Epoch [6/10], Step [2290/2737], Loss: 0.1471\n",
      "Epoch [6/10], Step [2300/2737], Loss: 0.2804\n",
      "Epoch [6/10], Step [2310/2737], Loss: 0.1929\n",
      "Epoch [6/10], Step [2320/2737], Loss: 0.2512\n",
      "Epoch [6/10], Step [2330/2737], Loss: 0.1953\n",
      "Epoch [6/10], Step [2340/2737], Loss: 0.2617\n",
      "Epoch [6/10], Step [2350/2737], Loss: 0.2230\n",
      "Epoch [6/10], Step [2360/2737], Loss: 0.1538\n",
      "Epoch [6/10], Step [2370/2737], Loss: 0.2637\n",
      "Epoch [6/10], Step [2380/2737], Loss: 0.2277\n",
      "Epoch [6/10], Step [2390/2737], Loss: 0.3163\n",
      "Epoch [6/10], Step [2400/2737], Loss: 0.2115\n",
      "Epoch [6/10], Step [2410/2737], Loss: 0.2630\n",
      "Epoch [6/10], Step [2420/2737], Loss: 0.2814\n",
      "Epoch [6/10], Step [2430/2737], Loss: 0.2396\n",
      "Epoch [6/10], Step [2440/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [2450/2737], Loss: 0.2476\n",
      "Epoch [6/10], Step [2460/2737], Loss: 0.2658\n",
      "Epoch [6/10], Step [2470/2737], Loss: 0.1804\n",
      "Epoch [6/10], Step [2480/2737], Loss: 0.2514\n",
      "Epoch [6/10], Step [2490/2737], Loss: 0.1909\n",
      "Epoch [6/10], Step [2500/2737], Loss: 0.1992\n",
      "Epoch [6/10], Step [2510/2737], Loss: 0.1675\n",
      "Epoch [6/10], Step [2520/2737], Loss: 0.2332\n",
      "Epoch [6/10], Step [2530/2737], Loss: 0.3368\n",
      "Epoch [6/10], Step [2540/2737], Loss: 0.3383\n",
      "Epoch [6/10], Step [2550/2737], Loss: 0.2585\n",
      "Epoch [6/10], Step [2560/2737], Loss: 0.1639\n",
      "Epoch [6/10], Step [2570/2737], Loss: 0.2991\n",
      "Epoch [6/10], Step [2580/2737], Loss: 0.2059\n",
      "Epoch [6/10], Step [2590/2737], Loss: 0.2599\n",
      "Epoch [6/10], Step [2600/2737], Loss: 0.2181\n",
      "Epoch [6/10], Step [2610/2737], Loss: 0.2372\n",
      "Epoch [6/10], Step [2620/2737], Loss: 0.3408\n",
      "Epoch [6/10], Step [2630/2737], Loss: 0.2497\n",
      "Epoch [6/10], Step [2640/2737], Loss: 0.2763\n",
      "Epoch [6/10], Step [2650/2737], Loss: 0.2864\n",
      "Epoch [6/10], Step [2660/2737], Loss: 0.1597\n",
      "Epoch [6/10], Step [2670/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [2680/2737], Loss: 0.2179\n",
      "Epoch [6/10], Step [2690/2737], Loss: 0.2257\n",
      "Epoch [6/10], Step [2700/2737], Loss: 0.2840\n",
      "Epoch [6/10], Step [2710/2737], Loss: 0.2707\n",
      "Epoch [6/10], Step [2720/2737], Loss: 0.3044\n",
      "Epoch [6/10], Step [2730/2737], Loss: 0.3274\n",
      "Epoch [6/10], train_loss: 0.2356, val_loss: 0.2441\n",
      "Epoch [7/10], Step [10/2737], Loss: 0.1594\n",
      "Epoch [7/10], Step [20/2737], Loss: 0.2025\n",
      "Epoch [7/10], Step [30/2737], Loss: 0.2029\n",
      "Epoch [7/10], Step [40/2737], Loss: 0.2526\n",
      "Epoch [7/10], Step [50/2737], Loss: 0.2172\n",
      "Epoch [7/10], Step [60/2737], Loss: 0.3085\n",
      "Epoch [7/10], Step [70/2737], Loss: 0.2770\n",
      "Epoch [7/10], Step [80/2737], Loss: 0.2279\n",
      "Epoch [7/10], Step [90/2737], Loss: 0.1450\n",
      "Epoch [7/10], Step [100/2737], Loss: 0.2270\n",
      "Epoch [7/10], Step [110/2737], Loss: 0.2148\n",
      "Epoch [7/10], Step [120/2737], Loss: 0.2562\n",
      "Epoch [7/10], Step [130/2737], Loss: 0.1968\n",
      "Epoch [7/10], Step [140/2737], Loss: 0.2214\n",
      "Epoch [7/10], Step [150/2737], Loss: 0.3466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [160/2737], Loss: 0.2528\n",
      "Epoch [7/10], Step [170/2737], Loss: 0.2148\n",
      "Epoch [7/10], Step [180/2737], Loss: 0.1958\n",
      "Epoch [7/10], Step [190/2737], Loss: 0.2290\n",
      "Epoch [7/10], Step [200/2737], Loss: 0.2095\n",
      "Epoch [7/10], Step [210/2737], Loss: 0.1101\n",
      "Epoch [7/10], Step [220/2737], Loss: 0.1680\n",
      "Epoch [7/10], Step [230/2737], Loss: 0.2212\n",
      "Epoch [7/10], Step [240/2737], Loss: 0.2047\n",
      "Epoch [7/10], Step [250/2737], Loss: 0.2129\n",
      "Epoch [7/10], Step [260/2737], Loss: 0.2778\n",
      "Epoch [7/10], Step [270/2737], Loss: 0.1962\n",
      "Epoch [7/10], Step [280/2737], Loss: 0.1449\n",
      "Epoch [7/10], Step [290/2737], Loss: 0.2381\n",
      "Epoch [7/10], Step [300/2737], Loss: 0.3047\n",
      "Epoch [7/10], Step [310/2737], Loss: 0.1480\n",
      "Epoch [7/10], Step [320/2737], Loss: 0.2692\n",
      "Epoch [7/10], Step [330/2737], Loss: 0.1334\n",
      "Epoch [7/10], Step [340/2737], Loss: 0.2245\n",
      "Epoch [7/10], Step [350/2737], Loss: 0.1773\n",
      "Epoch [7/10], Step [360/2737], Loss: 0.1841\n",
      "Epoch [7/10], Step [370/2737], Loss: 0.2326\n",
      "Epoch [7/10], Step [380/2737], Loss: 0.1620\n",
      "Epoch [7/10], Step [390/2737], Loss: 0.2517\n",
      "Epoch [7/10], Step [400/2737], Loss: 0.2352\n",
      "Epoch [7/10], Step [410/2737], Loss: 0.2595\n",
      "Epoch [7/10], Step [420/2737], Loss: 0.3060\n",
      "Epoch [7/10], Step [430/2737], Loss: 0.2346\n",
      "Epoch [7/10], Step [440/2737], Loss: 0.2428\n",
      "Epoch [7/10], Step [450/2737], Loss: 0.2326\n",
      "Epoch [7/10], Step [460/2737], Loss: 0.1569\n",
      "Epoch [7/10], Step [470/2737], Loss: 0.1644\n",
      "Epoch [7/10], Step [480/2737], Loss: 0.1732\n",
      "Epoch [7/10], Step [490/2737], Loss: 0.2094\n",
      "Epoch [7/10], Step [500/2737], Loss: 0.2092\n",
      "Epoch [7/10], Step [510/2737], Loss: 0.2192\n",
      "Epoch [7/10], Step [520/2737], Loss: 0.1699\n",
      "Epoch [7/10], Step [530/2737], Loss: 0.3134\n",
      "Epoch [7/10], Step [540/2737], Loss: 0.3118\n",
      "Epoch [7/10], Step [550/2737], Loss: 0.1947\n",
      "Epoch [7/10], Step [560/2737], Loss: 0.1503\n",
      "Epoch [7/10], Step [570/2737], Loss: 0.3264\n",
      "Epoch [7/10], Step [580/2737], Loss: 0.1746\n",
      "Epoch [7/10], Step [590/2737], Loss: 0.1810\n",
      "Epoch [7/10], Step [600/2737], Loss: 0.1886\n",
      "Epoch [7/10], Step [610/2737], Loss: 0.2307\n",
      "Epoch [7/10], Step [620/2737], Loss: 0.2115\n",
      "Epoch [7/10], Step [630/2737], Loss: 0.1996\n",
      "Epoch [7/10], Step [640/2737], Loss: 0.0767\n",
      "Epoch [7/10], Step [650/2737], Loss: 0.1663\n",
      "Epoch [7/10], Step [660/2737], Loss: 0.2456\n",
      "Epoch [7/10], Step [670/2737], Loss: 0.2984\n",
      "Epoch [7/10], Step [680/2737], Loss: 0.2786\n",
      "Epoch [7/10], Step [690/2737], Loss: 0.1676\n",
      "Epoch [7/10], Step [700/2737], Loss: 0.2325\n",
      "Epoch [7/10], Step [710/2737], Loss: 0.3197\n",
      "Epoch [7/10], Step [720/2737], Loss: 0.2149\n",
      "Epoch [7/10], Step [730/2737], Loss: 0.1988\n",
      "Epoch [7/10], Step [740/2737], Loss: 0.2168\n",
      "Epoch [7/10], Step [750/2737], Loss: 0.1506\n",
      "Epoch [7/10], Step [760/2737], Loss: 0.0968\n",
      "Epoch [7/10], Step [770/2737], Loss: 0.1731\n",
      "Epoch [7/10], Step [780/2737], Loss: 0.1989\n",
      "Epoch [7/10], Step [790/2737], Loss: 0.1901\n",
      "Epoch [7/10], Step [800/2737], Loss: 0.2696\n",
      "Epoch [7/10], Step [810/2737], Loss: 0.1745\n",
      "Epoch [7/10], Step [820/2737], Loss: 0.3063\n",
      "Epoch [7/10], Step [830/2737], Loss: 0.1927\n",
      "Epoch [7/10], Step [840/2737], Loss: 0.3013\n",
      "Epoch [7/10], Step [850/2737], Loss: 0.1974\n",
      "Epoch [7/10], Step [860/2737], Loss: 0.2202\n",
      "Epoch [7/10], Step [870/2737], Loss: 0.2393\n",
      "Epoch [7/10], Step [880/2737], Loss: 0.2642\n",
      "Epoch [7/10], Step [890/2737], Loss: 0.1664\n",
      "Epoch [7/10], Step [900/2737], Loss: 0.4001\n",
      "Epoch [7/10], Step [910/2737], Loss: 0.2129\n",
      "Epoch [7/10], Step [920/2737], Loss: 0.2142\n",
      "Epoch [7/10], Step [930/2737], Loss: 0.2200\n",
      "Epoch [7/10], Step [940/2737], Loss: 0.3169\n",
      "Epoch [7/10], Step [950/2737], Loss: 0.2264\n",
      "Epoch [7/10], Step [960/2737], Loss: 0.2587\n",
      "Epoch [7/10], Step [970/2737], Loss: 0.3638\n",
      "Epoch [7/10], Step [980/2737], Loss: 0.3700\n",
      "Epoch [7/10], Step [990/2737], Loss: 0.3850\n",
      "Epoch [7/10], Step [1000/2737], Loss: 0.2045\n",
      "Epoch [7/10], Step [1010/2737], Loss: 0.1953\n",
      "Epoch [7/10], Step [1020/2737], Loss: 0.1775\n",
      "Epoch [7/10], Step [1030/2737], Loss: 0.2780\n",
      "Epoch [7/10], Step [1040/2737], Loss: 0.2603\n",
      "Epoch [7/10], Step [1050/2737], Loss: 0.1485\n",
      "Epoch [7/10], Step [1060/2737], Loss: 0.1929\n",
      "Epoch [7/10], Step [1070/2737], Loss: 0.1719\n",
      "Epoch [7/10], Step [1080/2737], Loss: 0.1744\n",
      "Epoch [7/10], Step [1090/2737], Loss: 0.2331\n",
      "Epoch [7/10], Step [1100/2737], Loss: 0.2258\n",
      "Epoch [7/10], Step [1110/2737], Loss: 0.1914\n",
      "Epoch [7/10], Step [1120/2737], Loss: 0.3127\n",
      "Epoch [7/10], Step [1130/2737], Loss: 0.2979\n",
      "Epoch [7/10], Step [1140/2737], Loss: 0.1384\n",
      "Epoch [7/10], Step [1150/2737], Loss: 0.1913\n",
      "Epoch [7/10], Step [1160/2737], Loss: 0.2485\n",
      "Epoch [7/10], Step [1170/2737], Loss: 0.1755\n",
      "Epoch [7/10], Step [1180/2737], Loss: 0.1635\n",
      "Epoch [7/10], Step [1190/2737], Loss: 0.2555\n",
      "Epoch [7/10], Step [1200/2737], Loss: 0.1258\n",
      "Epoch [7/10], Step [1210/2737], Loss: 0.1290\n",
      "Epoch [7/10], Step [1220/2737], Loss: 0.3626\n",
      "Epoch [7/10], Step [1230/2737], Loss: 0.1622\n",
      "Epoch [7/10], Step [1240/2737], Loss: 0.2020\n",
      "Epoch [7/10], Step [1250/2737], Loss: 0.2498\n",
      "Epoch [7/10], Step [1260/2737], Loss: 0.2769\n",
      "Epoch [7/10], Step [1270/2737], Loss: 0.1761\n",
      "Epoch [7/10], Step [1280/2737], Loss: 0.2620\n",
      "Epoch [7/10], Step [1290/2737], Loss: 0.1851\n",
      "Epoch [7/10], Step [1300/2737], Loss: 0.2876\n",
      "Epoch [7/10], Step [1310/2737], Loss: 0.1730\n",
      "Epoch [7/10], Step [1320/2737], Loss: 0.2232\n",
      "Epoch [7/10], Step [1330/2737], Loss: 0.2293\n",
      "Epoch [7/10], Step [1340/2737], Loss: 0.2648\n",
      "Epoch [7/10], Step [1350/2737], Loss: 0.1774\n",
      "Epoch [7/10], Step [1360/2737], Loss: 0.2513\n",
      "Epoch [7/10], Step [1370/2737], Loss: 0.2056\n",
      "Epoch [7/10], Step [1380/2737], Loss: 0.2625\n",
      "Epoch [7/10], Step [1390/2737], Loss: 0.3453\n",
      "Epoch [7/10], Step [1400/2737], Loss: 0.1519\n",
      "Epoch [7/10], Step [1410/2737], Loss: 0.1841\n",
      "Epoch [7/10], Step [1420/2737], Loss: 0.2491\n",
      "Epoch [7/10], Step [1430/2737], Loss: 0.2675\n",
      "Epoch [7/10], Step [1440/2737], Loss: 0.2127\n",
      "Epoch [7/10], Step [1450/2737], Loss: 0.3254\n",
      "Epoch [7/10], Step [1460/2737], Loss: 0.2443\n",
      "Epoch [7/10], Step [1470/2737], Loss: 0.3133\n",
      "Epoch [7/10], Step [1480/2737], Loss: 0.2541\n",
      "Epoch [7/10], Step [1490/2737], Loss: 0.1899\n",
      "Epoch [7/10], Step [1500/2737], Loss: 0.3725\n",
      "Epoch [7/10], Step [1510/2737], Loss: 0.2089\n",
      "Epoch [7/10], Step [1520/2737], Loss: 0.2527\n",
      "Epoch [7/10], Step [1530/2737], Loss: 0.2174\n",
      "Epoch [7/10], Step [1540/2737], Loss: 0.2590\n",
      "Epoch [7/10], Step [1550/2737], Loss: 0.1368\n",
      "Epoch [7/10], Step [1560/2737], Loss: 0.2123\n",
      "Epoch [7/10], Step [1570/2737], Loss: 0.2211\n",
      "Epoch [7/10], Step [1580/2737], Loss: 0.2424\n",
      "Epoch [7/10], Step [1590/2737], Loss: 0.3763\n",
      "Epoch [7/10], Step [1600/2737], Loss: 0.2396\n",
      "Epoch [7/10], Step [1610/2737], Loss: 0.1850\n",
      "Epoch [7/10], Step [1620/2737], Loss: 0.3015\n",
      "Epoch [7/10], Step [1630/2737], Loss: 0.3206\n",
      "Epoch [7/10], Step [1640/2737], Loss: 0.1969\n",
      "Epoch [7/10], Step [1650/2737], Loss: 0.1662\n",
      "Epoch [7/10], Step [1660/2737], Loss: 0.1850\n",
      "Epoch [7/10], Step [1670/2737], Loss: 0.2493\n",
      "Epoch [7/10], Step [1680/2737], Loss: 0.1830\n",
      "Epoch [7/10], Step [1690/2737], Loss: 0.3316\n",
      "Epoch [7/10], Step [1700/2737], Loss: 0.2777\n",
      "Epoch [7/10], Step [1710/2737], Loss: 0.2358\n",
      "Epoch [7/10], Step [1720/2737], Loss: 0.1856\n",
      "Epoch [7/10], Step [1730/2737], Loss: 0.3463\n",
      "Epoch [7/10], Step [1740/2737], Loss: 0.2679\n",
      "Epoch [7/10], Step [1750/2737], Loss: 0.3528\n",
      "Epoch [7/10], Step [1760/2737], Loss: 0.1326\n",
      "Epoch [7/10], Step [1770/2737], Loss: 0.1366\n",
      "Epoch [7/10], Step [1780/2737], Loss: 0.1762\n",
      "Epoch [7/10], Step [1790/2737], Loss: 0.2101\n",
      "Epoch [7/10], Step [1800/2737], Loss: 0.2836\n",
      "Epoch [7/10], Step [1810/2737], Loss: 0.2742\n",
      "Epoch [7/10], Step [1820/2737], Loss: 0.1778\n",
      "Epoch [7/10], Step [1830/2737], Loss: 0.1366\n",
      "Epoch [7/10], Step [1840/2737], Loss: 0.2884\n",
      "Epoch [7/10], Step [1850/2737], Loss: 0.2024\n",
      "Epoch [7/10], Step [1860/2737], Loss: 0.3372\n",
      "Epoch [7/10], Step [1870/2737], Loss: 0.2788\n",
      "Epoch [7/10], Step [1880/2737], Loss: 0.2697\n",
      "Epoch [7/10], Step [1890/2737], Loss: 0.2226\n",
      "Epoch [7/10], Step [1900/2737], Loss: 0.3160\n",
      "Epoch [7/10], Step [1910/2737], Loss: 0.2057\n",
      "Epoch [7/10], Step [1920/2737], Loss: 0.2190\n",
      "Epoch [7/10], Step [1930/2737], Loss: 0.2346\n",
      "Epoch [7/10], Step [1940/2737], Loss: 0.2371\n",
      "Epoch [7/10], Step [1950/2737], Loss: 0.2098\n",
      "Epoch [7/10], Step [1960/2737], Loss: 0.1894\n",
      "Epoch [7/10], Step [1970/2737], Loss: 0.1966\n",
      "Epoch [7/10], Step [1980/2737], Loss: 0.2520\n",
      "Epoch [7/10], Step [1990/2737], Loss: 0.1832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [2000/2737], Loss: 0.2284\n",
      "Epoch [7/10], Step [2010/2737], Loss: 0.3736\n",
      "Epoch [7/10], Step [2020/2737], Loss: 0.1813\n",
      "Epoch [7/10], Step [2030/2737], Loss: 0.2575\n",
      "Epoch [7/10], Step [2040/2737], Loss: 0.3171\n",
      "Epoch [7/10], Step [2050/2737], Loss: 0.2876\n",
      "Epoch [7/10], Step [2060/2737], Loss: 0.4572\n",
      "Epoch [7/10], Step [2070/2737], Loss: 0.1575\n",
      "Epoch [7/10], Step [2080/2737], Loss: 0.2958\n",
      "Epoch [7/10], Step [2090/2737], Loss: 0.2242\n",
      "Epoch [7/10], Step [2100/2737], Loss: 0.2717\n",
      "Epoch [7/10], Step [2110/2737], Loss: 0.2569\n",
      "Epoch [7/10], Step [2120/2737], Loss: 0.2323\n",
      "Epoch [7/10], Step [2130/2737], Loss: 0.1852\n",
      "Epoch [7/10], Step [2140/2737], Loss: 0.2193\n",
      "Epoch [7/10], Step [2150/2737], Loss: 0.2209\n",
      "Epoch [7/10], Step [2160/2737], Loss: 0.2003\n",
      "Epoch [7/10], Step [2170/2737], Loss: 0.1767\n",
      "Epoch [7/10], Step [2180/2737], Loss: 0.1452\n",
      "Epoch [7/10], Step [2190/2737], Loss: 0.2118\n",
      "Epoch [7/10], Step [2200/2737], Loss: 0.2013\n",
      "Epoch [7/10], Step [2210/2737], Loss: 0.2284\n",
      "Epoch [7/10], Step [2220/2737], Loss: 0.2536\n",
      "Epoch [7/10], Step [2230/2737], Loss: 0.2028\n",
      "Epoch [7/10], Step [2240/2737], Loss: 0.2302\n",
      "Epoch [7/10], Step [2250/2737], Loss: 0.2141\n",
      "Epoch [7/10], Step [2260/2737], Loss: 0.3349\n",
      "Epoch [7/10], Step [2270/2737], Loss: 0.3514\n",
      "Epoch [7/10], Step [2280/2737], Loss: 0.2759\n",
      "Epoch [7/10], Step [2290/2737], Loss: 0.1626\n",
      "Epoch [7/10], Step [2300/2737], Loss: 0.1935\n",
      "Epoch [7/10], Step [2310/2737], Loss: 0.2106\n",
      "Epoch [7/10], Step [2320/2737], Loss: 0.2089\n",
      "Epoch [7/10], Step [2330/2737], Loss: 0.2202\n",
      "Epoch [7/10], Step [2340/2737], Loss: 0.3371\n",
      "Epoch [7/10], Step [2350/2737], Loss: 0.2943\n",
      "Epoch [7/10], Step [2360/2737], Loss: 0.2513\n",
      "Epoch [7/10], Step [2370/2737], Loss: 0.2894\n",
      "Epoch [7/10], Step [2380/2737], Loss: 0.2732\n",
      "Epoch [7/10], Step [2390/2737], Loss: 0.2378\n",
      "Epoch [7/10], Step [2400/2737], Loss: 0.2525\n",
      "Epoch [7/10], Step [2410/2737], Loss: 0.2989\n",
      "Epoch [7/10], Step [2420/2737], Loss: 0.2895\n",
      "Epoch [7/10], Step [2430/2737], Loss: 0.2719\n",
      "Epoch [7/10], Step [2440/2737], Loss: 0.2321\n",
      "Epoch [7/10], Step [2450/2737], Loss: 0.1733\n",
      "Epoch [7/10], Step [2460/2737], Loss: 0.2220\n",
      "Epoch [7/10], Step [2470/2737], Loss: 0.2397\n",
      "Epoch [7/10], Step [2480/2737], Loss: 0.2054\n",
      "Epoch [7/10], Step [2490/2737], Loss: 0.1552\n",
      "Epoch [7/10], Step [2500/2737], Loss: 0.2114\n",
      "Epoch [7/10], Step [2510/2737], Loss: 0.2650\n",
      "Epoch [7/10], Step [2520/2737], Loss: 0.1205\n",
      "Epoch [7/10], Step [2530/2737], Loss: 0.1700\n",
      "Epoch [7/10], Step [2540/2737], Loss: 0.2720\n",
      "Epoch [7/10], Step [2550/2737], Loss: 0.1581\n",
      "Epoch [7/10], Step [2560/2737], Loss: 0.1784\n",
      "Epoch [7/10], Step [2570/2737], Loss: 0.2293\n",
      "Epoch [7/10], Step [2580/2737], Loss: 0.2303\n",
      "Epoch [7/10], Step [2590/2737], Loss: 0.1607\n",
      "Epoch [7/10], Step [2600/2737], Loss: 0.3047\n",
      "Epoch [7/10], Step [2610/2737], Loss: 0.1658\n",
      "Epoch [7/10], Step [2620/2737], Loss: 0.2863\n",
      "Epoch [7/10], Step [2630/2737], Loss: 0.1851\n",
      "Epoch [7/10], Step [2640/2737], Loss: 0.2685\n",
      "Epoch [7/10], Step [2650/2737], Loss: 0.1755\n",
      "Epoch [7/10], Step [2660/2737], Loss: 0.1494\n",
      "Epoch [7/10], Step [2670/2737], Loss: 0.2075\n",
      "Epoch [7/10], Step [2680/2737], Loss: 0.1338\n",
      "Epoch [7/10], Step [2690/2737], Loss: 0.2079\n",
      "Epoch [7/10], Step [2700/2737], Loss: 0.2238\n",
      "Epoch [7/10], Step [2710/2737], Loss: 0.2644\n",
      "Epoch [7/10], Step [2720/2737], Loss: 0.2281\n",
      "Epoch [7/10], Step [2730/2737], Loss: 0.1722\n",
      "Epoch [7/10], train_loss: 0.2280, val_loss: 0.2687\n",
      "Epoch [8/10], Step [10/2737], Loss: 0.3494\n",
      "Epoch [8/10], Step [20/2737], Loss: 0.1875\n",
      "Epoch [8/10], Step [30/2737], Loss: 0.2051\n",
      "Epoch [8/10], Step [40/2737], Loss: 0.3002\n",
      "Epoch [8/10], Step [50/2737], Loss: 0.2417\n",
      "Epoch [8/10], Step [60/2737], Loss: 0.2351\n",
      "Epoch [8/10], Step [70/2737], Loss: 0.2114\n",
      "Epoch [8/10], Step [80/2737], Loss: 0.1969\n",
      "Epoch [8/10], Step [90/2737], Loss: 0.1571\n",
      "Epoch [8/10], Step [100/2737], Loss: 0.2906\n",
      "Epoch [8/10], Step [110/2737], Loss: 0.1626\n",
      "Epoch [8/10], Step [120/2737], Loss: 0.1502\n",
      "Epoch [8/10], Step [130/2737], Loss: 0.1424\n",
      "Epoch [8/10], Step [140/2737], Loss: 0.1938\n",
      "Epoch [8/10], Step [150/2737], Loss: 0.2546\n",
      "Epoch [8/10], Step [160/2737], Loss: 0.2465\n",
      "Epoch [8/10], Step [170/2737], Loss: 0.2173\n",
      "Epoch [8/10], Step [180/2737], Loss: 0.2119\n",
      "Epoch [8/10], Step [190/2737], Loss: 0.2328\n",
      "Epoch [8/10], Step [200/2737], Loss: 0.1985\n",
      "Epoch [8/10], Step [210/2737], Loss: 0.2017\n",
      "Epoch [8/10], Step [220/2737], Loss: 0.3470\n",
      "Epoch [8/10], Step [230/2737], Loss: 0.2520\n",
      "Epoch [8/10], Step [240/2737], Loss: 0.3268\n",
      "Epoch [8/10], Step [250/2737], Loss: 0.2950\n",
      "Epoch [8/10], Step [260/2737], Loss: 0.2087\n",
      "Epoch [8/10], Step [270/2737], Loss: 0.2273\n",
      "Epoch [8/10], Step [280/2737], Loss: 0.1199\n",
      "Epoch [8/10], Step [290/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [300/2737], Loss: 0.1652\n",
      "Epoch [8/10], Step [310/2737], Loss: 0.1934\n",
      "Epoch [8/10], Step [320/2737], Loss: 0.1789\n",
      "Epoch [8/10], Step [330/2737], Loss: 0.3082\n",
      "Epoch [8/10], Step [340/2737], Loss: 0.1909\n",
      "Epoch [8/10], Step [350/2737], Loss: 0.1752\n",
      "Epoch [8/10], Step [360/2737], Loss: 0.1953\n",
      "Epoch [8/10], Step [370/2737], Loss: 0.3456\n",
      "Epoch [8/10], Step [380/2737], Loss: 0.1751\n",
      "Epoch [8/10], Step [390/2737], Loss: 0.1452\n",
      "Epoch [8/10], Step [400/2737], Loss: 0.1593\n",
      "Epoch [8/10], Step [410/2737], Loss: 0.1835\n",
      "Epoch [8/10], Step [420/2737], Loss: 0.2931\n",
      "Epoch [8/10], Step [430/2737], Loss: 0.2489\n",
      "Epoch [8/10], Step [440/2737], Loss: 0.1880\n",
      "Epoch [8/10], Step [450/2737], Loss: 0.2069\n",
      "Epoch [8/10], Step [460/2737], Loss: 0.1470\n",
      "Epoch [8/10], Step [470/2737], Loss: 0.1917\n",
      "Epoch [8/10], Step [480/2737], Loss: 0.1491\n",
      "Epoch [8/10], Step [490/2737], Loss: 0.1841\n",
      "Epoch [8/10], Step [500/2737], Loss: 0.1924\n",
      "Epoch [8/10], Step [510/2737], Loss: 0.1711\n",
      "Epoch [8/10], Step [520/2737], Loss: 0.2473\n",
      "Epoch [8/10], Step [530/2737], Loss: 0.2432\n",
      "Epoch [8/10], Step [540/2737], Loss: 0.1700\n",
      "Epoch [8/10], Step [550/2737], Loss: 0.1841\n",
      "Epoch [8/10], Step [560/2737], Loss: 0.1351\n",
      "Epoch [8/10], Step [570/2737], Loss: 0.2102\n",
      "Epoch [8/10], Step [580/2737], Loss: 0.2431\n",
      "Epoch [8/10], Step [590/2737], Loss: 0.1818\n",
      "Epoch [8/10], Step [600/2737], Loss: 0.2169\n",
      "Epoch [8/10], Step [610/2737], Loss: 0.2693\n",
      "Epoch [8/10], Step [620/2737], Loss: 0.2456\n",
      "Epoch [8/10], Step [630/2737], Loss: 0.1524\n",
      "Epoch [8/10], Step [640/2737], Loss: 0.1269\n",
      "Epoch [8/10], Step [650/2737], Loss: 0.2300\n",
      "Epoch [8/10], Step [660/2737], Loss: 0.2898\n",
      "Epoch [8/10], Step [670/2737], Loss: 0.1603\n",
      "Epoch [8/10], Step [680/2737], Loss: 0.1773\n",
      "Epoch [8/10], Step [690/2737], Loss: 0.2542\n",
      "Epoch [8/10], Step [700/2737], Loss: 0.2885\n",
      "Epoch [8/10], Step [710/2737], Loss: 0.2273\n",
      "Epoch [8/10], Step [720/2737], Loss: 0.2563\n",
      "Epoch [8/10], Step [730/2737], Loss: 0.2152\n",
      "Epoch [8/10], Step [740/2737], Loss: 0.1889\n",
      "Epoch [8/10], Step [750/2737], Loss: 0.2694\n",
      "Epoch [8/10], Step [760/2737], Loss: 0.1720\n",
      "Epoch [8/10], Step [770/2737], Loss: 0.1916\n",
      "Epoch [8/10], Step [780/2737], Loss: 0.2886\n",
      "Epoch [8/10], Step [790/2737], Loss: 0.2547\n",
      "Epoch [8/10], Step [800/2737], Loss: 0.1994\n",
      "Epoch [8/10], Step [810/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [820/2737], Loss: 0.2206\n",
      "Epoch [8/10], Step [830/2737], Loss: 0.2152\n",
      "Epoch [8/10], Step [840/2737], Loss: 0.1237\n",
      "Epoch [8/10], Step [850/2737], Loss: 0.1890\n",
      "Epoch [8/10], Step [860/2737], Loss: 0.2395\n",
      "Epoch [8/10], Step [870/2737], Loss: 0.2348\n",
      "Epoch [8/10], Step [880/2737], Loss: 0.2423\n",
      "Epoch [8/10], Step [890/2737], Loss: 0.3033\n",
      "Epoch [8/10], Step [900/2737], Loss: 0.2576\n",
      "Epoch [8/10], Step [910/2737], Loss: 0.1949\n",
      "Epoch [8/10], Step [920/2737], Loss: 0.1787\n",
      "Epoch [8/10], Step [930/2737], Loss: 0.1472\n",
      "Epoch [8/10], Step [940/2737], Loss: 0.1720\n",
      "Epoch [8/10], Step [950/2737], Loss: 0.1843\n",
      "Epoch [8/10], Step [960/2737], Loss: 0.3166\n",
      "Epoch [8/10], Step [970/2737], Loss: 0.1556\n",
      "Epoch [8/10], Step [980/2737], Loss: 0.1696\n",
      "Epoch [8/10], Step [990/2737], Loss: 0.1944\n",
      "Epoch [8/10], Step [1000/2737], Loss: 0.2340\n",
      "Epoch [8/10], Step [1010/2737], Loss: 0.1634\n",
      "Epoch [8/10], Step [1020/2737], Loss: 0.3584\n",
      "Epoch [8/10], Step [1030/2737], Loss: 0.4108\n",
      "Epoch [8/10], Step [1040/2737], Loss: 0.1886\n",
      "Epoch [8/10], Step [1050/2737], Loss: 0.2562\n",
      "Epoch [8/10], Step [1060/2737], Loss: 0.1952\n",
      "Epoch [8/10], Step [1070/2737], Loss: 0.2236\n",
      "Epoch [8/10], Step [1080/2737], Loss: 0.2993\n",
      "Epoch [8/10], Step [1090/2737], Loss: 0.3349\n",
      "Epoch [8/10], Step [1100/2737], Loss: 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [1110/2737], Loss: 0.2067\n",
      "Epoch [8/10], Step [1120/2737], Loss: 0.2015\n",
      "Epoch [8/10], Step [1130/2737], Loss: 0.2624\n",
      "Epoch [8/10], Step [1140/2737], Loss: 0.1579\n",
      "Epoch [8/10], Step [1150/2737], Loss: 0.3011\n",
      "Epoch [8/10], Step [1160/2737], Loss: 0.2075\n",
      "Epoch [8/10], Step [1170/2737], Loss: 0.1660\n",
      "Epoch [8/10], Step [1180/2737], Loss: 0.1522\n",
      "Epoch [8/10], Step [1190/2737], Loss: 0.1484\n",
      "Epoch [8/10], Step [1200/2737], Loss: 0.1340\n",
      "Epoch [8/10], Step [1210/2737], Loss: 0.2764\n",
      "Epoch [8/10], Step [1220/2737], Loss: 0.2784\n",
      "Epoch [8/10], Step [1230/2737], Loss: 0.2371\n",
      "Epoch [8/10], Step [1240/2737], Loss: 0.3446\n",
      "Epoch [8/10], Step [1250/2737], Loss: 0.1732\n",
      "Epoch [8/10], Step [1260/2737], Loss: 0.1937\n",
      "Epoch [8/10], Step [1270/2737], Loss: 0.2802\n",
      "Epoch [8/10], Step [1280/2737], Loss: 0.2069\n",
      "Epoch [8/10], Step [1290/2737], Loss: 0.1952\n",
      "Epoch [8/10], Step [1300/2737], Loss: 0.3216\n",
      "Epoch [8/10], Step [1310/2737], Loss: 0.2556\n",
      "Epoch [8/10], Step [1320/2737], Loss: 0.3352\n",
      "Epoch [8/10], Step [1330/2737], Loss: 0.2524\n",
      "Epoch [8/10], Step [1340/2737], Loss: 0.2357\n",
      "Epoch [8/10], Step [1350/2737], Loss: 0.2310\n",
      "Epoch [8/10], Step [1360/2737], Loss: 0.2221\n",
      "Epoch [8/10], Step [1370/2737], Loss: 0.1959\n",
      "Epoch [8/10], Step [1380/2737], Loss: 0.1957\n",
      "Epoch [8/10], Step [1390/2737], Loss: 0.2349\n",
      "Epoch [8/10], Step [1400/2737], Loss: 0.3620\n",
      "Epoch [8/10], Step [1410/2737], Loss: 0.3011\n",
      "Epoch [8/10], Step [1420/2737], Loss: 0.2485\n",
      "Epoch [8/10], Step [1430/2737], Loss: 0.1126\n",
      "Epoch [8/10], Step [1440/2737], Loss: 0.1495\n",
      "Epoch [8/10], Step [1450/2737], Loss: 0.2224\n",
      "Epoch [8/10], Step [1460/2737], Loss: 0.2449\n",
      "Epoch [8/10], Step [1470/2737], Loss: 0.2226\n",
      "Epoch [8/10], Step [1480/2737], Loss: 0.2573\n",
      "Epoch [8/10], Step [1490/2737], Loss: 0.2197\n",
      "Epoch [8/10], Step [1500/2737], Loss: 0.1949\n",
      "Epoch [8/10], Step [1510/2737], Loss: 0.2944\n",
      "Epoch [8/10], Step [1520/2737], Loss: 0.1757\n",
      "Epoch [8/10], Step [1530/2737], Loss: 0.2159\n",
      "Epoch [8/10], Step [1540/2737], Loss: 0.2540\n",
      "Epoch [8/10], Step [1550/2737], Loss: 0.2341\n",
      "Epoch [8/10], Step [1560/2737], Loss: 0.1833\n",
      "Epoch [8/10], Step [1570/2737], Loss: 0.2384\n",
      "Epoch [8/10], Step [1580/2737], Loss: 0.1624\n",
      "Epoch [8/10], Step [1590/2737], Loss: 0.2106\n",
      "Epoch [8/10], Step [1600/2737], Loss: 0.2940\n",
      "Epoch [8/10], Step [1610/2737], Loss: 0.2111\n",
      "Epoch [8/10], Step [1620/2737], Loss: 0.2963\n",
      "Epoch [8/10], Step [1630/2737], Loss: 0.2215\n",
      "Epoch [8/10], Step [1640/2737], Loss: 0.3089\n",
      "Epoch [8/10], Step [1650/2737], Loss: 0.2139\n",
      "Epoch [8/10], Step [1660/2737], Loss: 0.3356\n",
      "Epoch [8/10], Step [1670/2737], Loss: 0.1299\n",
      "Epoch [8/10], Step [1680/2737], Loss: 0.3004\n",
      "Epoch [8/10], Step [1690/2737], Loss: 0.2454\n",
      "Epoch [8/10], Step [1700/2737], Loss: 0.1861\n",
      "Epoch [8/10], Step [1710/2737], Loss: 0.3094\n",
      "Epoch [8/10], Step [1720/2737], Loss: 0.3539\n",
      "Epoch [8/10], Step [1730/2737], Loss: 0.2150\n",
      "Epoch [8/10], Step [1740/2737], Loss: 0.2065\n",
      "Epoch [8/10], Step [1750/2737], Loss: 0.1472\n",
      "Epoch [8/10], Step [1760/2737], Loss: 0.3000\n",
      "Epoch [8/10], Step [1770/2737], Loss: 0.2332\n",
      "Epoch [8/10], Step [1780/2737], Loss: 0.2125\n",
      "Epoch [8/10], Step [1790/2737], Loss: 0.2555\n",
      "Epoch [8/10], Step [1800/2737], Loss: 0.2534\n",
      "Epoch [8/10], Step [1810/2737], Loss: 0.1699\n",
      "Epoch [8/10], Step [1820/2737], Loss: 0.1909\n",
      "Epoch [8/10], Step [1830/2737], Loss: 0.2320\n",
      "Epoch [8/10], Step [1840/2737], Loss: 0.2626\n",
      "Epoch [8/10], Step [1850/2737], Loss: 0.1833\n",
      "Epoch [8/10], Step [1860/2737], Loss: 0.1634\n",
      "Epoch [8/10], Step [1870/2737], Loss: 0.2566\n",
      "Epoch [8/10], Step [1880/2737], Loss: 0.1604\n",
      "Epoch [8/10], Step [1890/2737], Loss: 0.1147\n",
      "Epoch [8/10], Step [1900/2737], Loss: 0.1420\n",
      "Epoch [8/10], Step [1910/2737], Loss: 0.3716\n",
      "Epoch [8/10], Step [1920/2737], Loss: 0.2924\n",
      "Epoch [8/10], Step [1930/2737], Loss: 0.3237\n",
      "Epoch [8/10], Step [1940/2737], Loss: 0.1799\n",
      "Epoch [8/10], Step [1950/2737], Loss: 0.2147\n",
      "Epoch [8/10], Step [1960/2737], Loss: 0.2365\n",
      "Epoch [8/10], Step [1970/2737], Loss: 0.2901\n",
      "Epoch [8/10], Step [1980/2737], Loss: 0.1766\n",
      "Epoch [8/10], Step [1990/2737], Loss: 0.2586\n",
      "Epoch [8/10], Step [2000/2737], Loss: 0.1978\n",
      "Epoch [8/10], Step [2010/2737], Loss: 0.2696\n",
      "Epoch [8/10], Step [2020/2737], Loss: 0.1725\n",
      "Epoch [8/10], Step [2030/2737], Loss: 0.2206\n",
      "Epoch [8/10], Step [2040/2737], Loss: 0.2409\n",
      "Epoch [8/10], Step [2050/2737], Loss: 0.1399\n",
      "Epoch [8/10], Step [2060/2737], Loss: 0.3853\n",
      "Epoch [8/10], Step [2070/2737], Loss: 0.2157\n",
      "Epoch [8/10], Step [2080/2737], Loss: 0.2118\n",
      "Epoch [8/10], Step [2090/2737], Loss: 0.2346\n",
      "Epoch [8/10], Step [2100/2737], Loss: 0.2949\n",
      "Epoch [8/10], Step [2110/2737], Loss: 0.2263\n",
      "Epoch [8/10], Step [2120/2737], Loss: 0.2344\n",
      "Epoch [8/10], Step [2130/2737], Loss: 0.2509\n",
      "Epoch [8/10], Step [2140/2737], Loss: 0.2674\n",
      "Epoch [8/10], Step [2150/2737], Loss: 0.2656\n",
      "Epoch [8/10], Step [2160/2737], Loss: 0.2050\n",
      "Epoch [8/10], Step [2170/2737], Loss: 0.1643\n",
      "Epoch [8/10], Step [2180/2737], Loss: 0.1712\n",
      "Epoch [8/10], Step [2190/2737], Loss: 0.3754\n",
      "Epoch [8/10], Step [2200/2737], Loss: 0.1797\n",
      "Epoch [8/10], Step [2210/2737], Loss: 0.3144\n",
      "Epoch [8/10], Step [2220/2737], Loss: 0.1615\n",
      "Epoch [8/10], Step [2230/2737], Loss: 0.1430\n",
      "Epoch [8/10], Step [2240/2737], Loss: 0.3323\n",
      "Epoch [8/10], Step [2250/2737], Loss: 0.1462\n",
      "Epoch [8/10], Step [2260/2737], Loss: 0.2220\n",
      "Epoch [8/10], Step [2270/2737], Loss: 0.2817\n",
      "Epoch [8/10], Step [2280/2737], Loss: 0.2279\n",
      "Epoch [8/10], Step [2290/2737], Loss: 0.1759\n",
      "Epoch [8/10], Step [2300/2737], Loss: 0.2346\n",
      "Epoch [8/10], Step [2310/2737], Loss: 0.1962\n",
      "Epoch [8/10], Step [2320/2737], Loss: 0.2972\n",
      "Epoch [8/10], Step [2330/2737], Loss: 0.2367\n",
      "Epoch [8/10], Step [2340/2737], Loss: 0.1353\n",
      "Epoch [8/10], Step [2350/2737], Loss: 0.2848\n",
      "Epoch [8/10], Step [2360/2737], Loss: 0.2411\n",
      "Epoch [8/10], Step [2370/2737], Loss: 0.1883\n",
      "Epoch [8/10], Step [2380/2737], Loss: 0.1957\n",
      "Epoch [8/10], Step [2390/2737], Loss: 0.1769\n",
      "Epoch [8/10], Step [2400/2737], Loss: 0.2440\n",
      "Epoch [8/10], Step [2410/2737], Loss: 0.1939\n",
      "Epoch [8/10], Step [2420/2737], Loss: 0.1441\n",
      "Epoch [8/10], Step [2430/2737], Loss: 0.3202\n",
      "Epoch [8/10], Step [2440/2737], Loss: 0.3565\n",
      "Epoch [8/10], Step [2450/2737], Loss: 0.2559\n",
      "Epoch [8/10], Step [2460/2737], Loss: 0.1735\n",
      "Epoch [8/10], Step [2470/2737], Loss: 0.2194\n",
      "Epoch [8/10], Step [2480/2737], Loss: 0.4018\n",
      "Epoch [8/10], Step [2490/2737], Loss: 0.2595\n",
      "Epoch [8/10], Step [2500/2737], Loss: 0.2052\n",
      "Epoch [8/10], Step [2510/2737], Loss: 0.2896\n",
      "Epoch [8/10], Step [2520/2737], Loss: 0.2158\n",
      "Epoch [8/10], Step [2530/2737], Loss: 0.1657\n",
      "Epoch [8/10], Step [2540/2737], Loss: 0.3341\n",
      "Epoch [8/10], Step [2550/2737], Loss: 0.1713\n",
      "Epoch [8/10], Step [2560/2737], Loss: 0.2278\n",
      "Epoch [8/10], Step [2570/2737], Loss: 0.1899\n",
      "Epoch [8/10], Step [2580/2737], Loss: 0.1594\n",
      "Epoch [8/10], Step [2590/2737], Loss: 0.2512\n",
      "Epoch [8/10], Step [2600/2737], Loss: 0.2259\n",
      "Epoch [8/10], Step [2610/2737], Loss: 0.2610\n",
      "Epoch [8/10], Step [2620/2737], Loss: 0.1834\n",
      "Epoch [8/10], Step [2630/2737], Loss: 0.2726\n",
      "Epoch [8/10], Step [2640/2737], Loss: 0.3092\n",
      "Epoch [8/10], Step [2650/2737], Loss: 0.3461\n",
      "Epoch [8/10], Step [2660/2737], Loss: 0.2243\n",
      "Epoch [8/10], Step [2670/2737], Loss: 0.2946\n",
      "Epoch [8/10], Step [2680/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [2690/2737], Loss: 0.2492\n",
      "Epoch [8/10], Step [2700/2737], Loss: 0.1454\n",
      "Epoch [8/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [8/10], Step [2720/2737], Loss: 0.1905\n",
      "Epoch [8/10], Step [2730/2737], Loss: 0.1791\n",
      "Epoch [8/10], train_loss: 0.2242, val_loss: 0.2429\n",
      "Epoch [9/10], Step [10/2737], Loss: 0.1734\n",
      "Epoch [9/10], Step [20/2737], Loss: 0.1611\n",
      "Epoch [9/10], Step [30/2737], Loss: 0.2632\n",
      "Epoch [9/10], Step [40/2737], Loss: 0.1943\n",
      "Epoch [9/10], Step [50/2737], Loss: 0.1970\n",
      "Epoch [9/10], Step [60/2737], Loss: 0.1105\n",
      "Epoch [9/10], Step [70/2737], Loss: 0.2177\n",
      "Epoch [9/10], Step [80/2737], Loss: 0.1552\n",
      "Epoch [9/10], Step [90/2737], Loss: 0.2256\n",
      "Epoch [9/10], Step [100/2737], Loss: 0.1815\n",
      "Epoch [9/10], Step [110/2737], Loss: 0.3076\n",
      "Epoch [9/10], Step [120/2737], Loss: 0.3418\n",
      "Epoch [9/10], Step [130/2737], Loss: 0.3221\n",
      "Epoch [9/10], Step [140/2737], Loss: 0.1942\n",
      "Epoch [9/10], Step [150/2737], Loss: 0.1845\n",
      "Epoch [9/10], Step [160/2737], Loss: 0.2217\n",
      "Epoch [9/10], Step [170/2737], Loss: 0.2142\n",
      "Epoch [9/10], Step [180/2737], Loss: 0.1682\n",
      "Epoch [9/10], Step [190/2737], Loss: 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [200/2737], Loss: 0.2798\n",
      "Epoch [9/10], Step [210/2737], Loss: 0.2530\n",
      "Epoch [9/10], Step [220/2737], Loss: 0.2037\n",
      "Epoch [9/10], Step [230/2737], Loss: 0.2435\n",
      "Epoch [9/10], Step [240/2737], Loss: 0.2138\n",
      "Epoch [9/10], Step [250/2737], Loss: 0.2967\n",
      "Epoch [9/10], Step [260/2737], Loss: 0.1176\n",
      "Epoch [9/10], Step [270/2737], Loss: 0.3168\n",
      "Epoch [9/10], Step [280/2737], Loss: 0.1814\n",
      "Epoch [9/10], Step [290/2737], Loss: 0.1772\n",
      "Epoch [9/10], Step [300/2737], Loss: 0.2242\n",
      "Epoch [9/10], Step [310/2737], Loss: 0.1691\n",
      "Epoch [9/10], Step [320/2737], Loss: 0.1888\n",
      "Epoch [9/10], Step [330/2737], Loss: 0.2201\n",
      "Epoch [9/10], Step [340/2737], Loss: 0.2595\n",
      "Epoch [9/10], Step [350/2737], Loss: 0.1666\n",
      "Epoch [9/10], Step [360/2737], Loss: 0.1788\n",
      "Epoch [9/10], Step [370/2737], Loss: 0.2144\n",
      "Epoch [9/10], Step [380/2737], Loss: 0.3076\n",
      "Epoch [9/10], Step [390/2737], Loss: 0.2361\n",
      "Epoch [9/10], Step [400/2737], Loss: 0.2134\n",
      "Epoch [9/10], Step [410/2737], Loss: 0.2044\n",
      "Epoch [9/10], Step [420/2737], Loss: 0.2372\n",
      "Epoch [9/10], Step [430/2737], Loss: 0.2154\n",
      "Epoch [9/10], Step [440/2737], Loss: 0.1749\n",
      "Epoch [9/10], Step [450/2737], Loss: 0.2240\n",
      "Epoch [9/10], Step [460/2737], Loss: 0.2333\n",
      "Epoch [9/10], Step [470/2737], Loss: 0.1636\n",
      "Epoch [9/10], Step [480/2737], Loss: 0.1850\n",
      "Epoch [9/10], Step [490/2737], Loss: 0.1340\n",
      "Epoch [9/10], Step [500/2737], Loss: 0.2271\n",
      "Epoch [9/10], Step [510/2737], Loss: 0.1660\n",
      "Epoch [9/10], Step [520/2737], Loss: 0.1772\n",
      "Epoch [9/10], Step [530/2737], Loss: 0.1489\n",
      "Epoch [9/10], Step [540/2737], Loss: 0.2087\n",
      "Epoch [9/10], Step [550/2737], Loss: 0.1930\n",
      "Epoch [9/10], Step [560/2737], Loss: 0.2243\n",
      "Epoch [9/10], Step [570/2737], Loss: 0.2458\n",
      "Epoch [9/10], Step [580/2737], Loss: 0.2127\n",
      "Epoch [9/10], Step [590/2737], Loss: 0.1873\n",
      "Epoch [9/10], Step [600/2737], Loss: 0.1274\n",
      "Epoch [9/10], Step [610/2737], Loss: 0.2032\n",
      "Epoch [9/10], Step [620/2737], Loss: 0.2218\n",
      "Epoch [9/10], Step [630/2737], Loss: 0.1977\n",
      "Epoch [9/10], Step [640/2737], Loss: 0.1789\n",
      "Epoch [9/10], Step [650/2737], Loss: 0.2571\n",
      "Epoch [9/10], Step [660/2737], Loss: 0.2826\n",
      "Epoch [9/10], Step [670/2737], Loss: 0.2506\n",
      "Epoch [9/10], Step [680/2737], Loss: 0.4403\n",
      "Epoch [9/10], Step [690/2737], Loss: 0.3008\n",
      "Epoch [9/10], Step [700/2737], Loss: 0.3849\n",
      "Epoch [9/10], Step [710/2737], Loss: 0.1937\n",
      "Epoch [9/10], Step [720/2737], Loss: 0.2387\n",
      "Epoch [9/10], Step [730/2737], Loss: 0.1816\n",
      "Epoch [9/10], Step [740/2737], Loss: 0.3086\n",
      "Epoch [9/10], Step [750/2737], Loss: 0.2118\n",
      "Epoch [9/10], Step [760/2737], Loss: 0.1481\n",
      "Epoch [9/10], Step [770/2737], Loss: 0.1165\n",
      "Epoch [9/10], Step [780/2737], Loss: 0.2239\n",
      "Epoch [9/10], Step [790/2737], Loss: 0.2740\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "#optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#training\n",
    "train_loss, train_acc, val_loss, val_acc= train(10, train_data_loader,  val_data_loader,criterion, optimizer, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDeBukX4oxnG"
   },
   "source": [
    "## **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bwgu8k1Xo0dd"
   },
   "outputs": [],
   "source": [
    "path = 'saved_models/task3/task3.pt'\n",
    "torch.save(net.state_dict(), path)\n",
    "\n",
    "np.save('saved_models/task3/train_loss', train_loss)\n",
    "np.save('saved_models/task3/val_loss', val_loss)\n",
    "np.save('saved_models/task3/train_acc', train_acc)\n",
    "np.save('saved_models/task3/val_acc', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0ZFjfN1pL-f"
   },
   "source": [
    "## **Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1651881080701,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "i7zafyKTpTm6",
    "outputId": "f7277920-892e-4bf9-e0ea-5afa55624056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 0, 1, 0, 0, 1],\n",
      "        [0, 1, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_data_loader)\n",
    "img, test_labels = dataiter.next()\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1651881083253,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "EjIAdRf2qwC7",
    "outputId": "327fea7b-975b-4986-b01b-54d320c883c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 1],\n",
       "        [1, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "net.load_state_dict(torch.load(path))\n",
    "test_out = net(img.to(device))\n",
    "test_out.shape\n",
    "pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in test_out])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4408\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "path = 'saved_models/task3/task3.pt'\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(path))\n",
    "\n",
    "test_steps = len(test_data_loader)\n",
    "t_acc=0\n",
    "for i, data in enumerate(test_data_loader): # iterate over batches\n",
    "    # get image and labels data is in tuple form (inputs, label)\n",
    "    image, labels = data\n",
    "    image = image.to(device)\n",
    "    labels = labels.to(device).float()\n",
    "    outputs = net(image)\n",
    "    pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "    t_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "\n",
    "print('Test Accuracy: {:.4f}'.format(t_acc/test_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc9du2KHpMFZ"
   },
   "source": [
    "## **Visualize result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2330,
     "status": "ok",
     "timestamp": 1651918809981,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "myKs939VpTFY",
    "outputId": "6b9b03e2-d08e-4ad6-a550-e81f77705b03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'training and validation loss')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_loss = np.load('saved_models/task2/train_loss.npy')\n",
    "val_loss = np.load('saved_models/task2/val_loss.npy')\n",
    "val_acc = np.load('saved_models/task2/val_acc.npy')\n",
    "train_acc = np.load('saved_models/task2/train_acc.npy')\n",
    "plt.plot(train_loss, linewidth=2, label='train')\n",
    "plt.plot(val_loss, linewidth=2, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.legend()\n",
    "plt.title('training and validation loss')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
