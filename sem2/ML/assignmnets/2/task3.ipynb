{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-5VWruMtjA4"
   },
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3133,
     "status": "ok",
     "timestamp": 1651937482560,
     "user": {
      "displayName": "ALI KHALID KHALID MAHMOOD AHMAD",
      "userId": "04961537570896709275"
     },
     "user_tz": -300
    },
    "id": "YOrsQs3przvi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1651936475942,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "D8ZOVQkkdMvS",
    "outputId": "c247195d-6d45-4e82-870a-ee0e2a59e56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLBCZshD33oh"
   },
   "source": [
    "## **Preparing Dataset for task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYMwW4MD6MMy"
   },
   "source": [
    "**Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NF3M2PUc6Lh3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\TEMP/ipykernel_21412/3358446840.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data['image']=new_data.index\n"
     ]
    }
   ],
   "source": [
    "# specify paths \n",
    "label_file = 'CelebA\\Anno\\list_attr_celeba.txt'\n",
    "img_dir = 'CelebA\\Img\\img_align_celeba\\img_align_celeba'\n",
    "output_dir = 'task3_data'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# load labels file\n",
    "data= pd.read_csv(label_file, sep='\\s+',skiprows=[0], header=0) #on_bad_lines='skip'\n",
    "data = data[['Male','Black_Hair','Wearing_Earrings','Eyeglasses','Straight_Hair','Smiling','Wearing_Necktie']]\n",
    "#data = data.reset_index()\n",
    "label_map = {1 : 1, -1 : 0}\n",
    "data = data.applymap(label_map.get)\n",
    "\n",
    "# choosing combinmation such that label for each category are almost equal\n",
    "new_data = data[ (data['Wearing_Earrings'] == 1) |(data['Eyeglasses'] == 1) |(data['Wearing_Necktie'] == 1)]\n",
    "new_data['image']=new_data.index\n",
    "\n",
    "# saving data to csv file\n",
    "new_data.to_csv(os.path.join(output_dir,'task3.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xImxLDAU-XOm"
   },
   "source": [
    "**Data Loading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651937487589,
     "user": {
      "displayName": "ALI KHALID KHALID MAHMOOD AHMAD",
      "userId": "04961537570896709275"
     },
     "user_tz": -300
    },
    "id": "VywF9NyEDj4d"
   },
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None): \n",
    "        # Run once\n",
    "        self.img_labels = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the number of samples in dataset\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # loads and returns a sample from the dataset at the given index\n",
    "        img_path = os.path.join(self.img_dir ,self.img_labels.iloc[idx, -1])\n",
    "        image = PIL.Image.open(img_path)\n",
    "        label = torch.tensor(self.img_labels.iloc[idx][0:-1])\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o7JasRaVQLBW"
   },
   "outputs": [],
   "source": [
    "def load_dataset(csv_file, img_dir,train_size, validation_size, test_size, batch_size):\n",
    "\n",
    "    transform = transforms.Compose([#transforms.Grayscale(), \n",
    "                                     transforms.ToTensor(),\n",
    "                                     #transforms.Resize((28,28)), \n",
    "                                     #transforms.Normalize(0,0.5)\n",
    "                                     ])\n",
    "    # read dataset\n",
    "    dataset = Custom_Dataset(csv_file=csv_file, img_dir=img_dir, transform = transform)\n",
    "\n",
    "    # specify sizes\n",
    "    train_set_size = int(len(dataset) * train_size)\n",
    "    test_set_size = int(len(dataset) * test_size)\n",
    "    val_set_size = len(dataset) - train_set_size - test_set_size\n",
    "\n",
    "    # split data\n",
    "    train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_set_size,val_set_size,test_set_size])\n",
    "\n",
    "    # create dataloader for each data (test,train,val)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #return\n",
    "    return train_data_loader, val_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "error",
     "timestamp": 1651918321207,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "yiTxJ9SLYaa0",
    "outputId": "48ac96f1-68bf-4f1e-b6df-685fa9453ed8"
   },
   "outputs": [],
   "source": [
    "img_dir = 'CelebA\\Img\\img_align_celeba\\img_align_celeba'\n",
    "csv_file = 'task3_data/task3.csv'\n",
    "train_data_loader, val_data_loader, test_data_loader = load_dataset(csv_file,img_dir,0.7,0.15,0.15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKBtkDmfaolk"
   },
   "source": [
    "## **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ys55GWVAatBt"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*218*178, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 800)\n",
    "        self.fc3 = nn.Linear(800, 400)\n",
    "        self.fc4 = nn.Linear(400, 200)\n",
    "        self.fc5 = nn.Linear(200, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# model\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPhzrua7hZrv"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ovdn0w0qhfSW"
   },
   "outputs": [],
   "source": [
    "def train(epochs, train_data_loader, val_data_loader, loss_func, optimizer, learning_rate):\n",
    "    train_step = len(train_data_loader)\n",
    "    val_step = len(val_data_loader)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    for epoch in range(epochs): # iterate over epochs\n",
    "        t_loss = 0\n",
    "        t_acc = 0 \n",
    "        for i, data in enumerate(train_data_loader): # iterate over batches\n",
    "            \n",
    "          # get image and labels data is in tuple form (inputs, label)\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "         \n",
    "            # Zero-out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(image)\n",
    "            pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_loss += loss.item()\n",
    "            t_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, epochs, i+1, train_step, loss.item()))\n",
    "    \n",
    "        v_loss = 0\n",
    "        v_acc = 0\n",
    "        for i, data in enumerate(val_data_loader): # iterate over batches\n",
    "            # get image and labels data is in tuple form (inputs, label)\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = net(image)\n",
    "            #pred = np.squeeze(outputs)\n",
    "            pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "            loss = loss_func(outputs, labels)\n",
    "            v_loss += loss.item()\n",
    "            v_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "            v_acc += torch.sum(pred == labels)/len(labels)\n",
    "     \n",
    "        train_loss.append(t_loss/train_step)\n",
    "        train_acc.append(t_acc/train_step)\n",
    "        val_loss.append(v_loss/val_step)\n",
    "        val_acc.append(v_acc/val_step)\n",
    "        print ('Epoch [{}/{}], train_loss: {:.4f}, val_loss: {:.4f}' .format(epoch+1, epochs, train_loss[-1], val_loss[-1]))\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 106559,
     "status": "error",
     "timestamp": 1651881065083,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "pN0Nkf8Fh8fg",
    "outputId": "2f748680-34a9-46cf-e93f-370e7bcc611a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/2737], Loss: 0.6347\n",
      "Epoch [1/10], Step [20/2737], Loss: 0.6412\n",
      "Epoch [1/10], Step [30/2737], Loss: 0.5690\n",
      "Epoch [1/10], Step [40/2737], Loss: 0.5582\n",
      "Epoch [1/10], Step [50/2737], Loss: 0.8029\n",
      "Epoch [1/10], Step [60/2737], Loss: 0.5839\n",
      "Epoch [1/10], Step [70/2737], Loss: 0.5659\n",
      "Epoch [1/10], Step [80/2737], Loss: 0.5461\n",
      "Epoch [1/10], Step [90/2737], Loss: 0.6123\n",
      "Epoch [1/10], Step [100/2737], Loss: 0.5203\n",
      "Epoch [1/10], Step [110/2737], Loss: 0.5736\n",
      "Epoch [1/10], Step [120/2737], Loss: 0.4318\n",
      "Epoch [1/10], Step [130/2737], Loss: 0.4522\n",
      "Epoch [1/10], Step [140/2737], Loss: 0.4945\n",
      "Epoch [1/10], Step [150/2737], Loss: 0.4027\n",
      "Epoch [1/10], Step [160/2737], Loss: 0.4849\n",
      "Epoch [1/10], Step [170/2737], Loss: 0.4991\n",
      "Epoch [1/10], Step [180/2737], Loss: 0.4220\n",
      "Epoch [1/10], Step [190/2737], Loss: 0.3319\n",
      "Epoch [1/10], Step [200/2737], Loss: 0.5126\n",
      "Epoch [1/10], Step [210/2737], Loss: 0.3898\n",
      "Epoch [1/10], Step [220/2737], Loss: 0.4510\n",
      "Epoch [1/10], Step [230/2737], Loss: 0.4299\n",
      "Epoch [1/10], Step [240/2737], Loss: 0.4866\n",
      "Epoch [1/10], Step [250/2737], Loss: 0.3114\n",
      "Epoch [1/10], Step [260/2737], Loss: 0.4400\n",
      "Epoch [1/10], Step [270/2737], Loss: 0.6014\n",
      "Epoch [1/10], Step [280/2737], Loss: 0.5712\n",
      "Epoch [1/10], Step [290/2737], Loss: 0.4391\n",
      "Epoch [1/10], Step [300/2737], Loss: 0.5079\n",
      "Epoch [1/10], Step [310/2737], Loss: 0.4629\n",
      "Epoch [1/10], Step [320/2737], Loss: 0.3793\n",
      "Epoch [1/10], Step [330/2737], Loss: 0.6005\n",
      "Epoch [1/10], Step [340/2737], Loss: 0.4013\n",
      "Epoch [1/10], Step [350/2737], Loss: 0.4369\n",
      "Epoch [1/10], Step [360/2737], Loss: 0.4248\n",
      "Epoch [1/10], Step [370/2737], Loss: 0.3532\n",
      "Epoch [1/10], Step [380/2737], Loss: 0.3814\n",
      "Epoch [1/10], Step [390/2737], Loss: 0.4543\n",
      "Epoch [1/10], Step [400/2737], Loss: 0.4438\n",
      "Epoch [1/10], Step [410/2737], Loss: 0.3316\n",
      "Epoch [1/10], Step [420/2737], Loss: 0.3299\n",
      "Epoch [1/10], Step [430/2737], Loss: 0.3133\n",
      "Epoch [1/10], Step [440/2737], Loss: 0.4480\n",
      "Epoch [1/10], Step [450/2737], Loss: 0.3360\n",
      "Epoch [1/10], Step [460/2737], Loss: 0.4705\n",
      "Epoch [1/10], Step [470/2737], Loss: 0.5510\n",
      "Epoch [1/10], Step [480/2737], Loss: 0.3737\n",
      "Epoch [1/10], Step [490/2737], Loss: 0.5420\n",
      "Epoch [1/10], Step [500/2737], Loss: 0.3433\n",
      "Epoch [1/10], Step [510/2737], Loss: 0.3490\n",
      "Epoch [1/10], Step [520/2737], Loss: 0.3436\n",
      "Epoch [1/10], Step [530/2737], Loss: 0.3966\n",
      "Epoch [1/10], Step [540/2737], Loss: 0.3438\n",
      "Epoch [1/10], Step [550/2737], Loss: 0.3192\n",
      "Epoch [1/10], Step [560/2737], Loss: 0.3798\n",
      "Epoch [1/10], Step [570/2737], Loss: 0.3034\n",
      "Epoch [1/10], Step [580/2737], Loss: 0.4945\n",
      "Epoch [1/10], Step [590/2737], Loss: 0.3745\n",
      "Epoch [1/10], Step [600/2737], Loss: 0.4667\n",
      "Epoch [1/10], Step [610/2737], Loss: 0.4449\n",
      "Epoch [1/10], Step [620/2737], Loss: 0.3391\n",
      "Epoch [1/10], Step [630/2737], Loss: 0.3831\n",
      "Epoch [1/10], Step [640/2737], Loss: 0.4574\n",
      "Epoch [1/10], Step [650/2737], Loss: 0.3528\n",
      "Epoch [1/10], Step [660/2737], Loss: 0.6742\n",
      "Epoch [1/10], Step [670/2737], Loss: 0.3640\n",
      "Epoch [1/10], Step [680/2737], Loss: 0.4181\n",
      "Epoch [1/10], Step [690/2737], Loss: 0.3586\n",
      "Epoch [1/10], Step [700/2737], Loss: 0.3501\n",
      "Epoch [1/10], Step [710/2737], Loss: 0.3959\n",
      "Epoch [1/10], Step [720/2737], Loss: 0.3788\n",
      "Epoch [1/10], Step [730/2737], Loss: 0.2791\n",
      "Epoch [1/10], Step [740/2737], Loss: 0.3418\n",
      "Epoch [1/10], Step [750/2737], Loss: 0.3650\n",
      "Epoch [1/10], Step [760/2737], Loss: 0.4469\n",
      "Epoch [1/10], Step [770/2737], Loss: 0.4107\n",
      "Epoch [1/10], Step [780/2737], Loss: 0.4397\n",
      "Epoch [1/10], Step [790/2737], Loss: 0.3722\n",
      "Epoch [1/10], Step [800/2737], Loss: 0.3678\n",
      "Epoch [1/10], Step [810/2737], Loss: 0.3397\n",
      "Epoch [1/10], Step [820/2737], Loss: 0.4227\n",
      "Epoch [1/10], Step [830/2737], Loss: 0.3558\n",
      "Epoch [1/10], Step [840/2737], Loss: 0.2607\n",
      "Epoch [1/10], Step [850/2737], Loss: 0.4413\n",
      "Epoch [1/10], Step [860/2737], Loss: 0.2922\n",
      "Epoch [1/10], Step [870/2737], Loss: 0.3331\n",
      "Epoch [1/10], Step [880/2737], Loss: 0.4401\n",
      "Epoch [1/10], Step [890/2737], Loss: 0.2690\n",
      "Epoch [1/10], Step [900/2737], Loss: 0.2908\n",
      "Epoch [1/10], Step [910/2737], Loss: 0.4645\n",
      "Epoch [1/10], Step [920/2737], Loss: 0.3796\n",
      "Epoch [1/10], Step [930/2737], Loss: 0.3020\n",
      "Epoch [1/10], Step [940/2737], Loss: 0.4379\n",
      "Epoch [1/10], Step [950/2737], Loss: 0.4690\n",
      "Epoch [1/10], Step [960/2737], Loss: 0.1816\n",
      "Epoch [1/10], Step [970/2737], Loss: 0.4188\n",
      "Epoch [1/10], Step [980/2737], Loss: 0.2630\n",
      "Epoch [1/10], Step [990/2737], Loss: 0.5335\n",
      "Epoch [1/10], Step [1000/2737], Loss: 0.4984\n",
      "Epoch [1/10], Step [1010/2737], Loss: 0.3785\n",
      "Epoch [1/10], Step [1020/2737], Loss: 0.2460\n",
      "Epoch [1/10], Step [1030/2737], Loss: 0.3400\n",
      "Epoch [1/10], Step [1040/2737], Loss: 0.3245\n",
      "Epoch [1/10], Step [1050/2737], Loss: 0.4029\n",
      "Epoch [1/10], Step [1060/2737], Loss: 0.2900\n",
      "Epoch [1/10], Step [1070/2737], Loss: 0.3600\n",
      "Epoch [1/10], Step [1080/2737], Loss: 0.4334\n",
      "Epoch [1/10], Step [1090/2737], Loss: 0.2401\n",
      "Epoch [1/10], Step [1100/2737], Loss: 0.3983\n",
      "Epoch [1/10], Step [1110/2737], Loss: 0.4932\n",
      "Epoch [1/10], Step [1120/2737], Loss: 0.2846\n",
      "Epoch [1/10], Step [1130/2737], Loss: 0.3737\n",
      "Epoch [1/10], Step [1140/2737], Loss: 0.3397\n",
      "Epoch [1/10], Step [1150/2737], Loss: 0.2977\n",
      "Epoch [1/10], Step [1160/2737], Loss: 0.5110\n",
      "Epoch [1/10], Step [1170/2737], Loss: 0.2850\n",
      "Epoch [1/10], Step [1180/2737], Loss: 0.2504\n",
      "Epoch [1/10], Step [1190/2737], Loss: 0.3118\n",
      "Epoch [1/10], Step [1200/2737], Loss: 0.3650\n",
      "Epoch [1/10], Step [1210/2737], Loss: 0.3575\n",
      "Epoch [1/10], Step [1220/2737], Loss: 0.4839\n",
      "Epoch [1/10], Step [1230/2737], Loss: 0.4914\n",
      "Epoch [1/10], Step [1240/2737], Loss: 0.4012\n",
      "Epoch [1/10], Step [1250/2737], Loss: 0.4397\n",
      "Epoch [1/10], Step [1260/2737], Loss: 0.4079\n",
      "Epoch [1/10], Step [1270/2737], Loss: 0.3430\n",
      "Epoch [1/10], Step [1280/2737], Loss: 0.2449\n",
      "Epoch [1/10], Step [1290/2737], Loss: 0.3647\n",
      "Epoch [1/10], Step [1300/2737], Loss: 0.3539\n",
      "Epoch [1/10], Step [1310/2737], Loss: 0.2687\n",
      "Epoch [1/10], Step [1320/2737], Loss: 0.4603\n",
      "Epoch [1/10], Step [1330/2737], Loss: 0.3444\n",
      "Epoch [1/10], Step [1340/2737], Loss: 0.4164\n",
      "Epoch [1/10], Step [1350/2737], Loss: 0.4203\n",
      "Epoch [1/10], Step [1360/2737], Loss: 0.4069\n",
      "Epoch [1/10], Step [1370/2737], Loss: 0.3241\n",
      "Epoch [1/10], Step [1380/2737], Loss: 0.2936\n",
      "Epoch [1/10], Step [1390/2737], Loss: 0.2853\n",
      "Epoch [1/10], Step [1400/2737], Loss: 0.2112\n",
      "Epoch [1/10], Step [1410/2737], Loss: 0.2518\n",
      "Epoch [1/10], Step [1420/2737], Loss: 0.2984\n",
      "Epoch [1/10], Step [1430/2737], Loss: 0.3444\n",
      "Epoch [1/10], Step [1440/2737], Loss: 0.2562\n",
      "Epoch [1/10], Step [1450/2737], Loss: 0.2882\n",
      "Epoch [1/10], Step [1460/2737], Loss: 0.3764\n",
      "Epoch [1/10], Step [1470/2737], Loss: 0.3236\n",
      "Epoch [1/10], Step [1480/2737], Loss: 0.2801\n",
      "Epoch [1/10], Step [1490/2737], Loss: 0.3630\n",
      "Epoch [1/10], Step [1500/2737], Loss: 0.2848\n",
      "Epoch [1/10], Step [1510/2737], Loss: 0.2013\n",
      "Epoch [1/10], Step [1520/2737], Loss: 0.2342\n",
      "Epoch [1/10], Step [1530/2737], Loss: 0.4347\n",
      "Epoch [1/10], Step [1540/2737], Loss: 0.2487\n",
      "Epoch [1/10], Step [1550/2737], Loss: 0.3629\n",
      "Epoch [1/10], Step [1560/2737], Loss: 0.3145\n",
      "Epoch [1/10], Step [1570/2737], Loss: 0.3211\n",
      "Epoch [1/10], Step [1580/2737], Loss: 0.2647\n",
      "Epoch [1/10], Step [1590/2737], Loss: 0.2772\n",
      "Epoch [1/10], Step [1600/2737], Loss: 0.3572\n",
      "Epoch [1/10], Step [1610/2737], Loss: 0.2498\n",
      "Epoch [1/10], Step [1620/2737], Loss: 0.2926\n",
      "Epoch [1/10], Step [1630/2737], Loss: 0.3137\n",
      "Epoch [1/10], Step [1640/2737], Loss: 0.3792\n",
      "Epoch [1/10], Step [1650/2737], Loss: 0.3304\n",
      "Epoch [1/10], Step [1660/2737], Loss: 0.2395\n",
      "Epoch [1/10], Step [1670/2737], Loss: 0.1984\n",
      "Epoch [1/10], Step [1680/2737], Loss: 0.2898\n",
      "Epoch [1/10], Step [1690/2737], Loss: 0.2663\n",
      "Epoch [1/10], Step [1700/2737], Loss: 0.4083\n",
      "Epoch [1/10], Step [1710/2737], Loss: 0.3004\n",
      "Epoch [1/10], Step [1720/2737], Loss: 0.3836\n",
      "Epoch [1/10], Step [1730/2737], Loss: 0.3114\n",
      "Epoch [1/10], Step [1740/2737], Loss: 0.3434\n",
      "Epoch [1/10], Step [1750/2737], Loss: 0.3468\n",
      "Epoch [1/10], Step [1760/2737], Loss: 0.3810\n",
      "Epoch [1/10], Step [1770/2737], Loss: 0.3296\n",
      "Epoch [1/10], Step [1780/2737], Loss: 0.2838\n",
      "Epoch [1/10], Step [1790/2737], Loss: 0.2718\n",
      "Epoch [1/10], Step [1800/2737], Loss: 0.2558\n",
      "Epoch [1/10], Step [1810/2737], Loss: 0.3266\n",
      "Epoch [1/10], Step [1820/2737], Loss: 0.3492\n",
      "Epoch [1/10], Step [1830/2737], Loss: 0.2857\n",
      "Epoch [1/10], Step [1840/2737], Loss: 0.2076\n",
      "Epoch [1/10], Step [1850/2737], Loss: 0.2512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1860/2737], Loss: 0.6161\n",
      "Epoch [1/10], Step [1870/2737], Loss: 0.4435\n",
      "Epoch [1/10], Step [1880/2737], Loss: 0.3535\n",
      "Epoch [1/10], Step [1890/2737], Loss: 0.2830\n",
      "Epoch [1/10], Step [1900/2737], Loss: 0.3222\n",
      "Epoch [1/10], Step [1910/2737], Loss: 0.2536\n",
      "Epoch [1/10], Step [1920/2737], Loss: 0.3628\n",
      "Epoch [1/10], Step [1930/2737], Loss: 0.2671\n",
      "Epoch [1/10], Step [1940/2737], Loss: 0.4350\n",
      "Epoch [1/10], Step [1950/2737], Loss: 0.2387\n",
      "Epoch [1/10], Step [1960/2737], Loss: 0.3143\n",
      "Epoch [1/10], Step [1970/2737], Loss: 0.2704\n",
      "Epoch [1/10], Step [1980/2737], Loss: 0.3894\n",
      "Epoch [1/10], Step [1990/2737], Loss: 0.3281\n",
      "Epoch [1/10], Step [2000/2737], Loss: 0.3664\n",
      "Epoch [1/10], Step [2010/2737], Loss: 0.1877\n",
      "Epoch [1/10], Step [2020/2737], Loss: 0.2870\n",
      "Epoch [1/10], Step [2030/2737], Loss: 0.4393\n",
      "Epoch [1/10], Step [2040/2737], Loss: 0.3046\n",
      "Epoch [1/10], Step [2050/2737], Loss: 0.2980\n",
      "Epoch [1/10], Step [2060/2737], Loss: 0.4119\n",
      "Epoch [1/10], Step [2070/2737], Loss: 0.3236\n",
      "Epoch [1/10], Step [2080/2737], Loss: 0.4050\n",
      "Epoch [1/10], Step [2090/2737], Loss: 0.2724\n",
      "Epoch [1/10], Step [2100/2737], Loss: 0.4611\n",
      "Epoch [1/10], Step [2110/2737], Loss: 0.2472\n",
      "Epoch [1/10], Step [2120/2737], Loss: 0.3386\n",
      "Epoch [1/10], Step [2130/2737], Loss: 0.3515\n",
      "Epoch [1/10], Step [2140/2737], Loss: 0.2965\n",
      "Epoch [1/10], Step [2150/2737], Loss: 0.3455\n",
      "Epoch [1/10], Step [2160/2737], Loss: 0.2966\n",
      "Epoch [1/10], Step [2170/2737], Loss: 0.2532\n",
      "Epoch [1/10], Step [2180/2737], Loss: 0.2742\n",
      "Epoch [1/10], Step [2190/2737], Loss: 0.2973\n",
      "Epoch [1/10], Step [2200/2737], Loss: 0.1870\n",
      "Epoch [1/10], Step [2210/2737], Loss: 0.3272\n",
      "Epoch [1/10], Step [2220/2737], Loss: 0.3214\n",
      "Epoch [1/10], Step [2230/2737], Loss: 0.2485\n",
      "Epoch [1/10], Step [2240/2737], Loss: 0.1989\n",
      "Epoch [1/10], Step [2250/2737], Loss: 0.1922\n",
      "Epoch [1/10], Step [2260/2737], Loss: 0.2426\n",
      "Epoch [1/10], Step [2270/2737], Loss: 0.2566\n",
      "Epoch [1/10], Step [2280/2737], Loss: 0.2392\n",
      "Epoch [1/10], Step [2290/2737], Loss: 0.2705\n",
      "Epoch [1/10], Step [2300/2737], Loss: 0.2951\n",
      "Epoch [1/10], Step [2310/2737], Loss: 0.2953\n",
      "Epoch [1/10], Step [2320/2737], Loss: 0.3213\n",
      "Epoch [1/10], Step [2330/2737], Loss: 0.2589\n",
      "Epoch [1/10], Step [2340/2737], Loss: 0.2620\n",
      "Epoch [1/10], Step [2350/2737], Loss: 0.4424\n",
      "Epoch [1/10], Step [2360/2737], Loss: 0.4139\n",
      "Epoch [1/10], Step [2370/2737], Loss: 0.3125\n",
      "Epoch [1/10], Step [2380/2737], Loss: 0.3646\n",
      "Epoch [1/10], Step [2390/2737], Loss: 0.3448\n",
      "Epoch [1/10], Step [2400/2737], Loss: 0.3325\n",
      "Epoch [1/10], Step [2410/2737], Loss: 0.2897\n",
      "Epoch [1/10], Step [2420/2737], Loss: 0.4350\n",
      "Epoch [1/10], Step [2430/2737], Loss: 0.2779\n",
      "Epoch [1/10], Step [2440/2737], Loss: 0.3315\n",
      "Epoch [1/10], Step [2450/2737], Loss: 0.4091\n",
      "Epoch [1/10], Step [2460/2737], Loss: 0.3135\n",
      "Epoch [1/10], Step [2470/2737], Loss: 0.2908\n",
      "Epoch [1/10], Step [2480/2737], Loss: 0.4593\n",
      "Epoch [1/10], Step [2490/2737], Loss: 0.2502\n",
      "Epoch [1/10], Step [2500/2737], Loss: 0.5488\n",
      "Epoch [1/10], Step [2510/2737], Loss: 0.2763\n",
      "Epoch [1/10], Step [2520/2737], Loss: 0.2806\n",
      "Epoch [1/10], Step [2530/2737], Loss: 0.2376\n",
      "Epoch [1/10], Step [2540/2737], Loss: 0.3657\n",
      "Epoch [1/10], Step [2550/2737], Loss: 0.3551\n",
      "Epoch [1/10], Step [2560/2737], Loss: 0.2763\n",
      "Epoch [1/10], Step [2570/2737], Loss: 0.2515\n",
      "Epoch [1/10], Step [2580/2737], Loss: 0.3588\n",
      "Epoch [1/10], Step [2590/2737], Loss: 0.2913\n",
      "Epoch [1/10], Step [2600/2737], Loss: 0.2884\n",
      "Epoch [1/10], Step [2610/2737], Loss: 0.4628\n",
      "Epoch [1/10], Step [2620/2737], Loss: 0.2482\n",
      "Epoch [1/10], Step [2630/2737], Loss: 0.2961\n",
      "Epoch [1/10], Step [2640/2737], Loss: 0.2562\n",
      "Epoch [1/10], Step [2650/2737], Loss: 0.3311\n",
      "Epoch [1/10], Step [2660/2737], Loss: 0.2540\n",
      "Epoch [1/10], Step [2670/2737], Loss: 0.3189\n",
      "Epoch [1/10], Step [2680/2737], Loss: 0.2783\n",
      "Epoch [1/10], Step [2690/2737], Loss: 0.2649\n",
      "Epoch [1/10], Step [2700/2737], Loss: 0.3987\n",
      "Epoch [1/10], Step [2710/2737], Loss: 0.2813\n",
      "Epoch [1/10], Step [2720/2737], Loss: 0.3455\n",
      "Epoch [1/10], Step [2730/2737], Loss: 0.3220\n",
      "Epoch [1/10], train_loss: 0.3532, val_loss: 0.2786\n",
      "Epoch [2/10], Step [10/2737], Loss: 0.1948\n",
      "Epoch [2/10], Step [20/2737], Loss: 0.2215\n",
      "Epoch [2/10], Step [30/2737], Loss: 0.3229\n",
      "Epoch [2/10], Step [40/2737], Loss: 0.2095\n",
      "Epoch [2/10], Step [50/2737], Loss: 0.2611\n",
      "Epoch [2/10], Step [60/2737], Loss: 0.3777\n",
      "Epoch [2/10], Step [70/2737], Loss: 0.1583\n",
      "Epoch [2/10], Step [80/2737], Loss: 0.2646\n",
      "Epoch [2/10], Step [90/2737], Loss: 0.4058\n",
      "Epoch [2/10], Step [100/2737], Loss: 0.2776\n",
      "Epoch [2/10], Step [110/2737], Loss: 0.3061\n",
      "Epoch [2/10], Step [120/2737], Loss: 0.2384\n",
      "Epoch [2/10], Step [130/2737], Loss: 0.3859\n",
      "Epoch [2/10], Step [140/2737], Loss: 0.4587\n",
      "Epoch [2/10], Step [150/2737], Loss: 0.5029\n",
      "Epoch [2/10], Step [160/2737], Loss: 0.2766\n",
      "Epoch [2/10], Step [170/2737], Loss: 0.2518\n",
      "Epoch [2/10], Step [180/2737], Loss: 0.1587\n",
      "Epoch [2/10], Step [190/2737], Loss: 0.4281\n",
      "Epoch [2/10], Step [200/2737], Loss: 0.2839\n",
      "Epoch [2/10], Step [210/2737], Loss: 0.2122\n",
      "Epoch [2/10], Step [220/2737], Loss: 0.2592\n",
      "Epoch [2/10], Step [230/2737], Loss: 0.3165\n",
      "Epoch [2/10], Step [240/2737], Loss: 0.2765\n",
      "Epoch [2/10], Step [250/2737], Loss: 0.2689\n",
      "Epoch [2/10], Step [260/2737], Loss: 0.2827\n",
      "Epoch [2/10], Step [270/2737], Loss: 0.1556\n",
      "Epoch [2/10], Step [280/2737], Loss: 0.2998\n",
      "Epoch [2/10], Step [290/2737], Loss: 0.2944\n",
      "Epoch [2/10], Step [300/2737], Loss: 0.3321\n",
      "Epoch [2/10], Step [310/2737], Loss: 0.1481\n",
      "Epoch [2/10], Step [320/2737], Loss: 0.3072\n",
      "Epoch [2/10], Step [330/2737], Loss: 0.3709\n",
      "Epoch [2/10], Step [340/2737], Loss: 0.3433\n",
      "Epoch [2/10], Step [350/2737], Loss: 0.3453\n",
      "Epoch [2/10], Step [360/2737], Loss: 0.2309\n",
      "Epoch [2/10], Step [370/2737], Loss: 0.2981\n",
      "Epoch [2/10], Step [380/2737], Loss: 0.2936\n",
      "Epoch [2/10], Step [390/2737], Loss: 0.2490\n",
      "Epoch [2/10], Step [400/2737], Loss: 0.2518\n",
      "Epoch [2/10], Step [410/2737], Loss: 0.3046\n",
      "Epoch [2/10], Step [420/2737], Loss: 0.3520\n",
      "Epoch [2/10], Step [430/2737], Loss: 0.2988\n",
      "Epoch [2/10], Step [440/2737], Loss: 0.3078\n",
      "Epoch [2/10], Step [450/2737], Loss: 0.3054\n",
      "Epoch [2/10], Step [460/2737], Loss: 0.3102\n",
      "Epoch [2/10], Step [470/2737], Loss: 0.2127\n",
      "Epoch [2/10], Step [480/2737], Loss: 0.2896\n",
      "Epoch [2/10], Step [490/2737], Loss: 0.2803\n",
      "Epoch [2/10], Step [500/2737], Loss: 0.2311\n",
      "Epoch [2/10], Step [510/2737], Loss: 0.2661\n",
      "Epoch [2/10], Step [520/2737], Loss: 0.1799\n",
      "Epoch [2/10], Step [530/2737], Loss: 0.3493\n",
      "Epoch [2/10], Step [540/2737], Loss: 0.3496\n",
      "Epoch [2/10], Step [550/2737], Loss: 0.3174\n",
      "Epoch [2/10], Step [560/2737], Loss: 0.2696\n",
      "Epoch [2/10], Step [570/2737], Loss: 0.3591\n",
      "Epoch [2/10], Step [580/2737], Loss: 0.1832\n",
      "Epoch [2/10], Step [590/2737], Loss: 0.2550\n",
      "Epoch [2/10], Step [600/2737], Loss: 0.1712\n",
      "Epoch [2/10], Step [610/2737], Loss: 0.2624\n",
      "Epoch [2/10], Step [620/2737], Loss: 0.4196\n",
      "Epoch [2/10], Step [630/2737], Loss: 0.3019\n",
      "Epoch [2/10], Step [640/2737], Loss: 0.3208\n",
      "Epoch [2/10], Step [650/2737], Loss: 0.2418\n",
      "Epoch [2/10], Step [660/2737], Loss: 0.2263\n",
      "Epoch [2/10], Step [670/2737], Loss: 0.2366\n",
      "Epoch [2/10], Step [680/2737], Loss: 0.2537\n",
      "Epoch [2/10], Step [690/2737], Loss: 0.3575\n",
      "Epoch [2/10], Step [700/2737], Loss: 0.3000\n",
      "Epoch [2/10], Step [710/2737], Loss: 0.2911\n",
      "Epoch [2/10], Step [720/2737], Loss: 0.2991\n",
      "Epoch [2/10], Step [730/2737], Loss: 0.3500\n",
      "Epoch [2/10], Step [740/2737], Loss: 0.1960\n",
      "Epoch [2/10], Step [750/2737], Loss: 0.2184\n",
      "Epoch [2/10], Step [760/2737], Loss: 0.2928\n",
      "Epoch [2/10], Step [770/2737], Loss: 0.1464\n",
      "Epoch [2/10], Step [780/2737], Loss: 0.1584\n",
      "Epoch [2/10], Step [790/2737], Loss: 0.4475\n",
      "Epoch [2/10], Step [800/2737], Loss: 0.2794\n",
      "Epoch [2/10], Step [810/2737], Loss: 0.2365\n",
      "Epoch [2/10], Step [820/2737], Loss: 0.3158\n",
      "Epoch [2/10], Step [830/2737], Loss: 0.3801\n",
      "Epoch [2/10], Step [840/2737], Loss: 0.2477\n",
      "Epoch [2/10], Step [850/2737], Loss: 0.3431\n",
      "Epoch [2/10], Step [860/2737], Loss: 0.2801\n",
      "Epoch [2/10], Step [870/2737], Loss: 0.3080\n",
      "Epoch [2/10], Step [880/2737], Loss: 0.3751\n",
      "Epoch [2/10], Step [890/2737], Loss: 0.3612\n",
      "Epoch [2/10], Step [900/2737], Loss: 0.3367\n",
      "Epoch [2/10], Step [910/2737], Loss: 0.3230\n",
      "Epoch [2/10], Step [920/2737], Loss: 0.2491\n",
      "Epoch [2/10], Step [930/2737], Loss: 0.2393\n",
      "Epoch [2/10], Step [940/2737], Loss: 0.3161\n",
      "Epoch [2/10], Step [950/2737], Loss: 0.2386\n",
      "Epoch [2/10], Step [960/2737], Loss: 0.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [970/2737], Loss: 0.2124\n",
      "Epoch [2/10], Step [980/2737], Loss: 0.2613\n",
      "Epoch [2/10], Step [990/2737], Loss: 0.3166\n",
      "Epoch [2/10], Step [1000/2737], Loss: 0.3213\n",
      "Epoch [2/10], Step [1010/2737], Loss: 0.2558\n",
      "Epoch [2/10], Step [1020/2737], Loss: 0.2922\n",
      "Epoch [2/10], Step [1030/2737], Loss: 0.3712\n",
      "Epoch [2/10], Step [1040/2737], Loss: 0.2259\n",
      "Epoch [2/10], Step [1050/2737], Loss: 0.4103\n",
      "Epoch [2/10], Step [1060/2737], Loss: 0.3622\n",
      "Epoch [2/10], Step [1070/2737], Loss: 0.3085\n",
      "Epoch [2/10], Step [1080/2737], Loss: 0.2129\n",
      "Epoch [2/10], Step [1090/2737], Loss: 0.3451\n",
      "Epoch [2/10], Step [1100/2737], Loss: 0.3445\n",
      "Epoch [2/10], Step [1110/2737], Loss: 0.2603\n",
      "Epoch [2/10], Step [1120/2737], Loss: 0.2954\n",
      "Epoch [2/10], Step [1130/2737], Loss: 0.3169\n",
      "Epoch [2/10], Step [1140/2737], Loss: 0.2361\n",
      "Epoch [2/10], Step [1150/2737], Loss: 0.3008\n",
      "Epoch [2/10], Step [1160/2737], Loss: 0.2211\n",
      "Epoch [2/10], Step [1170/2737], Loss: 0.3245\n",
      "Epoch [2/10], Step [1180/2737], Loss: 0.2422\n",
      "Epoch [2/10], Step [1190/2737], Loss: 0.3583\n",
      "Epoch [2/10], Step [1200/2737], Loss: 0.3922\n",
      "Epoch [2/10], Step [1210/2737], Loss: 0.3436\n",
      "Epoch [2/10], Step [1220/2737], Loss: 0.2441\n",
      "Epoch [2/10], Step [1230/2737], Loss: 0.3068\n",
      "Epoch [2/10], Step [1240/2737], Loss: 0.3584\n",
      "Epoch [2/10], Step [1250/2737], Loss: 0.4079\n",
      "Epoch [2/10], Step [1260/2737], Loss: 0.3259\n",
      "Epoch [2/10], Step [1270/2737], Loss: 0.3404\n",
      "Epoch [2/10], Step [1280/2737], Loss: 0.2780\n",
      "Epoch [2/10], Step [1290/2737], Loss: 0.1993\n",
      "Epoch [2/10], Step [1300/2737], Loss: 0.2764\n",
      "Epoch [2/10], Step [1310/2737], Loss: 0.3581\n",
      "Epoch [2/10], Step [1320/2737], Loss: 0.2305\n",
      "Epoch [2/10], Step [1330/2737], Loss: 0.3435\n",
      "Epoch [2/10], Step [1340/2737], Loss: 0.2182\n",
      "Epoch [2/10], Step [1350/2737], Loss: 0.3003\n",
      "Epoch [2/10], Step [1360/2737], Loss: 0.2957\n",
      "Epoch [2/10], Step [1370/2737], Loss: 0.2346\n",
      "Epoch [2/10], Step [1380/2737], Loss: 0.3530\n",
      "Epoch [2/10], Step [1390/2737], Loss: 0.3209\n",
      "Epoch [2/10], Step [1400/2737], Loss: 0.1644\n",
      "Epoch [2/10], Step [1410/2737], Loss: 0.2684\n",
      "Epoch [2/10], Step [1420/2737], Loss: 0.2453\n",
      "Epoch [2/10], Step [1430/2737], Loss: 0.1491\n",
      "Epoch [2/10], Step [1440/2737], Loss: 0.3047\n",
      "Epoch [2/10], Step [1450/2737], Loss: 0.2769\n",
      "Epoch [2/10], Step [1460/2737], Loss: 0.2697\n",
      "Epoch [2/10], Step [1470/2737], Loss: 0.4954\n",
      "Epoch [2/10], Step [1480/2737], Loss: 0.2432\n",
      "Epoch [2/10], Step [1490/2737], Loss: 0.3405\n",
      "Epoch [2/10], Step [1500/2737], Loss: 0.2512\n",
      "Epoch [2/10], Step [1510/2737], Loss: 0.3152\n",
      "Epoch [2/10], Step [1520/2737], Loss: 0.2453\n",
      "Epoch [2/10], Step [1530/2737], Loss: 0.3315\n",
      "Epoch [2/10], Step [1540/2737], Loss: 0.2620\n",
      "Epoch [2/10], Step [1550/2737], Loss: 0.2538\n",
      "Epoch [2/10], Step [1560/2737], Loss: 0.3547\n",
      "Epoch [2/10], Step [1570/2737], Loss: 0.2355\n",
      "Epoch [2/10], Step [1580/2737], Loss: 0.4180\n",
      "Epoch [2/10], Step [1590/2737], Loss: 0.2346\n",
      "Epoch [2/10], Step [1600/2737], Loss: 0.2810\n",
      "Epoch [2/10], Step [1610/2737], Loss: 0.2074\n",
      "Epoch [2/10], Step [1620/2737], Loss: 0.2465\n",
      "Epoch [2/10], Step [1630/2737], Loss: 0.2084\n",
      "Epoch [2/10], Step [1640/2737], Loss: 0.3350\n",
      "Epoch [2/10], Step [1650/2737], Loss: 0.2591\n",
      "Epoch [2/10], Step [1660/2737], Loss: 0.3070\n",
      "Epoch [2/10], Step [1670/2737], Loss: 0.2975\n",
      "Epoch [2/10], Step [1680/2737], Loss: 0.3409\n",
      "Epoch [2/10], Step [1690/2737], Loss: 0.2204\n",
      "Epoch [2/10], Step [1700/2737], Loss: 0.4147\n",
      "Epoch [2/10], Step [1710/2737], Loss: 0.2604\n",
      "Epoch [2/10], Step [1720/2737], Loss: 0.2999\n",
      "Epoch [2/10], Step [1730/2737], Loss: 0.3302\n",
      "Epoch [2/10], Step [1740/2737], Loss: 0.2229\n",
      "Epoch [2/10], Step [1750/2737], Loss: 0.2419\n",
      "Epoch [2/10], Step [1760/2737], Loss: 0.2602\n",
      "Epoch [2/10], Step [1770/2737], Loss: 0.2119\n",
      "Epoch [2/10], Step [1780/2737], Loss: 0.2618\n",
      "Epoch [2/10], Step [1790/2737], Loss: 0.2381\n",
      "Epoch [2/10], Step [1800/2737], Loss: 0.2910\n",
      "Epoch [2/10], Step [1810/2737], Loss: 0.5176\n",
      "Epoch [2/10], Step [1820/2737], Loss: 0.2684\n",
      "Epoch [2/10], Step [1830/2737], Loss: 0.3837\n",
      "Epoch [2/10], Step [1840/2737], Loss: 0.3372\n",
      "Epoch [2/10], Step [1850/2737], Loss: 0.3257\n",
      "Epoch [2/10], Step [1860/2737], Loss: 0.1877\n",
      "Epoch [2/10], Step [1870/2737], Loss: 0.3112\n",
      "Epoch [2/10], Step [1880/2737], Loss: 0.3301\n",
      "Epoch [2/10], Step [1890/2737], Loss: 0.1941\n",
      "Epoch [2/10], Step [1900/2737], Loss: 0.2181\n",
      "Epoch [2/10], Step [1910/2737], Loss: 0.3129\n",
      "Epoch [2/10], Step [1920/2737], Loss: 0.1620\n",
      "Epoch [2/10], Step [1930/2737], Loss: 0.2935\n",
      "Epoch [2/10], Step [1940/2737], Loss: 0.3706\n",
      "Epoch [2/10], Step [1950/2737], Loss: 0.1351\n",
      "Epoch [2/10], Step [1960/2737], Loss: 0.2350\n",
      "Epoch [2/10], Step [1970/2737], Loss: 0.2348\n",
      "Epoch [2/10], Step [1980/2737], Loss: 0.2610\n",
      "Epoch [2/10], Step [1990/2737], Loss: 0.3222\n",
      "Epoch [2/10], Step [2000/2737], Loss: 0.2907\n",
      "Epoch [2/10], Step [2010/2737], Loss: 0.1779\n",
      "Epoch [2/10], Step [2020/2737], Loss: 0.3463\n",
      "Epoch [2/10], Step [2030/2737], Loss: 0.2590\n",
      "Epoch [2/10], Step [2040/2737], Loss: 0.3325\n",
      "Epoch [2/10], Step [2050/2737], Loss: 0.3321\n",
      "Epoch [2/10], Step [2060/2737], Loss: 0.2468\n",
      "Epoch [2/10], Step [2070/2737], Loss: 0.2338\n",
      "Epoch [2/10], Step [2080/2737], Loss: 0.2036\n",
      "Epoch [2/10], Step [2090/2737], Loss: 0.2898\n",
      "Epoch [2/10], Step [2100/2737], Loss: 0.3981\n",
      "Epoch [2/10], Step [2110/2737], Loss: 0.2381\n",
      "Epoch [2/10], Step [2120/2737], Loss: 0.3260\n",
      "Epoch [2/10], Step [2130/2737], Loss: 0.2183\n",
      "Epoch [2/10], Step [2140/2737], Loss: 0.3172\n",
      "Epoch [2/10], Step [2150/2737], Loss: 0.3130\n",
      "Epoch [2/10], Step [2160/2737], Loss: 0.2582\n",
      "Epoch [2/10], Step [2170/2737], Loss: 0.4021\n",
      "Epoch [2/10], Step [2180/2737], Loss: 0.1514\n",
      "Epoch [2/10], Step [2190/2737], Loss: 0.3189\n",
      "Epoch [2/10], Step [2200/2737], Loss: 0.2601\n",
      "Epoch [2/10], Step [2210/2737], Loss: 0.2901\n",
      "Epoch [2/10], Step [2220/2737], Loss: 0.1829\n",
      "Epoch [2/10], Step [2230/2737], Loss: 0.1691\n",
      "Epoch [2/10], Step [2240/2737], Loss: 0.2785\n",
      "Epoch [2/10], Step [2250/2737], Loss: 0.2332\n",
      "Epoch [2/10], Step [2260/2737], Loss: 0.3538\n",
      "Epoch [2/10], Step [2270/2737], Loss: 0.1607\n",
      "Epoch [2/10], Step [2280/2737], Loss: 0.3564\n",
      "Epoch [2/10], Step [2290/2737], Loss: 0.1898\n",
      "Epoch [2/10], Step [2300/2737], Loss: 0.2706\n",
      "Epoch [2/10], Step [2310/2737], Loss: 0.4054\n",
      "Epoch [2/10], Step [2320/2737], Loss: 0.2776\n",
      "Epoch [2/10], Step [2330/2737], Loss: 0.2940\n",
      "Epoch [2/10], Step [2340/2737], Loss: 0.1618\n",
      "Epoch [2/10], Step [2350/2737], Loss: 0.3026\n",
      "Epoch [2/10], Step [2360/2737], Loss: 0.3643\n",
      "Epoch [2/10], Step [2370/2737], Loss: 0.2739\n",
      "Epoch [2/10], Step [2380/2737], Loss: 0.3018\n",
      "Epoch [2/10], Step [2390/2737], Loss: 0.2869\n",
      "Epoch [2/10], Step [2400/2737], Loss: 0.2411\n",
      "Epoch [2/10], Step [2410/2737], Loss: 0.1805\n",
      "Epoch [2/10], Step [2420/2737], Loss: 0.3725\n",
      "Epoch [2/10], Step [2430/2737], Loss: 0.1753\n",
      "Epoch [2/10], Step [2440/2737], Loss: 0.2352\n",
      "Epoch [2/10], Step [2450/2737], Loss: 0.3161\n",
      "Epoch [2/10], Step [2460/2737], Loss: 0.2505\n",
      "Epoch [2/10], Step [2470/2737], Loss: 0.2641\n",
      "Epoch [2/10], Step [2480/2737], Loss: 0.2509\n",
      "Epoch [2/10], Step [2490/2737], Loss: 0.3292\n",
      "Epoch [2/10], Step [2500/2737], Loss: 0.1844\n",
      "Epoch [2/10], Step [2510/2737], Loss: 0.3627\n",
      "Epoch [2/10], Step [2520/2737], Loss: 0.1846\n",
      "Epoch [2/10], Step [2530/2737], Loss: 0.3564\n",
      "Epoch [2/10], Step [2540/2737], Loss: 0.2420\n",
      "Epoch [2/10], Step [2550/2737], Loss: 0.5439\n",
      "Epoch [2/10], Step [2560/2737], Loss: 0.1925\n",
      "Epoch [2/10], Step [2570/2737], Loss: 0.3371\n",
      "Epoch [2/10], Step [2580/2737], Loss: 0.3004\n",
      "Epoch [2/10], Step [2590/2737], Loss: 0.2354\n",
      "Epoch [2/10], Step [2600/2737], Loss: 0.2764\n",
      "Epoch [2/10], Step [2610/2737], Loss: 0.2076\n",
      "Epoch [2/10], Step [2620/2737], Loss: 0.3045\n",
      "Epoch [2/10], Step [2630/2737], Loss: 0.3923\n",
      "Epoch [2/10], Step [2640/2737], Loss: 0.1564\n",
      "Epoch [2/10], Step [2650/2737], Loss: 0.2854\n",
      "Epoch [2/10], Step [2660/2737], Loss: 0.3522\n",
      "Epoch [2/10], Step [2670/2737], Loss: 0.3407\n",
      "Epoch [2/10], Step [2680/2737], Loss: 0.2616\n",
      "Epoch [2/10], Step [2690/2737], Loss: 0.2534\n",
      "Epoch [2/10], Step [2700/2737], Loss: 0.2740\n",
      "Epoch [2/10], Step [2710/2737], Loss: 0.2691\n",
      "Epoch [2/10], Step [2720/2737], Loss: 0.2865\n",
      "Epoch [2/10], Step [2730/2737], Loss: 0.3158\n",
      "Epoch [2/10], train_loss: 0.2824, val_loss: 0.3078\n",
      "Epoch [3/10], Step [10/2737], Loss: 0.2317\n",
      "Epoch [3/10], Step [20/2737], Loss: 0.1558\n",
      "Epoch [3/10], Step [30/2737], Loss: 0.2794\n",
      "Epoch [3/10], Step [40/2737], Loss: 0.4031\n",
      "Epoch [3/10], Step [50/2737], Loss: 0.2167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [60/2737], Loss: 0.2720\n",
      "Epoch [3/10], Step [70/2737], Loss: 0.3092\n",
      "Epoch [3/10], Step [80/2737], Loss: 0.2218\n",
      "Epoch [3/10], Step [90/2737], Loss: 0.2028\n",
      "Epoch [3/10], Step [100/2737], Loss: 0.1771\n",
      "Epoch [3/10], Step [110/2737], Loss: 0.3466\n",
      "Epoch [3/10], Step [120/2737], Loss: 0.3289\n",
      "Epoch [3/10], Step [130/2737], Loss: 0.2625\n",
      "Epoch [3/10], Step [140/2737], Loss: 0.3588\n",
      "Epoch [3/10], Step [150/2737], Loss: 0.1895\n",
      "Epoch [3/10], Step [160/2737], Loss: 0.1968\n",
      "Epoch [3/10], Step [170/2737], Loss: 0.3151\n",
      "Epoch [3/10], Step [180/2737], Loss: 0.2710\n",
      "Epoch [3/10], Step [190/2737], Loss: 0.2775\n",
      "Epoch [3/10], Step [200/2737], Loss: 0.2901\n",
      "Epoch [3/10], Step [210/2737], Loss: 0.3863\n",
      "Epoch [3/10], Step [220/2737], Loss: 0.2597\n",
      "Epoch [3/10], Step [230/2737], Loss: 0.3538\n",
      "Epoch [3/10], Step [240/2737], Loss: 0.2628\n",
      "Epoch [3/10], Step [250/2737], Loss: 0.1670\n",
      "Epoch [3/10], Step [260/2737], Loss: 0.2480\n",
      "Epoch [3/10], Step [270/2737], Loss: 0.3016\n",
      "Epoch [3/10], Step [280/2737], Loss: 0.1674\n",
      "Epoch [3/10], Step [290/2737], Loss: 0.3411\n",
      "Epoch [3/10], Step [300/2737], Loss: 0.2228\n",
      "Epoch [3/10], Step [310/2737], Loss: 0.2793\n",
      "Epoch [3/10], Step [320/2737], Loss: 0.3700\n",
      "Epoch [3/10], Step [330/2737], Loss: 0.3613\n",
      "Epoch [3/10], Step [340/2737], Loss: 0.2663\n",
      "Epoch [3/10], Step [350/2737], Loss: 0.2242\n",
      "Epoch [3/10], Step [360/2737], Loss: 0.2019\n",
      "Epoch [3/10], Step [370/2737], Loss: 0.2584\n",
      "Epoch [3/10], Step [380/2737], Loss: 0.3040\n",
      "Epoch [3/10], Step [390/2737], Loss: 0.2409\n",
      "Epoch [3/10], Step [400/2737], Loss: 0.1970\n",
      "Epoch [3/10], Step [410/2737], Loss: 0.2446\n",
      "Epoch [3/10], Step [420/2737], Loss: 0.2835\n",
      "Epoch [3/10], Step [430/2737], Loss: 0.2459\n",
      "Epoch [3/10], Step [440/2737], Loss: 0.2970\n",
      "Epoch [3/10], Step [450/2737], Loss: 0.2386\n",
      "Epoch [3/10], Step [460/2737], Loss: 0.2277\n",
      "Epoch [3/10], Step [470/2737], Loss: 0.3009\n",
      "Epoch [3/10], Step [480/2737], Loss: 0.3189\n",
      "Epoch [3/10], Step [490/2737], Loss: 0.1994\n",
      "Epoch [3/10], Step [500/2737], Loss: 0.2308\n",
      "Epoch [3/10], Step [510/2737], Loss: 0.2709\n",
      "Epoch [3/10], Step [520/2737], Loss: 0.2246\n",
      "Epoch [3/10], Step [530/2737], Loss: 0.3098\n",
      "Epoch [3/10], Step [540/2737], Loss: 0.2940\n",
      "Epoch [3/10], Step [550/2737], Loss: 0.2674\n",
      "Epoch [3/10], Step [560/2737], Loss: 0.2537\n",
      "Epoch [3/10], Step [570/2737], Loss: 0.3596\n",
      "Epoch [3/10], Step [580/2737], Loss: 0.2253\n",
      "Epoch [3/10], Step [590/2737], Loss: 0.2109\n",
      "Epoch [3/10], Step [600/2737], Loss: 0.2317\n",
      "Epoch [3/10], Step [610/2737], Loss: 0.2280\n",
      "Epoch [3/10], Step [620/2737], Loss: 0.2214\n",
      "Epoch [3/10], Step [630/2737], Loss: 0.3029\n",
      "Epoch [3/10], Step [640/2737], Loss: 0.2049\n",
      "Epoch [3/10], Step [650/2737], Loss: 0.1446\n",
      "Epoch [3/10], Step [660/2737], Loss: 0.1979\n",
      "Epoch [3/10], Step [670/2737], Loss: 0.2963\n",
      "Epoch [3/10], Step [680/2737], Loss: 0.1833\n",
      "Epoch [3/10], Step [690/2737], Loss: 0.1684\n",
      "Epoch [3/10], Step [700/2737], Loss: 0.2137\n",
      "Epoch [3/10], Step [710/2737], Loss: 0.3383\n",
      "Epoch [3/10], Step [720/2737], Loss: 0.2879\n",
      "Epoch [3/10], Step [730/2737], Loss: 0.1672\n",
      "Epoch [3/10], Step [740/2737], Loss: 0.2809\n",
      "Epoch [3/10], Step [750/2737], Loss: 0.2125\n",
      "Epoch [3/10], Step [760/2737], Loss: 0.1673\n",
      "Epoch [3/10], Step [770/2737], Loss: 0.2944\n",
      "Epoch [3/10], Step [780/2737], Loss: 0.2802\n",
      "Epoch [3/10], Step [790/2737], Loss: 0.2727\n",
      "Epoch [3/10], Step [800/2737], Loss: 0.3669\n",
      "Epoch [3/10], Step [810/2737], Loss: 0.3047\n",
      "Epoch [3/10], Step [820/2737], Loss: 0.3704\n",
      "Epoch [3/10], Step [830/2737], Loss: 0.2843\n",
      "Epoch [3/10], Step [840/2737], Loss: 0.3158\n",
      "Epoch [3/10], Step [850/2737], Loss: 0.2904\n",
      "Epoch [3/10], Step [860/2737], Loss: 0.2652\n",
      "Epoch [3/10], Step [870/2737], Loss: 0.3986\n",
      "Epoch [3/10], Step [880/2737], Loss: 0.2665\n",
      "Epoch [3/10], Step [890/2737], Loss: 0.3601\n",
      "Epoch [3/10], Step [900/2737], Loss: 0.2153\n",
      "Epoch [3/10], Step [910/2737], Loss: 0.2086\n",
      "Epoch [3/10], Step [920/2737], Loss: 0.2053\n",
      "Epoch [3/10], Step [930/2737], Loss: 0.1952\n",
      "Epoch [3/10], Step [940/2737], Loss: 0.0790\n",
      "Epoch [3/10], Step [950/2737], Loss: 0.3329\n",
      "Epoch [3/10], Step [960/2737], Loss: 0.3079\n",
      "Epoch [3/10], Step [970/2737], Loss: 0.3667\n",
      "Epoch [3/10], Step [980/2737], Loss: 0.2105\n",
      "Epoch [3/10], Step [990/2737], Loss: 0.2128\n",
      "Epoch [3/10], Step [1000/2737], Loss: 0.2424\n",
      "Epoch [3/10], Step [1010/2737], Loss: 0.2388\n",
      "Epoch [3/10], Step [1020/2737], Loss: 0.3508\n",
      "Epoch [3/10], Step [1030/2737], Loss: 0.3070\n",
      "Epoch [3/10], Step [1040/2737], Loss: 0.3469\n",
      "Epoch [3/10], Step [1050/2737], Loss: 0.3466\n",
      "Epoch [3/10], Step [1060/2737], Loss: 0.1860\n",
      "Epoch [3/10], Step [1070/2737], Loss: 0.3836\n",
      "Epoch [3/10], Step [1080/2737], Loss: 0.2766\n",
      "Epoch [3/10], Step [1090/2737], Loss: 0.1683\n",
      "Epoch [3/10], Step [1100/2737], Loss: 0.3776\n",
      "Epoch [3/10], Step [1110/2737], Loss: 0.2600\n",
      "Epoch [3/10], Step [1120/2737], Loss: 0.2017\n",
      "Epoch [3/10], Step [1130/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1140/2737], Loss: 0.1950\n",
      "Epoch [3/10], Step [1150/2737], Loss: 0.1993\n",
      "Epoch [3/10], Step [1160/2737], Loss: 0.1515\n",
      "Epoch [3/10], Step [1170/2737], Loss: 0.2963\n",
      "Epoch [3/10], Step [1180/2737], Loss: 0.3108\n",
      "Epoch [3/10], Step [1190/2737], Loss: 0.3467\n",
      "Epoch [3/10], Step [1200/2737], Loss: 0.3132\n",
      "Epoch [3/10], Step [1210/2737], Loss: 0.2472\n",
      "Epoch [3/10], Step [1220/2737], Loss: 0.2408\n",
      "Epoch [3/10], Step [1230/2737], Loss: 0.1885\n",
      "Epoch [3/10], Step [1240/2737], Loss: 0.2948\n",
      "Epoch [3/10], Step [1250/2737], Loss: 0.1945\n",
      "Epoch [3/10], Step [1260/2737], Loss: 0.3484\n",
      "Epoch [3/10], Step [1270/2737], Loss: 0.2122\n",
      "Epoch [3/10], Step [1280/2737], Loss: 0.2272\n",
      "Epoch [3/10], Step [1290/2737], Loss: 0.1439\n",
      "Epoch [3/10], Step [1300/2737], Loss: 0.4767\n",
      "Epoch [3/10], Step [1310/2737], Loss: 0.4168\n",
      "Epoch [3/10], Step [1320/2737], Loss: 0.3483\n",
      "Epoch [3/10], Step [1330/2737], Loss: 0.2428\n",
      "Epoch [3/10], Step [1340/2737], Loss: 0.1845\n",
      "Epoch [3/10], Step [1350/2737], Loss: 0.2360\n",
      "Epoch [3/10], Step [1360/2737], Loss: 0.2660\n",
      "Epoch [3/10], Step [1370/2737], Loss: 0.2900\n",
      "Epoch [3/10], Step [1380/2737], Loss: 0.2304\n",
      "Epoch [3/10], Step [1390/2737], Loss: 0.3351\n",
      "Epoch [3/10], Step [1400/2737], Loss: 0.1782\n",
      "Epoch [3/10], Step [1410/2737], Loss: 0.2238\n",
      "Epoch [3/10], Step [1420/2737], Loss: 0.2844\n",
      "Epoch [3/10], Step [1430/2737], Loss: 0.2154\n",
      "Epoch [3/10], Step [1440/2737], Loss: 0.3239\n",
      "Epoch [3/10], Step [1450/2737], Loss: 0.3079\n",
      "Epoch [3/10], Step [1460/2737], Loss: 0.4107\n",
      "Epoch [3/10], Step [1470/2737], Loss: 0.2216\n",
      "Epoch [3/10], Step [1480/2737], Loss: 0.3415\n",
      "Epoch [3/10], Step [1490/2737], Loss: 0.2917\n",
      "Epoch [3/10], Step [1500/2737], Loss: 0.2102\n",
      "Epoch [3/10], Step [1510/2737], Loss: 0.3804\n",
      "Epoch [3/10], Step [1520/2737], Loss: 0.3440\n",
      "Epoch [3/10], Step [1530/2737], Loss: 0.2813\n",
      "Epoch [3/10], Step [1540/2737], Loss: 0.2343\n",
      "Epoch [3/10], Step [1550/2737], Loss: 0.1991\n",
      "Epoch [3/10], Step [1560/2737], Loss: 0.2651\n",
      "Epoch [3/10], Step [1570/2737], Loss: 0.2403\n",
      "Epoch [3/10], Step [1580/2737], Loss: 0.1873\n",
      "Epoch [3/10], Step [1590/2737], Loss: 0.1672\n",
      "Epoch [3/10], Step [1600/2737], Loss: 0.3302\n",
      "Epoch [3/10], Step [1610/2737], Loss: 0.2131\n",
      "Epoch [3/10], Step [1620/2737], Loss: 0.2527\n",
      "Epoch [3/10], Step [1630/2737], Loss: 0.1473\n",
      "Epoch [3/10], Step [1640/2737], Loss: 0.2585\n",
      "Epoch [3/10], Step [1650/2737], Loss: 0.2901\n",
      "Epoch [3/10], Step [1660/2737], Loss: 0.1559\n",
      "Epoch [3/10], Step [1670/2737], Loss: 0.2076\n",
      "Epoch [3/10], Step [1680/2737], Loss: 0.2081\n",
      "Epoch [3/10], Step [1690/2737], Loss: 0.2003\n",
      "Epoch [3/10], Step [1700/2737], Loss: 0.2249\n",
      "Epoch [3/10], Step [1710/2737], Loss: 0.3678\n",
      "Epoch [3/10], Step [1720/2737], Loss: 0.2119\n",
      "Epoch [3/10], Step [1730/2737], Loss: 0.2572\n",
      "Epoch [3/10], Step [1740/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1750/2737], Loss: 0.2143\n",
      "Epoch [3/10], Step [1760/2737], Loss: 0.3421\n",
      "Epoch [3/10], Step [1770/2737], Loss: 0.2232\n",
      "Epoch [3/10], Step [1780/2737], Loss: 0.2711\n",
      "Epoch [3/10], Step [1790/2737], Loss: 0.3006\n",
      "Epoch [3/10], Step [1800/2737], Loss: 0.2784\n",
      "Epoch [3/10], Step [1810/2737], Loss: 0.3540\n",
      "Epoch [3/10], Step [1820/2737], Loss: 0.3274\n",
      "Epoch [3/10], Step [1830/2737], Loss: 0.1926\n",
      "Epoch [3/10], Step [1840/2737], Loss: 0.2195\n",
      "Epoch [3/10], Step [1850/2737], Loss: 0.3931\n",
      "Epoch [3/10], Step [1860/2737], Loss: 0.2293\n",
      "Epoch [3/10], Step [1870/2737], Loss: 0.3100\n",
      "Epoch [3/10], Step [1880/2737], Loss: 0.1571\n",
      "Epoch [3/10], Step [1890/2737], Loss: 0.3826\n",
      "Epoch [3/10], Step [1900/2737], Loss: 0.3410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [1910/2737], Loss: 0.2496\n",
      "Epoch [3/10], Step [1920/2737], Loss: 0.3327\n",
      "Epoch [3/10], Step [1930/2737], Loss: 0.2223\n",
      "Epoch [3/10], Step [1940/2737], Loss: 0.1943\n",
      "Epoch [3/10], Step [1950/2737], Loss: 0.2297\n",
      "Epoch [3/10], Step [1960/2737], Loss: 0.2605\n",
      "Epoch [3/10], Step [1970/2737], Loss: 0.2887\n",
      "Epoch [3/10], Step [1980/2737], Loss: 0.1267\n",
      "Epoch [3/10], Step [1990/2737], Loss: 0.2843\n",
      "Epoch [3/10], Step [2000/2737], Loss: 0.2725\n",
      "Epoch [3/10], Step [2010/2737], Loss: 0.2230\n",
      "Epoch [3/10], Step [2020/2737], Loss: 0.3268\n",
      "Epoch [3/10], Step [2030/2737], Loss: 0.2588\n",
      "Epoch [3/10], Step [2040/2737], Loss: 0.2651\n",
      "Epoch [3/10], Step [2050/2737], Loss: 0.2929\n",
      "Epoch [3/10], Step [2060/2737], Loss: 0.2330\n",
      "Epoch [3/10], Step [2070/2737], Loss: 0.1868\n",
      "Epoch [3/10], Step [2080/2737], Loss: 0.1871\n",
      "Epoch [3/10], Step [2090/2737], Loss: 0.2948\n",
      "Epoch [3/10], Step [2100/2737], Loss: 0.2231\n",
      "Epoch [3/10], Step [2110/2737], Loss: 0.3068\n",
      "Epoch [3/10], Step [2120/2737], Loss: 0.2099\n",
      "Epoch [3/10], Step [2130/2737], Loss: 0.1532\n",
      "Epoch [3/10], Step [2140/2737], Loss: 0.3230\n",
      "Epoch [3/10], Step [2150/2737], Loss: 0.2237\n",
      "Epoch [3/10], Step [2160/2737], Loss: 0.2665\n",
      "Epoch [3/10], Step [2170/2737], Loss: 0.2069\n",
      "Epoch [3/10], Step [2180/2737], Loss: 0.1532\n",
      "Epoch [3/10], Step [2190/2737], Loss: 0.3616\n",
      "Epoch [3/10], Step [2200/2737], Loss: 0.2379\n",
      "Epoch [3/10], Step [2210/2737], Loss: 0.3390\n",
      "Epoch [3/10], Step [2220/2737], Loss: 0.5115\n",
      "Epoch [3/10], Step [2230/2737], Loss: 0.2745\n",
      "Epoch [3/10], Step [2240/2737], Loss: 0.2779\n",
      "Epoch [3/10], Step [2250/2737], Loss: 0.2110\n",
      "Epoch [3/10], Step [2260/2737], Loss: 0.2473\n",
      "Epoch [3/10], Step [2270/2737], Loss: 0.3654\n",
      "Epoch [3/10], Step [2280/2737], Loss: 0.2325\n",
      "Epoch [3/10], Step [2290/2737], Loss: 0.1744\n",
      "Epoch [3/10], Step [2300/2737], Loss: 0.2851\n",
      "Epoch [3/10], Step [2310/2737], Loss: 0.2434\n",
      "Epoch [3/10], Step [2320/2737], Loss: 0.1793\n",
      "Epoch [3/10], Step [2330/2737], Loss: 0.3277\n",
      "Epoch [3/10], Step [2340/2737], Loss: 0.3020\n",
      "Epoch [3/10], Step [2350/2737], Loss: 0.2339\n",
      "Epoch [3/10], Step [2360/2737], Loss: 0.2239\n",
      "Epoch [3/10], Step [2370/2737], Loss: 0.2578\n",
      "Epoch [3/10], Step [2380/2737], Loss: 0.1928\n",
      "Epoch [3/10], Step [2390/2737], Loss: 0.3699\n",
      "Epoch [3/10], Step [2400/2737], Loss: 0.3122\n",
      "Epoch [3/10], Step [2410/2737], Loss: 0.1850\n",
      "Epoch [3/10], Step [2420/2737], Loss: 0.3510\n",
      "Epoch [3/10], Step [2430/2737], Loss: 0.2469\n",
      "Epoch [3/10], Step [2440/2737], Loss: 0.3292\n",
      "Epoch [3/10], Step [2450/2737], Loss: 0.3481\n",
      "Epoch [3/10], Step [2460/2737], Loss: 0.3791\n",
      "Epoch [3/10], Step [2470/2737], Loss: 0.3122\n",
      "Epoch [3/10], Step [2480/2737], Loss: 0.2950\n",
      "Epoch [3/10], Step [2490/2737], Loss: 0.2983\n",
      "Epoch [3/10], Step [2500/2737], Loss: 0.3030\n",
      "Epoch [3/10], Step [2510/2737], Loss: 0.3164\n",
      "Epoch [3/10], Step [2520/2737], Loss: 0.2375\n",
      "Epoch [3/10], Step [2530/2737], Loss: 0.2933\n",
      "Epoch [3/10], Step [2540/2737], Loss: 0.2120\n",
      "Epoch [3/10], Step [2550/2737], Loss: 0.2050\n",
      "Epoch [3/10], Step [2560/2737], Loss: 0.2469\n",
      "Epoch [3/10], Step [2570/2737], Loss: 0.4300\n",
      "Epoch [3/10], Step [2580/2737], Loss: 0.2299\n",
      "Epoch [3/10], Step [2590/2737], Loss: 0.2560\n",
      "Epoch [3/10], Step [2600/2737], Loss: 0.2268\n",
      "Epoch [3/10], Step [2610/2737], Loss: 0.3580\n",
      "Epoch [3/10], Step [2620/2737], Loss: 0.2245\n",
      "Epoch [3/10], Step [2630/2737], Loss: 0.2408\n",
      "Epoch [3/10], Step [2640/2737], Loss: 0.2720\n",
      "Epoch [3/10], Step [2650/2737], Loss: 0.2562\n",
      "Epoch [3/10], Step [2660/2737], Loss: 0.2422\n",
      "Epoch [3/10], Step [2670/2737], Loss: 0.2215\n",
      "Epoch [3/10], Step [2680/2737], Loss: 0.1819\n",
      "Epoch [3/10], Step [2690/2737], Loss: 0.2772\n",
      "Epoch [3/10], Step [2700/2737], Loss: 0.3313\n",
      "Epoch [3/10], Step [2710/2737], Loss: 0.2622\n",
      "Epoch [3/10], Step [2720/2737], Loss: 0.3202\n",
      "Epoch [3/10], Step [2730/2737], Loss: 0.1857\n",
      "Epoch [3/10], train_loss: 0.2641, val_loss: 0.2491\n",
      "Epoch [4/10], Step [10/2737], Loss: 0.2554\n",
      "Epoch [4/10], Step [20/2737], Loss: 0.2359\n",
      "Epoch [4/10], Step [30/2737], Loss: 0.2434\n",
      "Epoch [4/10], Step [40/2737], Loss: 0.2190\n",
      "Epoch [4/10], Step [50/2737], Loss: 0.1885\n",
      "Epoch [4/10], Step [60/2737], Loss: 0.2695\n",
      "Epoch [4/10], Step [70/2737], Loss: 0.1699\n",
      "Epoch [4/10], Step [80/2737], Loss: 0.1467\n",
      "Epoch [4/10], Step [90/2737], Loss: 0.3995\n",
      "Epoch [4/10], Step [100/2737], Loss: 0.2748\n",
      "Epoch [4/10], Step [110/2737], Loss: 0.2573\n",
      "Epoch [4/10], Step [120/2737], Loss: 0.2871\n",
      "Epoch [4/10], Step [130/2737], Loss: 0.3748\n",
      "Epoch [4/10], Step [140/2737], Loss: 0.2859\n",
      "Epoch [4/10], Step [150/2737], Loss: 0.2657\n",
      "Epoch [4/10], Step [160/2737], Loss: 0.1993\n",
      "Epoch [4/10], Step [170/2737], Loss: 0.1865\n",
      "Epoch [4/10], Step [180/2737], Loss: 0.2115\n",
      "Epoch [4/10], Step [190/2737], Loss: 0.2330\n",
      "Epoch [4/10], Step [200/2737], Loss: 0.2117\n",
      "Epoch [4/10], Step [210/2737], Loss: 0.2022\n",
      "Epoch [4/10], Step [220/2737], Loss: 0.2166\n",
      "Epoch [4/10], Step [230/2737], Loss: 0.1876\n",
      "Epoch [4/10], Step [240/2737], Loss: 0.2339\n",
      "Epoch [4/10], Step [250/2737], Loss: 0.1915\n",
      "Epoch [4/10], Step [260/2737], Loss: 0.2235\n",
      "Epoch [4/10], Step [270/2737], Loss: 0.2271\n",
      "Epoch [4/10], Step [280/2737], Loss: 0.2586\n",
      "Epoch [4/10], Step [290/2737], Loss: 0.2522\n",
      "Epoch [4/10], Step [300/2737], Loss: 0.3513\n",
      "Epoch [4/10], Step [310/2737], Loss: 0.2835\n",
      "Epoch [4/10], Step [320/2737], Loss: 0.1705\n",
      "Epoch [4/10], Step [330/2737], Loss: 0.2028\n",
      "Epoch [4/10], Step [340/2737], Loss: 0.2862\n",
      "Epoch [4/10], Step [350/2737], Loss: 0.2339\n",
      "Epoch [4/10], Step [360/2737], Loss: 0.2466\n",
      "Epoch [4/10], Step [370/2737], Loss: 0.1813\n",
      "Epoch [4/10], Step [380/2737], Loss: 0.2844\n",
      "Epoch [4/10], Step [390/2737], Loss: 0.3292\n",
      "Epoch [4/10], Step [400/2737], Loss: 0.3282\n",
      "Epoch [4/10], Step [410/2737], Loss: 0.3365\n",
      "Epoch [4/10], Step [420/2737], Loss: 0.1699\n",
      "Epoch [4/10], Step [430/2737], Loss: 0.2450\n",
      "Epoch [4/10], Step [440/2737], Loss: 0.1856\n",
      "Epoch [4/10], Step [450/2737], Loss: 0.2593\n",
      "Epoch [4/10], Step [460/2737], Loss: 0.3263\n",
      "Epoch [4/10], Step [470/2737], Loss: 0.3748\n",
      "Epoch [4/10], Step [480/2737], Loss: 0.2041\n",
      "Epoch [4/10], Step [490/2737], Loss: 0.1654\n",
      "Epoch [4/10], Step [500/2737], Loss: 0.2834\n",
      "Epoch [4/10], Step [510/2737], Loss: 0.2629\n",
      "Epoch [4/10], Step [520/2737], Loss: 0.1769\n",
      "Epoch [4/10], Step [530/2737], Loss: 0.3588\n",
      "Epoch [4/10], Step [540/2737], Loss: 0.2211\n",
      "Epoch [4/10], Step [550/2737], Loss: 0.1943\n",
      "Epoch [4/10], Step [560/2737], Loss: 0.3445\n",
      "Epoch [4/10], Step [570/2737], Loss: 0.1793\n",
      "Epoch [4/10], Step [580/2737], Loss: 0.2438\n",
      "Epoch [4/10], Step [590/2737], Loss: 0.2776\n",
      "Epoch [4/10], Step [600/2737], Loss: 0.1618\n",
      "Epoch [4/10], Step [610/2737], Loss: 0.2334\n",
      "Epoch [4/10], Step [620/2737], Loss: 0.2615\n",
      "Epoch [4/10], Step [630/2737], Loss: 0.1887\n",
      "Epoch [4/10], Step [640/2737], Loss: 0.3008\n",
      "Epoch [4/10], Step [650/2737], Loss: 0.2543\n",
      "Epoch [4/10], Step [660/2737], Loss: 0.1566\n",
      "Epoch [4/10], Step [670/2737], Loss: 0.2719\n",
      "Epoch [4/10], Step [680/2737], Loss: 0.2484\n",
      "Epoch [4/10], Step [690/2737], Loss: 0.2107\n",
      "Epoch [4/10], Step [700/2737], Loss: 0.2611\n",
      "Epoch [4/10], Step [710/2737], Loss: 0.2708\n",
      "Epoch [4/10], Step [720/2737], Loss: 0.3045\n",
      "Epoch [4/10], Step [730/2737], Loss: 0.2660\n",
      "Epoch [4/10], Step [740/2737], Loss: 0.1867\n",
      "Epoch [4/10], Step [750/2737], Loss: 0.2346\n",
      "Epoch [4/10], Step [760/2737], Loss: 0.2035\n",
      "Epoch [4/10], Step [770/2737], Loss: 0.2539\n",
      "Epoch [4/10], Step [780/2737], Loss: 0.2323\n",
      "Epoch [4/10], Step [790/2737], Loss: 0.1811\n",
      "Epoch [4/10], Step [800/2737], Loss: 0.2130\n",
      "Epoch [4/10], Step [810/2737], Loss: 0.2779\n",
      "Epoch [4/10], Step [820/2737], Loss: 0.2106\n",
      "Epoch [4/10], Step [830/2737], Loss: 0.2814\n",
      "Epoch [4/10], Step [840/2737], Loss: 0.1882\n",
      "Epoch [4/10], Step [850/2737], Loss: 0.2407\n",
      "Epoch [4/10], Step [860/2737], Loss: 0.2799\n",
      "Epoch [4/10], Step [870/2737], Loss: 0.1944\n",
      "Epoch [4/10], Step [880/2737], Loss: 0.3110\n",
      "Epoch [4/10], Step [890/2737], Loss: 0.2944\n",
      "Epoch [4/10], Step [900/2737], Loss: 0.1750\n",
      "Epoch [4/10], Step [910/2737], Loss: 0.2288\n",
      "Epoch [4/10], Step [920/2737], Loss: 0.3645\n",
      "Epoch [4/10], Step [930/2737], Loss: 0.2696\n",
      "Epoch [4/10], Step [940/2737], Loss: 0.2800\n",
      "Epoch [4/10], Step [950/2737], Loss: 0.1528\n",
      "Epoch [4/10], Step [960/2737], Loss: 0.1900\n",
      "Epoch [4/10], Step [970/2737], Loss: 0.2992\n",
      "Epoch [4/10], Step [980/2737], Loss: 0.2601\n",
      "Epoch [4/10], Step [990/2737], Loss: 0.2658\n",
      "Epoch [4/10], Step [1000/2737], Loss: 0.3921\n",
      "Epoch [4/10], Step [1010/2737], Loss: 0.2342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [1020/2737], Loss: 0.2035\n",
      "Epoch [4/10], Step [1030/2737], Loss: 0.2174\n",
      "Epoch [4/10], Step [1040/2737], Loss: 0.2355\n",
      "Epoch [4/10], Step [1050/2737], Loss: 0.2862\n",
      "Epoch [4/10], Step [1060/2737], Loss: 0.2165\n",
      "Epoch [4/10], Step [1070/2737], Loss: 0.2105\n",
      "Epoch [4/10], Step [1080/2737], Loss: 0.1819\n",
      "Epoch [4/10], Step [1090/2737], Loss: 0.1999\n",
      "Epoch [4/10], Step [1100/2737], Loss: 0.2083\n",
      "Epoch [4/10], Step [1110/2737], Loss: 0.2109\n",
      "Epoch [4/10], Step [1120/2737], Loss: 0.2611\n",
      "Epoch [4/10], Step [1130/2737], Loss: 0.3400\n",
      "Epoch [4/10], Step [1140/2737], Loss: 0.1697\n",
      "Epoch [4/10], Step [1150/2737], Loss: 0.2786\n",
      "Epoch [4/10], Step [1160/2737], Loss: 0.2589\n",
      "Epoch [4/10], Step [1170/2737], Loss: 0.2785\n",
      "Epoch [4/10], Step [1180/2737], Loss: 0.2205\n",
      "Epoch [4/10], Step [1190/2737], Loss: 0.2235\n",
      "Epoch [4/10], Step [1200/2737], Loss: 0.3073\n",
      "Epoch [4/10], Step [1210/2737], Loss: 0.2322\n",
      "Epoch [4/10], Step [1220/2737], Loss: 0.2220\n",
      "Epoch [4/10], Step [1230/2737], Loss: 0.2056\n",
      "Epoch [4/10], Step [1240/2737], Loss: 0.1298\n",
      "Epoch [4/10], Step [1250/2737], Loss: 0.2443\n",
      "Epoch [4/10], Step [1260/2737], Loss: 0.1538\n",
      "Epoch [4/10], Step [1270/2737], Loss: 0.2436\n",
      "Epoch [4/10], Step [1280/2737], Loss: 0.3210\n",
      "Epoch [4/10], Step [1290/2737], Loss: 0.2475\n",
      "Epoch [4/10], Step [1300/2737], Loss: 0.2402\n",
      "Epoch [4/10], Step [1310/2737], Loss: 0.3219\n",
      "Epoch [4/10], Step [1320/2737], Loss: 0.2993\n",
      "Epoch [4/10], Step [1330/2737], Loss: 0.2356\n",
      "Epoch [4/10], Step [1340/2737], Loss: 0.1945\n",
      "Epoch [4/10], Step [1350/2737], Loss: 0.2716\n",
      "Epoch [4/10], Step [1360/2737], Loss: 0.3257\n",
      "Epoch [4/10], Step [1370/2737], Loss: 0.1692\n",
      "Epoch [4/10], Step [1380/2737], Loss: 0.2602\n",
      "Epoch [4/10], Step [1390/2737], Loss: 0.2240\n",
      "Epoch [4/10], Step [1400/2737], Loss: 0.1166\n",
      "Epoch [4/10], Step [1410/2737], Loss: 0.3073\n",
      "Epoch [4/10], Step [1420/2737], Loss: 0.2177\n",
      "Epoch [4/10], Step [1430/2737], Loss: 0.2914\n",
      "Epoch [4/10], Step [1440/2737], Loss: 0.2204\n",
      "Epoch [4/10], Step [1450/2737], Loss: 0.2941\n",
      "Epoch [4/10], Step [1460/2737], Loss: 0.1786\n",
      "Epoch [4/10], Step [1470/2737], Loss: 0.3096\n",
      "Epoch [4/10], Step [1480/2737], Loss: 0.2193\n",
      "Epoch [4/10], Step [1490/2737], Loss: 0.1734\n",
      "Epoch [4/10], Step [1500/2737], Loss: 0.3091\n",
      "Epoch [4/10], Step [1510/2737], Loss: 0.1922\n",
      "Epoch [4/10], Step [1520/2737], Loss: 0.2198\n",
      "Epoch [4/10], Step [1530/2737], Loss: 0.1598\n",
      "Epoch [4/10], Step [1540/2737], Loss: 0.3466\n",
      "Epoch [4/10], Step [1550/2737], Loss: 0.1269\n",
      "Epoch [4/10], Step [1560/2737], Loss: 0.3752\n",
      "Epoch [4/10], Step [1570/2737], Loss: 0.3126\n",
      "Epoch [4/10], Step [1580/2737], Loss: 0.3225\n",
      "Epoch [4/10], Step [1590/2737], Loss: 0.2842\n",
      "Epoch [4/10], Step [1600/2737], Loss: 0.1766\n",
      "Epoch [4/10], Step [1610/2737], Loss: 0.3023\n",
      "Epoch [4/10], Step [1620/2737], Loss: 0.2315\n",
      "Epoch [4/10], Step [1630/2737], Loss: 0.3884\n",
      "Epoch [4/10], Step [1640/2737], Loss: 0.1848\n",
      "Epoch [4/10], Step [1650/2737], Loss: 0.3060\n",
      "Epoch [4/10], Step [1660/2737], Loss: 0.2333\n",
      "Epoch [4/10], Step [1670/2737], Loss: 0.1670\n",
      "Epoch [4/10], Step [1680/2737], Loss: 0.2592\n",
      "Epoch [4/10], Step [1690/2737], Loss: 0.2547\n",
      "Epoch [4/10], Step [1700/2737], Loss: 0.1882\n",
      "Epoch [4/10], Step [1710/2737], Loss: 0.2744\n",
      "Epoch [4/10], Step [1720/2737], Loss: 0.3068\n",
      "Epoch [4/10], Step [1730/2737], Loss: 0.3498\n",
      "Epoch [4/10], Step [1740/2737], Loss: 0.3035\n",
      "Epoch [4/10], Step [1750/2737], Loss: 0.1611\n",
      "Epoch [4/10], Step [1760/2737], Loss: 0.2349\n",
      "Epoch [4/10], Step [1770/2737], Loss: 0.2413\n",
      "Epoch [4/10], Step [1780/2737], Loss: 0.1678\n",
      "Epoch [4/10], Step [1790/2737], Loss: 0.3053\n",
      "Epoch [4/10], Step [1800/2737], Loss: 0.2489\n",
      "Epoch [4/10], Step [1810/2737], Loss: 0.3003\n",
      "Epoch [4/10], Step [1820/2737], Loss: 0.2067\n",
      "Epoch [4/10], Step [1830/2737], Loss: 0.2178\n",
      "Epoch [4/10], Step [1840/2737], Loss: 0.2557\n",
      "Epoch [4/10], Step [1850/2737], Loss: 0.2291\n",
      "Epoch [4/10], Step [1860/2737], Loss: 0.1497\n",
      "Epoch [4/10], Step [1870/2737], Loss: 0.2739\n",
      "Epoch [4/10], Step [1880/2737], Loss: 0.2822\n",
      "Epoch [4/10], Step [1890/2737], Loss: 0.2359\n",
      "Epoch [4/10], Step [1900/2737], Loss: 0.2004\n",
      "Epoch [4/10], Step [1910/2737], Loss: 0.1270\n",
      "Epoch [4/10], Step [1920/2737], Loss: 0.2289\n",
      "Epoch [4/10], Step [1930/2737], Loss: 0.2782\n",
      "Epoch [4/10], Step [1940/2737], Loss: 0.2841\n",
      "Epoch [4/10], Step [1950/2737], Loss: 0.2231\n",
      "Epoch [4/10], Step [1960/2737], Loss: 0.2614\n",
      "Epoch [4/10], Step [1970/2737], Loss: 0.2762\n",
      "Epoch [4/10], Step [1980/2737], Loss: 0.2729\n",
      "Epoch [4/10], Step [1990/2737], Loss: 0.3491\n",
      "Epoch [4/10], Step [2000/2737], Loss: 0.2688\n",
      "Epoch [4/10], Step [2010/2737], Loss: 0.1739\n",
      "Epoch [4/10], Step [2020/2737], Loss: 0.1481\n",
      "Epoch [4/10], Step [2030/2737], Loss: 0.2758\n",
      "Epoch [4/10], Step [2040/2737], Loss: 0.3145\n",
      "Epoch [4/10], Step [2050/2737], Loss: 0.2169\n",
      "Epoch [4/10], Step [2060/2737], Loss: 0.3111\n",
      "Epoch [4/10], Step [2070/2737], Loss: 0.3245\n",
      "Epoch [4/10], Step [2080/2737], Loss: 0.2453\n",
      "Epoch [4/10], Step [2090/2737], Loss: 0.2302\n",
      "Epoch [4/10], Step [2100/2737], Loss: 0.5206\n",
      "Epoch [4/10], Step [2110/2737], Loss: 0.3612\n",
      "Epoch [4/10], Step [2120/2737], Loss: 0.3326\n",
      "Epoch [4/10], Step [2130/2737], Loss: 0.2513\n",
      "Epoch [4/10], Step [2140/2737], Loss: 0.2297\n",
      "Epoch [4/10], Step [2150/2737], Loss: 0.2209\n",
      "Epoch [4/10], Step [2160/2737], Loss: 0.1177\n",
      "Epoch [4/10], Step [2170/2737], Loss: 0.2822\n",
      "Epoch [4/10], Step [2180/2737], Loss: 0.2089\n",
      "Epoch [4/10], Step [2190/2737], Loss: 0.2206\n",
      "Epoch [4/10], Step [2200/2737], Loss: 0.4167\n",
      "Epoch [4/10], Step [2210/2737], Loss: 0.2203\n",
      "Epoch [4/10], Step [2220/2737], Loss: 0.2796\n",
      "Epoch [4/10], Step [2230/2737], Loss: 0.2547\n",
      "Epoch [4/10], Step [2240/2737], Loss: 0.4792\n",
      "Epoch [4/10], Step [2250/2737], Loss: 0.1460\n",
      "Epoch [4/10], Step [2260/2737], Loss: 0.2228\n",
      "Epoch [4/10], Step [2270/2737], Loss: 0.2047\n",
      "Epoch [4/10], Step [2280/2737], Loss: 0.4087\n",
      "Epoch [4/10], Step [2290/2737], Loss: 0.3161\n",
      "Epoch [4/10], Step [2300/2737], Loss: 0.1940\n",
      "Epoch [4/10], Step [2310/2737], Loss: 0.2777\n",
      "Epoch [4/10], Step [2320/2737], Loss: 0.2468\n",
      "Epoch [4/10], Step [2330/2737], Loss: 0.2603\n",
      "Epoch [4/10], Step [2340/2737], Loss: 0.1447\n",
      "Epoch [4/10], Step [2350/2737], Loss: 0.1757\n",
      "Epoch [4/10], Step [2360/2737], Loss: 0.2234\n",
      "Epoch [4/10], Step [2370/2737], Loss: 0.3188\n",
      "Epoch [4/10], Step [2380/2737], Loss: 0.2005\n",
      "Epoch [4/10], Step [2390/2737], Loss: 0.2826\n",
      "Epoch [4/10], Step [2400/2737], Loss: 0.1750\n",
      "Epoch [4/10], Step [2410/2737], Loss: 0.3469\n",
      "Epoch [4/10], Step [2420/2737], Loss: 0.1581\n",
      "Epoch [4/10], Step [2430/2737], Loss: 0.1807\n",
      "Epoch [4/10], Step [2440/2737], Loss: 0.1911\n",
      "Epoch [4/10], Step [2450/2737], Loss: 0.2965\n",
      "Epoch [4/10], Step [2460/2737], Loss: 0.2809\n",
      "Epoch [4/10], Step [2470/2737], Loss: 0.2230\n",
      "Epoch [4/10], Step [2480/2737], Loss: 0.2592\n",
      "Epoch [4/10], Step [2490/2737], Loss: 0.2327\n",
      "Epoch [4/10], Step [2500/2737], Loss: 0.1557\n",
      "Epoch [4/10], Step [2510/2737], Loss: 0.2340\n",
      "Epoch [4/10], Step [2520/2737], Loss: 0.2390\n",
      "Epoch [4/10], Step [2530/2737], Loss: 0.3193\n",
      "Epoch [4/10], Step [2540/2737], Loss: 0.2763\n",
      "Epoch [4/10], Step [2550/2737], Loss: 0.1961\n",
      "Epoch [4/10], Step [2560/2737], Loss: 0.1762\n",
      "Epoch [4/10], Step [2570/2737], Loss: 0.1822\n",
      "Epoch [4/10], Step [2580/2737], Loss: 0.3754\n",
      "Epoch [4/10], Step [2590/2737], Loss: 0.3234\n",
      "Epoch [4/10], Step [2600/2737], Loss: 0.3397\n",
      "Epoch [4/10], Step [2610/2737], Loss: 0.2288\n",
      "Epoch [4/10], Step [2620/2737], Loss: 0.1669\n",
      "Epoch [4/10], Step [2630/2737], Loss: 0.1332\n",
      "Epoch [4/10], Step [2640/2737], Loss: 0.2457\n",
      "Epoch [4/10], Step [2650/2737], Loss: 0.2364\n",
      "Epoch [4/10], Step [2660/2737], Loss: 0.1919\n",
      "Epoch [4/10], Step [2670/2737], Loss: 0.4178\n",
      "Epoch [4/10], Step [2680/2737], Loss: 0.1555\n",
      "Epoch [4/10], Step [2690/2737], Loss: 0.0864\n",
      "Epoch [4/10], Step [2700/2737], Loss: 0.3297\n",
      "Epoch [4/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [4/10], Step [2720/2737], Loss: 0.2807\n",
      "Epoch [4/10], Step [2730/2737], Loss: 0.2513\n",
      "Epoch [4/10], train_loss: 0.2542, val_loss: 0.2580\n",
      "Epoch [5/10], Step [10/2737], Loss: 0.2138\n",
      "Epoch [5/10], Step [20/2737], Loss: 0.3143\n",
      "Epoch [5/10], Step [30/2737], Loss: 0.3769\n",
      "Epoch [5/10], Step [40/2737], Loss: 0.1672\n",
      "Epoch [5/10], Step [50/2737], Loss: 0.3064\n",
      "Epoch [5/10], Step [60/2737], Loss: 0.2459\n",
      "Epoch [5/10], Step [70/2737], Loss: 0.1879\n",
      "Epoch [5/10], Step [80/2737], Loss: 0.2945\n",
      "Epoch [5/10], Step [90/2737], Loss: 0.1855\n",
      "Epoch [5/10], Step [100/2737], Loss: 0.2786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [110/2737], Loss: 0.2059\n",
      "Epoch [5/10], Step [120/2737], Loss: 0.2566\n",
      "Epoch [5/10], Step [130/2737], Loss: 0.3254\n",
      "Epoch [5/10], Step [140/2737], Loss: 0.2996\n",
      "Epoch [5/10], Step [150/2737], Loss: 0.1725\n",
      "Epoch [5/10], Step [160/2737], Loss: 0.3611\n",
      "Epoch [5/10], Step [170/2737], Loss: 0.2006\n",
      "Epoch [5/10], Step [180/2737], Loss: 0.1974\n",
      "Epoch [5/10], Step [190/2737], Loss: 0.2512\n",
      "Epoch [5/10], Step [200/2737], Loss: 0.2975\n",
      "Epoch [5/10], Step [210/2737], Loss: 0.3245\n",
      "Epoch [5/10], Step [220/2737], Loss: 0.1999\n",
      "Epoch [5/10], Step [230/2737], Loss: 0.1557\n",
      "Epoch [5/10], Step [240/2737], Loss: 0.2195\n",
      "Epoch [5/10], Step [250/2737], Loss: 0.3012\n",
      "Epoch [5/10], Step [260/2737], Loss: 0.2492\n",
      "Epoch [5/10], Step [270/2737], Loss: 0.2987\n",
      "Epoch [5/10], Step [280/2737], Loss: 0.2235\n",
      "Epoch [5/10], Step [290/2737], Loss: 0.1898\n",
      "Epoch [5/10], Step [300/2737], Loss: 0.3154\n",
      "Epoch [5/10], Step [310/2737], Loss: 0.1528\n",
      "Epoch [5/10], Step [320/2737], Loss: 0.3160\n",
      "Epoch [5/10], Step [330/2737], Loss: 0.1782\n",
      "Epoch [5/10], Step [340/2737], Loss: 0.3089\n",
      "Epoch [5/10], Step [350/2737], Loss: 0.1910\n",
      "Epoch [5/10], Step [360/2737], Loss: 0.2240\n",
      "Epoch [5/10], Step [370/2737], Loss: 0.3146\n",
      "Epoch [5/10], Step [380/2737], Loss: 0.2761\n",
      "Epoch [5/10], Step [390/2737], Loss: 0.1950\n",
      "Epoch [5/10], Step [400/2737], Loss: 0.3579\n",
      "Epoch [5/10], Step [410/2737], Loss: 0.2278\n",
      "Epoch [5/10], Step [420/2737], Loss: 0.2221\n",
      "Epoch [5/10], Step [430/2737], Loss: 0.1270\n",
      "Epoch [5/10], Step [440/2737], Loss: 0.1526\n",
      "Epoch [5/10], Step [450/2737], Loss: 0.2150\n",
      "Epoch [5/10], Step [460/2737], Loss: 0.2334\n",
      "Epoch [5/10], Step [470/2737], Loss: 0.3908\n",
      "Epoch [5/10], Step [480/2737], Loss: 0.2114\n",
      "Epoch [5/10], Step [490/2737], Loss: 0.1848\n",
      "Epoch [5/10], Step [500/2737], Loss: 0.2933\n",
      "Epoch [5/10], Step [510/2737], Loss: 0.2520\n",
      "Epoch [5/10], Step [520/2737], Loss: 0.1979\n",
      "Epoch [5/10], Step [530/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [540/2737], Loss: 0.2786\n",
      "Epoch [5/10], Step [550/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [560/2737], Loss: 0.1831\n",
      "Epoch [5/10], Step [570/2737], Loss: 0.1742\n",
      "Epoch [5/10], Step [580/2737], Loss: 0.1743\n",
      "Epoch [5/10], Step [590/2737], Loss: 0.2465\n",
      "Epoch [5/10], Step [600/2737], Loss: 0.3461\n",
      "Epoch [5/10], Step [610/2737], Loss: 0.2334\n",
      "Epoch [5/10], Step [620/2737], Loss: 0.2697\n",
      "Epoch [5/10], Step [630/2737], Loss: 0.2714\n",
      "Epoch [5/10], Step [640/2737], Loss: 0.3149\n",
      "Epoch [5/10], Step [650/2737], Loss: 0.2563\n",
      "Epoch [5/10], Step [660/2737], Loss: 0.1851\n",
      "Epoch [5/10], Step [670/2737], Loss: 0.2095\n",
      "Epoch [5/10], Step [680/2737], Loss: 0.2314\n",
      "Epoch [5/10], Step [690/2737], Loss: 0.2697\n",
      "Epoch [5/10], Step [700/2737], Loss: 0.2157\n",
      "Epoch [5/10], Step [710/2737], Loss: 0.1976\n",
      "Epoch [5/10], Step [720/2737], Loss: 0.1553\n",
      "Epoch [5/10], Step [730/2737], Loss: 0.1741\n",
      "Epoch [5/10], Step [740/2737], Loss: 0.2149\n",
      "Epoch [5/10], Step [750/2737], Loss: 0.2458\n",
      "Epoch [5/10], Step [760/2737], Loss: 0.2404\n",
      "Epoch [5/10], Step [770/2737], Loss: 0.1647\n",
      "Epoch [5/10], Step [780/2737], Loss: 0.2524\n",
      "Epoch [5/10], Step [790/2737], Loss: 0.2297\n",
      "Epoch [5/10], Step [800/2737], Loss: 0.2642\n",
      "Epoch [5/10], Step [810/2737], Loss: 0.3220\n",
      "Epoch [5/10], Step [820/2737], Loss: 0.1634\n",
      "Epoch [5/10], Step [830/2737], Loss: 0.2018\n",
      "Epoch [5/10], Step [840/2737], Loss: 0.3159\n",
      "Epoch [5/10], Step [850/2737], Loss: 0.2041\n",
      "Epoch [5/10], Step [860/2737], Loss: 0.2400\n",
      "Epoch [5/10], Step [870/2737], Loss: 0.3107\n",
      "Epoch [5/10], Step [880/2737], Loss: 0.3802\n",
      "Epoch [5/10], Step [890/2737], Loss: 0.2965\n",
      "Epoch [5/10], Step [900/2737], Loss: 0.2049\n",
      "Epoch [5/10], Step [910/2737], Loss: 0.2369\n",
      "Epoch [5/10], Step [920/2737], Loss: 0.3160\n",
      "Epoch [5/10], Step [930/2737], Loss: 0.1780\n",
      "Epoch [5/10], Step [940/2737], Loss: 0.1988\n",
      "Epoch [5/10], Step [950/2737], Loss: 0.2191\n",
      "Epoch [5/10], Step [960/2737], Loss: 0.3614\n",
      "Epoch [5/10], Step [970/2737], Loss: 0.1545\n",
      "Epoch [5/10], Step [980/2737], Loss: 0.2005\n",
      "Epoch [5/10], Step [990/2737], Loss: 0.3368\n",
      "Epoch [5/10], Step [1000/2737], Loss: 0.1860\n",
      "Epoch [5/10], Step [1010/2737], Loss: 0.2706\n",
      "Epoch [5/10], Step [1020/2737], Loss: 0.2504\n",
      "Epoch [5/10], Step [1030/2737], Loss: 0.2169\n",
      "Epoch [5/10], Step [1040/2737], Loss: 0.2674\n",
      "Epoch [5/10], Step [1050/2737], Loss: 0.2996\n",
      "Epoch [5/10], Step [1060/2737], Loss: 0.2125\n",
      "Epoch [5/10], Step [1070/2737], Loss: 0.2466\n",
      "Epoch [5/10], Step [1080/2737], Loss: 0.1739\n",
      "Epoch [5/10], Step [1090/2737], Loss: 0.2635\n",
      "Epoch [5/10], Step [1100/2737], Loss: 0.2433\n",
      "Epoch [5/10], Step [1110/2737], Loss: 0.2189\n",
      "Epoch [5/10], Step [1120/2737], Loss: 0.2007\n",
      "Epoch [5/10], Step [1130/2737], Loss: 0.1910\n",
      "Epoch [5/10], Step [1140/2737], Loss: 0.2203\n",
      "Epoch [5/10], Step [1150/2737], Loss: 0.1879\n",
      "Epoch [5/10], Step [1160/2737], Loss: 0.3532\n",
      "Epoch [5/10], Step [1170/2737], Loss: 0.4066\n",
      "Epoch [5/10], Step [1180/2737], Loss: 0.2145\n",
      "Epoch [5/10], Step [1190/2737], Loss: 0.2938\n",
      "Epoch [5/10], Step [1200/2737], Loss: 0.3332\n",
      "Epoch [5/10], Step [1210/2737], Loss: 0.3104\n",
      "Epoch [5/10], Step [1220/2737], Loss: 0.2097\n",
      "Epoch [5/10], Step [1230/2737], Loss: 0.2497\n",
      "Epoch [5/10], Step [1240/2737], Loss: 0.1822\n",
      "Epoch [5/10], Step [1250/2737], Loss: 0.2364\n",
      "Epoch [5/10], Step [1260/2737], Loss: 0.2834\n",
      "Epoch [5/10], Step [1270/2737], Loss: 0.2551\n",
      "Epoch [5/10], Step [1280/2737], Loss: 0.2480\n",
      "Epoch [5/10], Step [1290/2737], Loss: 0.2307\n",
      "Epoch [5/10], Step [1300/2737], Loss: 0.2255\n",
      "Epoch [5/10], Step [1310/2737], Loss: 0.1816\n",
      "Epoch [5/10], Step [1320/2737], Loss: 0.3270\n",
      "Epoch [5/10], Step [1330/2737], Loss: 0.2127\n",
      "Epoch [5/10], Step [1340/2737], Loss: 0.2097\n",
      "Epoch [5/10], Step [1350/2737], Loss: 0.2475\n",
      "Epoch [5/10], Step [1360/2737], Loss: 0.2003\n",
      "Epoch [5/10], Step [1370/2737], Loss: 0.1421\n",
      "Epoch [5/10], Step [1380/2737], Loss: 0.3890\n",
      "Epoch [5/10], Step [1390/2737], Loss: 0.3063\n",
      "Epoch [5/10], Step [1400/2737], Loss: 0.1367\n",
      "Epoch [5/10], Step [1410/2737], Loss: 0.2023\n",
      "Epoch [5/10], Step [1420/2737], Loss: 0.2761\n",
      "Epoch [5/10], Step [1430/2737], Loss: 0.2451\n",
      "Epoch [5/10], Step [1440/2737], Loss: 0.2532\n",
      "Epoch [5/10], Step [1450/2737], Loss: 0.2576\n",
      "Epoch [5/10], Step [1460/2737], Loss: 0.2775\n",
      "Epoch [5/10], Step [1470/2737], Loss: 0.3423\n",
      "Epoch [5/10], Step [1480/2737], Loss: 0.1840\n",
      "Epoch [5/10], Step [1490/2737], Loss: 0.2529\n",
      "Epoch [5/10], Step [1500/2737], Loss: 0.2180\n",
      "Epoch [5/10], Step [1510/2737], Loss: 0.2648\n",
      "Epoch [5/10], Step [1520/2737], Loss: 0.2676\n",
      "Epoch [5/10], Step [1530/2737], Loss: 0.3482\n",
      "Epoch [5/10], Step [1540/2737], Loss: 0.3686\n",
      "Epoch [5/10], Step [1550/2737], Loss: 0.2608\n",
      "Epoch [5/10], Step [1560/2737], Loss: 0.2308\n",
      "Epoch [5/10], Step [1570/2737], Loss: 0.2459\n",
      "Epoch [5/10], Step [1580/2737], Loss: 0.2255\n",
      "Epoch [5/10], Step [1590/2737], Loss: 0.3731\n",
      "Epoch [5/10], Step [1600/2737], Loss: 0.1635\n",
      "Epoch [5/10], Step [1610/2737], Loss: 0.1566\n",
      "Epoch [5/10], Step [1620/2737], Loss: 0.3457\n",
      "Epoch [5/10], Step [1630/2737], Loss: 0.2003\n",
      "Epoch [5/10], Step [1640/2737], Loss: 0.2055\n",
      "Epoch [5/10], Step [1650/2737], Loss: 0.2493\n",
      "Epoch [5/10], Step [1660/2737], Loss: 0.3560\n",
      "Epoch [5/10], Step [1670/2737], Loss: 0.2814\n",
      "Epoch [5/10], Step [1680/2737], Loss: 0.2269\n",
      "Epoch [5/10], Step [1690/2737], Loss: 0.3397\n",
      "Epoch [5/10], Step [1700/2737], Loss: 0.1368\n",
      "Epoch [5/10], Step [1710/2737], Loss: 0.2014\n",
      "Epoch [5/10], Step [1720/2737], Loss: 0.1770\n",
      "Epoch [5/10], Step [1730/2737], Loss: 0.2575\n",
      "Epoch [5/10], Step [1740/2737], Loss: 0.2006\n",
      "Epoch [5/10], Step [1750/2737], Loss: 0.3720\n",
      "Epoch [5/10], Step [1760/2737], Loss: 0.3870\n",
      "Epoch [5/10], Step [1770/2737], Loss: 0.3476\n",
      "Epoch [5/10], Step [1780/2737], Loss: 0.1513\n",
      "Epoch [5/10], Step [1790/2737], Loss: 0.3279\n",
      "Epoch [5/10], Step [1800/2737], Loss: 0.1369\n",
      "Epoch [5/10], Step [1810/2737], Loss: 0.3396\n",
      "Epoch [5/10], Step [1820/2737], Loss: 0.2507\n",
      "Epoch [5/10], Step [1830/2737], Loss: 0.2702\n",
      "Epoch [5/10], Step [1840/2737], Loss: 0.2985\n",
      "Epoch [5/10], Step [1850/2737], Loss: 0.2279\n",
      "Epoch [5/10], Step [1860/2737], Loss: 0.1906\n",
      "Epoch [5/10], Step [1870/2737], Loss: 0.3266\n",
      "Epoch [5/10], Step [1880/2737], Loss: 0.2345\n",
      "Epoch [5/10], Step [1890/2737], Loss: 0.3471\n",
      "Epoch [5/10], Step [1900/2737], Loss: 0.1794\n",
      "Epoch [5/10], Step [1910/2737], Loss: 0.2188\n",
      "Epoch [5/10], Step [1920/2737], Loss: 0.2596\n",
      "Epoch [5/10], Step [1930/2737], Loss: 0.2727\n",
      "Epoch [5/10], Step [1940/2737], Loss: 0.3056\n",
      "Epoch [5/10], Step [1950/2737], Loss: 0.2032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [1960/2737], Loss: 0.2734\n",
      "Epoch [5/10], Step [1970/2737], Loss: 0.1680\n",
      "Epoch [5/10], Step [1980/2737], Loss: 0.2803\n",
      "Epoch [5/10], Step [1990/2737], Loss: 0.1871\n",
      "Epoch [5/10], Step [2000/2737], Loss: 0.2070\n",
      "Epoch [5/10], Step [2010/2737], Loss: 0.3230\n",
      "Epoch [5/10], Step [2020/2737], Loss: 0.1803\n",
      "Epoch [5/10], Step [2030/2737], Loss: 0.1942\n",
      "Epoch [5/10], Step [2040/2737], Loss: 0.4921\n",
      "Epoch [5/10], Step [2050/2737], Loss: 0.2501\n",
      "Epoch [5/10], Step [2060/2737], Loss: 0.2593\n",
      "Epoch [5/10], Step [2070/2737], Loss: 0.2574\n",
      "Epoch [5/10], Step [2080/2737], Loss: 0.3038\n",
      "Epoch [5/10], Step [2090/2737], Loss: 0.2516\n",
      "Epoch [5/10], Step [2100/2737], Loss: 0.1678\n",
      "Epoch [5/10], Step [2110/2737], Loss: 0.2534\n",
      "Epoch [5/10], Step [2120/2737], Loss: 0.1811\n",
      "Epoch [5/10], Step [2130/2737], Loss: 0.1898\n",
      "Epoch [5/10], Step [2140/2737], Loss: 0.2184\n",
      "Epoch [5/10], Step [2150/2737], Loss: 0.2274\n",
      "Epoch [5/10], Step [2160/2737], Loss: 0.3393\n",
      "Epoch [5/10], Step [2170/2737], Loss: 0.2346\n",
      "Epoch [5/10], Step [2180/2737], Loss: 0.1343\n",
      "Epoch [5/10], Step [2190/2737], Loss: 0.4325\n",
      "Epoch [5/10], Step [2200/2737], Loss: 0.2500\n",
      "Epoch [5/10], Step [2210/2737], Loss: 0.2939\n",
      "Epoch [5/10], Step [2220/2737], Loss: 0.2062\n",
      "Epoch [5/10], Step [2230/2737], Loss: 0.4011\n",
      "Epoch [5/10], Step [2240/2737], Loss: 0.2823\n",
      "Epoch [5/10], Step [2250/2737], Loss: 0.2248\n",
      "Epoch [5/10], Step [2260/2737], Loss: 0.2756\n",
      "Epoch [5/10], Step [2270/2737], Loss: 0.2905\n",
      "Epoch [5/10], Step [2280/2737], Loss: 0.1613\n",
      "Epoch [5/10], Step [2290/2737], Loss: 0.2423\n",
      "Epoch [5/10], Step [2300/2737], Loss: 0.1463\n",
      "Epoch [5/10], Step [2310/2737], Loss: 0.3284\n",
      "Epoch [5/10], Step [2320/2737], Loss: 0.2030\n",
      "Epoch [5/10], Step [2330/2737], Loss: 0.2759\n",
      "Epoch [5/10], Step [2340/2737], Loss: 0.2804\n",
      "Epoch [5/10], Step [2350/2737], Loss: 0.1909\n",
      "Epoch [5/10], Step [2360/2737], Loss: 0.1859\n",
      "Epoch [5/10], Step [2370/2737], Loss: 0.1553\n",
      "Epoch [5/10], Step [2380/2737], Loss: 0.3481\n",
      "Epoch [5/10], Step [2390/2737], Loss: 0.2297\n",
      "Epoch [5/10], Step [2400/2737], Loss: 0.2233\n",
      "Epoch [5/10], Step [2410/2737], Loss: 0.2296\n",
      "Epoch [5/10], Step [2420/2737], Loss: 0.2759\n",
      "Epoch [5/10], Step [2430/2737], Loss: 0.3474\n",
      "Epoch [5/10], Step [2440/2737], Loss: 0.3373\n",
      "Epoch [5/10], Step [2450/2737], Loss: 0.2806\n",
      "Epoch [5/10], Step [2460/2737], Loss: 0.2096\n",
      "Epoch [5/10], Step [2470/2737], Loss: 0.2719\n",
      "Epoch [5/10], Step [2480/2737], Loss: 0.3741\n",
      "Epoch [5/10], Step [2490/2737], Loss: 0.1980\n",
      "Epoch [5/10], Step [2500/2737], Loss: 0.1809\n",
      "Epoch [5/10], Step [2510/2737], Loss: 0.1512\n",
      "Epoch [5/10], Step [2520/2737], Loss: 0.2237\n",
      "Epoch [5/10], Step [2530/2737], Loss: 0.1557\n",
      "Epoch [5/10], Step [2540/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2550/2737], Loss: 0.3314\n",
      "Epoch [5/10], Step [2560/2737], Loss: 0.2072\n",
      "Epoch [5/10], Step [2570/2737], Loss: 0.2738\n",
      "Epoch [5/10], Step [2580/2737], Loss: 0.1457\n",
      "Epoch [5/10], Step [2590/2737], Loss: 0.2146\n",
      "Epoch [5/10], Step [2600/2737], Loss: 0.1958\n",
      "Epoch [5/10], Step [2610/2737], Loss: 0.3227\n",
      "Epoch [5/10], Step [2620/2737], Loss: 0.1516\n",
      "Epoch [5/10], Step [2630/2737], Loss: 0.1751\n",
      "Epoch [5/10], Step [2640/2737], Loss: 0.2057\n",
      "Epoch [5/10], Step [2650/2737], Loss: 0.2331\n",
      "Epoch [5/10], Step [2660/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2670/2737], Loss: 0.1795\n",
      "Epoch [5/10], Step [2680/2737], Loss: 0.2770\n",
      "Epoch [5/10], Step [2690/2737], Loss: 0.1525\n",
      "Epoch [5/10], Step [2700/2737], Loss: 0.2407\n",
      "Epoch [5/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [5/10], Step [2720/2737], Loss: 0.3222\n",
      "Epoch [5/10], Step [2730/2737], Loss: 0.3452\n",
      "Epoch [5/10], train_loss: 0.2452, val_loss: 0.2533\n",
      "Epoch [6/10], Step [10/2737], Loss: 0.2399\n",
      "Epoch [6/10], Step [20/2737], Loss: 0.3109\n",
      "Epoch [6/10], Step [30/2737], Loss: 0.2173\n",
      "Epoch [6/10], Step [40/2737], Loss: 0.1588\n",
      "Epoch [6/10], Step [50/2737], Loss: 0.3325\n",
      "Epoch [6/10], Step [60/2737], Loss: 0.2895\n",
      "Epoch [6/10], Step [70/2737], Loss: 0.2511\n",
      "Epoch [6/10], Step [80/2737], Loss: 0.2264\n",
      "Epoch [6/10], Step [90/2737], Loss: 0.2219\n",
      "Epoch [6/10], Step [100/2737], Loss: 0.2470\n",
      "Epoch [6/10], Step [110/2737], Loss: 0.2169\n",
      "Epoch [6/10], Step [120/2737], Loss: 0.3072\n",
      "Epoch [6/10], Step [130/2737], Loss: 0.2151\n",
      "Epoch [6/10], Step [140/2737], Loss: 0.2086\n",
      "Epoch [6/10], Step [150/2737], Loss: 0.3174\n",
      "Epoch [6/10], Step [160/2737], Loss: 0.3035\n",
      "Epoch [6/10], Step [170/2737], Loss: 0.1876\n",
      "Epoch [6/10], Step [180/2737], Loss: 0.1755\n",
      "Epoch [6/10], Step [190/2737], Loss: 0.1656\n",
      "Epoch [6/10], Step [200/2737], Loss: 0.2095\n",
      "Epoch [6/10], Step [210/2737], Loss: 0.3081\n",
      "Epoch [6/10], Step [220/2737], Loss: 0.3216\n",
      "Epoch [6/10], Step [230/2737], Loss: 0.1953\n",
      "Epoch [6/10], Step [240/2737], Loss: 0.1706\n",
      "Epoch [6/10], Step [250/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [260/2737], Loss: 0.2748\n",
      "Epoch [6/10], Step [270/2737], Loss: 0.2488\n",
      "Epoch [6/10], Step [280/2737], Loss: 0.1966\n",
      "Epoch [6/10], Step [290/2737], Loss: 0.4001\n",
      "Epoch [6/10], Step [300/2737], Loss: 0.2444\n",
      "Epoch [6/10], Step [310/2737], Loss: 0.2748\n",
      "Epoch [6/10], Step [320/2737], Loss: 0.3118\n",
      "Epoch [6/10], Step [330/2737], Loss: 0.1643\n",
      "Epoch [6/10], Step [340/2737], Loss: 0.2160\n",
      "Epoch [6/10], Step [350/2737], Loss: 0.1563\n",
      "Epoch [6/10], Step [360/2737], Loss: 0.1825\n",
      "Epoch [6/10], Step [370/2737], Loss: 0.4525\n",
      "Epoch [6/10], Step [380/2737], Loss: 0.2207\n",
      "Epoch [6/10], Step [390/2737], Loss: 0.2590\n",
      "Epoch [6/10], Step [400/2737], Loss: 0.2436\n",
      "Epoch [6/10], Step [410/2737], Loss: 0.3093\n",
      "Epoch [6/10], Step [420/2737], Loss: 0.2623\n",
      "Epoch [6/10], Step [430/2737], Loss: 0.2598\n",
      "Epoch [6/10], Step [440/2737], Loss: 0.2909\n",
      "Epoch [6/10], Step [450/2737], Loss: 0.3834\n",
      "Epoch [6/10], Step [460/2737], Loss: 0.4520\n",
      "Epoch [6/10], Step [470/2737], Loss: 0.1534\n",
      "Epoch [6/10], Step [480/2737], Loss: 0.1611\n",
      "Epoch [6/10], Step [490/2737], Loss: 0.1487\n",
      "Epoch [6/10], Step [500/2737], Loss: 0.2089\n",
      "Epoch [6/10], Step [510/2737], Loss: 0.1885\n",
      "Epoch [6/10], Step [520/2737], Loss: 0.2510\n",
      "Epoch [6/10], Step [530/2737], Loss: 0.2429\n",
      "Epoch [6/10], Step [540/2737], Loss: 0.2840\n",
      "Epoch [6/10], Step [550/2737], Loss: 0.1823\n",
      "Epoch [6/10], Step [560/2737], Loss: 0.3027\n",
      "Epoch [6/10], Step [570/2737], Loss: 0.1330\n",
      "Epoch [6/10], Step [580/2737], Loss: 0.1631\n",
      "Epoch [6/10], Step [590/2737], Loss: 0.2279\n",
      "Epoch [6/10], Step [600/2737], Loss: 0.3450\n",
      "Epoch [6/10], Step [610/2737], Loss: 0.1958\n",
      "Epoch [6/10], Step [620/2737], Loss: 0.3047\n",
      "Epoch [6/10], Step [630/2737], Loss: 0.1443\n",
      "Epoch [6/10], Step [640/2737], Loss: 0.1624\n",
      "Epoch [6/10], Step [650/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [660/2737], Loss: 0.1812\n",
      "Epoch [6/10], Step [670/2737], Loss: 0.2932\n",
      "Epoch [6/10], Step [680/2737], Loss: 0.1794\n",
      "Epoch [6/10], Step [690/2737], Loss: 0.1544\n",
      "Epoch [6/10], Step [700/2737], Loss: 0.2793\n",
      "Epoch [6/10], Step [710/2737], Loss: 0.2587\n",
      "Epoch [6/10], Step [720/2737], Loss: 0.2631\n",
      "Epoch [6/10], Step [730/2737], Loss: 0.1850\n",
      "Epoch [6/10], Step [740/2737], Loss: 0.2496\n",
      "Epoch [6/10], Step [750/2737], Loss: 0.1894\n",
      "Epoch [6/10], Step [760/2737], Loss: 0.2997\n",
      "Epoch [6/10], Step [770/2737], Loss: 0.2157\n",
      "Epoch [6/10], Step [780/2737], Loss: 0.1629\n",
      "Epoch [6/10], Step [790/2737], Loss: 0.2091\n",
      "Epoch [6/10], Step [800/2737], Loss: 0.3174\n",
      "Epoch [6/10], Step [810/2737], Loss: 0.3003\n",
      "Epoch [6/10], Step [820/2737], Loss: 0.1328\n",
      "Epoch [6/10], Step [830/2737], Loss: 0.2981\n",
      "Epoch [6/10], Step [840/2737], Loss: 0.2193\n",
      "Epoch [6/10], Step [850/2737], Loss: 0.2247\n",
      "Epoch [6/10], Step [860/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [870/2737], Loss: 0.1607\n",
      "Epoch [6/10], Step [880/2737], Loss: 0.2132\n",
      "Epoch [6/10], Step [890/2737], Loss: 0.2149\n",
      "Epoch [6/10], Step [900/2737], Loss: 0.3021\n",
      "Epoch [6/10], Step [910/2737], Loss: 0.2299\n",
      "Epoch [6/10], Step [920/2737], Loss: 0.2172\n",
      "Epoch [6/10], Step [930/2737], Loss: 0.2158\n",
      "Epoch [6/10], Step [940/2737], Loss: 0.2203\n",
      "Epoch [6/10], Step [950/2737], Loss: 0.3613\n",
      "Epoch [6/10], Step [960/2737], Loss: 0.2182\n",
      "Epoch [6/10], Step [970/2737], Loss: 0.3053\n",
      "Epoch [6/10], Step [980/2737], Loss: 0.2027\n",
      "Epoch [6/10], Step [990/2737], Loss: 0.4155\n",
      "Epoch [6/10], Step [1000/2737], Loss: 0.2440\n",
      "Epoch [6/10], Step [1010/2737], Loss: 0.1947\n",
      "Epoch [6/10], Step [1020/2737], Loss: 0.2665\n",
      "Epoch [6/10], Step [1030/2737], Loss: 0.2227\n",
      "Epoch [6/10], Step [1040/2737], Loss: 0.1396\n",
      "Epoch [6/10], Step [1050/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [1060/2737], Loss: 0.3280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1070/2737], Loss: 0.3962\n",
      "Epoch [6/10], Step [1080/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [1090/2737], Loss: 0.2299\n",
      "Epoch [6/10], Step [1100/2737], Loss: 0.1609\n",
      "Epoch [6/10], Step [1110/2737], Loss: 0.2819\n",
      "Epoch [6/10], Step [1120/2737], Loss: 0.2479\n",
      "Epoch [6/10], Step [1130/2737], Loss: 0.1879\n",
      "Epoch [6/10], Step [1140/2737], Loss: 0.3115\n",
      "Epoch [6/10], Step [1150/2737], Loss: 0.4137\n",
      "Epoch [6/10], Step [1160/2737], Loss: 0.2154\n",
      "Epoch [6/10], Step [1170/2737], Loss: 0.1494\n",
      "Epoch [6/10], Step [1180/2737], Loss: 0.2525\n",
      "Epoch [6/10], Step [1190/2737], Loss: 0.1396\n",
      "Epoch [6/10], Step [1200/2737], Loss: 0.2226\n",
      "Epoch [6/10], Step [1210/2737], Loss: 0.2488\n",
      "Epoch [6/10], Step [1220/2737], Loss: 0.2417\n",
      "Epoch [6/10], Step [1230/2737], Loss: 0.2195\n",
      "Epoch [6/10], Step [1240/2737], Loss: 0.1813\n",
      "Epoch [6/10], Step [1250/2737], Loss: 0.2161\n",
      "Epoch [6/10], Step [1260/2737], Loss: 0.2236\n",
      "Epoch [6/10], Step [1270/2737], Loss: 0.2535\n",
      "Epoch [6/10], Step [1280/2737], Loss: 0.1698\n",
      "Epoch [6/10], Step [1290/2737], Loss: 0.2793\n",
      "Epoch [6/10], Step [1300/2737], Loss: 0.2263\n",
      "Epoch [6/10], Step [1310/2737], Loss: 0.3163\n",
      "Epoch [6/10], Step [1320/2737], Loss: 0.2331\n",
      "Epoch [6/10], Step [1330/2737], Loss: 0.1535\n",
      "Epoch [6/10], Step [1340/2737], Loss: 0.1738\n",
      "Epoch [6/10], Step [1350/2737], Loss: 0.1811\n",
      "Epoch [6/10], Step [1360/2737], Loss: 0.2075\n",
      "Epoch [6/10], Step [1370/2737], Loss: 0.2942\n",
      "Epoch [6/10], Step [1380/2737], Loss: 0.2646\n",
      "Epoch [6/10], Step [1390/2737], Loss: 0.2357\n",
      "Epoch [6/10], Step [1400/2737], Loss: 0.2916\n",
      "Epoch [6/10], Step [1410/2737], Loss: 0.2435\n",
      "Epoch [6/10], Step [1420/2737], Loss: 0.2645\n",
      "Epoch [6/10], Step [1430/2737], Loss: 0.2057\n",
      "Epoch [6/10], Step [1440/2737], Loss: 0.1827\n",
      "Epoch [6/10], Step [1450/2737], Loss: 0.2126\n",
      "Epoch [6/10], Step [1460/2737], Loss: 0.1197\n",
      "Epoch [6/10], Step [1470/2737], Loss: 0.2231\n",
      "Epoch [6/10], Step [1480/2737], Loss: 0.2629\n",
      "Epoch [6/10], Step [1490/2737], Loss: 0.1988\n",
      "Epoch [6/10], Step [1500/2737], Loss: 0.2455\n",
      "Epoch [6/10], Step [1510/2737], Loss: 0.2607\n",
      "Epoch [6/10], Step [1520/2737], Loss: 0.1935\n",
      "Epoch [6/10], Step [1530/2737], Loss: 0.2826\n",
      "Epoch [6/10], Step [1540/2737], Loss: 0.2359\n",
      "Epoch [6/10], Step [1550/2737], Loss: 0.1852\n",
      "Epoch [6/10], Step [1560/2737], Loss: 0.2834\n",
      "Epoch [6/10], Step [1570/2737], Loss: 0.1566\n",
      "Epoch [6/10], Step [1580/2737], Loss: 0.2699\n",
      "Epoch [6/10], Step [1590/2737], Loss: 0.2292\n",
      "Epoch [6/10], Step [1600/2737], Loss: 0.1955\n",
      "Epoch [6/10], Step [1610/2737], Loss: 0.2322\n",
      "Epoch [6/10], Step [1620/2737], Loss: 0.1754\n",
      "Epoch [6/10], Step [1630/2737], Loss: 0.2235\n",
      "Epoch [6/10], Step [1640/2737], Loss: 0.2567\n",
      "Epoch [6/10], Step [1650/2737], Loss: 0.2731\n",
      "Epoch [6/10], Step [1660/2737], Loss: 0.2869\n",
      "Epoch [6/10], Step [1670/2737], Loss: 0.1806\n",
      "Epoch [6/10], Step [1680/2737], Loss: 0.2657\n",
      "Epoch [6/10], Step [1690/2737], Loss: 0.1818\n",
      "Epoch [6/10], Step [1700/2737], Loss: 0.2556\n",
      "Epoch [6/10], Step [1710/2737], Loss: 0.2027\n",
      "Epoch [6/10], Step [1720/2737], Loss: 0.2089\n",
      "Epoch [6/10], Step [1730/2737], Loss: 0.2058\n",
      "Epoch [6/10], Step [1740/2737], Loss: 0.2078\n",
      "Epoch [6/10], Step [1750/2737], Loss: 0.2312\n",
      "Epoch [6/10], Step [1760/2737], Loss: 0.2606\n",
      "Epoch [6/10], Step [1770/2737], Loss: 0.1989\n",
      "Epoch [6/10], Step [1780/2737], Loss: 0.2604\n",
      "Epoch [6/10], Step [1790/2737], Loss: 0.2495\n",
      "Epoch [6/10], Step [1800/2737], Loss: 0.1883\n",
      "Epoch [6/10], Step [1810/2737], Loss: 0.2228\n",
      "Epoch [6/10], Step [1820/2737], Loss: 0.1882\n",
      "Epoch [6/10], Step [1830/2737], Loss: 0.2804\n",
      "Epoch [6/10], Step [1840/2737], Loss: 0.2193\n",
      "Epoch [6/10], Step [1850/2737], Loss: 0.2181\n",
      "Epoch [6/10], Step [1860/2737], Loss: 0.1784\n",
      "Epoch [6/10], Step [1870/2737], Loss: 0.2031\n",
      "Epoch [6/10], Step [1880/2737], Loss: 0.1491\n",
      "Epoch [6/10], Step [1890/2737], Loss: 0.2264\n",
      "Epoch [6/10], Step [1900/2737], Loss: 0.3413\n",
      "Epoch [6/10], Step [1910/2737], Loss: 0.2449\n",
      "Epoch [6/10], Step [1920/2737], Loss: 0.1872\n",
      "Epoch [6/10], Step [1930/2737], Loss: 0.3247\n",
      "Epoch [6/10], Step [1940/2737], Loss: 0.1834\n",
      "Epoch [6/10], Step [1950/2737], Loss: 0.1835\n",
      "Epoch [6/10], Step [1960/2737], Loss: 0.2018\n",
      "Epoch [6/10], Step [1970/2737], Loss: 0.3102\n",
      "Epoch [6/10], Step [1980/2737], Loss: 0.1900\n",
      "Epoch [6/10], Step [1990/2737], Loss: 0.3024\n",
      "Epoch [6/10], Step [2000/2737], Loss: 0.2398\n",
      "Epoch [6/10], Step [2010/2737], Loss: 0.1952\n",
      "Epoch [6/10], Step [2020/2737], Loss: 0.2420\n",
      "Epoch [6/10], Step [2030/2737], Loss: 0.1913\n",
      "Epoch [6/10], Step [2040/2737], Loss: 0.2291\n",
      "Epoch [6/10], Step [2050/2737], Loss: 0.3700\n",
      "Epoch [6/10], Step [2060/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [2070/2737], Loss: 0.2278\n",
      "Epoch [6/10], Step [2080/2737], Loss: 0.2239\n",
      "Epoch [6/10], Step [2090/2737], Loss: 0.1725\n",
      "Epoch [6/10], Step [2100/2737], Loss: 0.1441\n",
      "Epoch [6/10], Step [2110/2737], Loss: 0.2016\n",
      "Epoch [6/10], Step [2120/2737], Loss: 0.1823\n",
      "Epoch [6/10], Step [2130/2737], Loss: 0.3051\n",
      "Epoch [6/10], Step [2140/2737], Loss: 0.2536\n",
      "Epoch [6/10], Step [2150/2737], Loss: 0.2268\n",
      "Epoch [6/10], Step [2160/2737], Loss: 0.1682\n",
      "Epoch [6/10], Step [2170/2737], Loss: 0.1671\n",
      "Epoch [6/10], Step [2180/2737], Loss: 0.1806\n",
      "Epoch [6/10], Step [2190/2737], Loss: 0.2460\n",
      "Epoch [6/10], Step [2200/2737], Loss: 0.3287\n",
      "Epoch [6/10], Step [2210/2737], Loss: 0.2584\n",
      "Epoch [6/10], Step [2220/2737], Loss: 0.3187\n",
      "Epoch [6/10], Step [2230/2737], Loss: 0.1651\n",
      "Epoch [6/10], Step [2240/2737], Loss: 0.2865\n",
      "Epoch [6/10], Step [2250/2737], Loss: 0.2187\n",
      "Epoch [6/10], Step [2260/2737], Loss: 0.2387\n",
      "Epoch [6/10], Step [2270/2737], Loss: 0.2493\n",
      "Epoch [6/10], Step [2280/2737], Loss: 0.1984\n",
      "Epoch [6/10], Step [2290/2737], Loss: 0.1471\n",
      "Epoch [6/10], Step [2300/2737], Loss: 0.2804\n",
      "Epoch [6/10], Step [2310/2737], Loss: 0.1929\n",
      "Epoch [6/10], Step [2320/2737], Loss: 0.2512\n",
      "Epoch [6/10], Step [2330/2737], Loss: 0.1953\n",
      "Epoch [6/10], Step [2340/2737], Loss: 0.2617\n",
      "Epoch [6/10], Step [2350/2737], Loss: 0.2230\n",
      "Epoch [6/10], Step [2360/2737], Loss: 0.1538\n",
      "Epoch [6/10], Step [2370/2737], Loss: 0.2637\n",
      "Epoch [6/10], Step [2380/2737], Loss: 0.2277\n",
      "Epoch [6/10], Step [2390/2737], Loss: 0.3163\n",
      "Epoch [6/10], Step [2400/2737], Loss: 0.2115\n",
      "Epoch [6/10], Step [2410/2737], Loss: 0.2630\n",
      "Epoch [6/10], Step [2420/2737], Loss: 0.2814\n",
      "Epoch [6/10], Step [2430/2737], Loss: 0.2396\n",
      "Epoch [6/10], Step [2440/2737], Loss: 0.2164\n",
      "Epoch [6/10], Step [2450/2737], Loss: 0.2476\n",
      "Epoch [6/10], Step [2460/2737], Loss: 0.2658\n",
      "Epoch [6/10], Step [2470/2737], Loss: 0.1804\n",
      "Epoch [6/10], Step [2480/2737], Loss: 0.2514\n",
      "Epoch [6/10], Step [2490/2737], Loss: 0.1909\n",
      "Epoch [6/10], Step [2500/2737], Loss: 0.1992\n",
      "Epoch [6/10], Step [2510/2737], Loss: 0.1675\n",
      "Epoch [6/10], Step [2520/2737], Loss: 0.2332\n",
      "Epoch [6/10], Step [2530/2737], Loss: 0.3368\n",
      "Epoch [6/10], Step [2540/2737], Loss: 0.3383\n",
      "Epoch [6/10], Step [2550/2737], Loss: 0.2585\n",
      "Epoch [6/10], Step [2560/2737], Loss: 0.1639\n",
      "Epoch [6/10], Step [2570/2737], Loss: 0.2991\n",
      "Epoch [6/10], Step [2580/2737], Loss: 0.2059\n",
      "Epoch [6/10], Step [2590/2737], Loss: 0.2599\n",
      "Epoch [6/10], Step [2600/2737], Loss: 0.2181\n",
      "Epoch [6/10], Step [2610/2737], Loss: 0.2372\n",
      "Epoch [6/10], Step [2620/2737], Loss: 0.3408\n",
      "Epoch [6/10], Step [2630/2737], Loss: 0.2497\n",
      "Epoch [6/10], Step [2640/2737], Loss: 0.2763\n",
      "Epoch [6/10], Step [2650/2737], Loss: 0.2864\n",
      "Epoch [6/10], Step [2660/2737], Loss: 0.1597\n",
      "Epoch [6/10], Step [2670/2737], Loss: 0.2117\n",
      "Epoch [6/10], Step [2680/2737], Loss: 0.2179\n",
      "Epoch [6/10], Step [2690/2737], Loss: 0.2257\n",
      "Epoch [6/10], Step [2700/2737], Loss: 0.2840\n",
      "Epoch [6/10], Step [2710/2737], Loss: 0.2707\n",
      "Epoch [6/10], Step [2720/2737], Loss: 0.3044\n",
      "Epoch [6/10], Step [2730/2737], Loss: 0.3274\n",
      "Epoch [6/10], train_loss: 0.2356, val_loss: 0.2441\n",
      "Epoch [7/10], Step [10/2737], Loss: 0.1594\n",
      "Epoch [7/10], Step [20/2737], Loss: 0.2025\n",
      "Epoch [7/10], Step [30/2737], Loss: 0.2029\n",
      "Epoch [7/10], Step [40/2737], Loss: 0.2526\n",
      "Epoch [7/10], Step [50/2737], Loss: 0.2172\n",
      "Epoch [7/10], Step [60/2737], Loss: 0.3085\n",
      "Epoch [7/10], Step [70/2737], Loss: 0.2770\n",
      "Epoch [7/10], Step [80/2737], Loss: 0.2279\n",
      "Epoch [7/10], Step [90/2737], Loss: 0.1450\n",
      "Epoch [7/10], Step [100/2737], Loss: 0.2270\n",
      "Epoch [7/10], Step [110/2737], Loss: 0.2148\n",
      "Epoch [7/10], Step [120/2737], Loss: 0.2562\n",
      "Epoch [7/10], Step [130/2737], Loss: 0.1968\n",
      "Epoch [7/10], Step [140/2737], Loss: 0.2214\n",
      "Epoch [7/10], Step [150/2737], Loss: 0.3466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [160/2737], Loss: 0.2528\n",
      "Epoch [7/10], Step [170/2737], Loss: 0.2148\n",
      "Epoch [7/10], Step [180/2737], Loss: 0.1958\n",
      "Epoch [7/10], Step [190/2737], Loss: 0.2290\n",
      "Epoch [7/10], Step [200/2737], Loss: 0.2095\n",
      "Epoch [7/10], Step [210/2737], Loss: 0.1101\n",
      "Epoch [7/10], Step [220/2737], Loss: 0.1680\n",
      "Epoch [7/10], Step [230/2737], Loss: 0.2212\n",
      "Epoch [7/10], Step [240/2737], Loss: 0.2047\n",
      "Epoch [7/10], Step [250/2737], Loss: 0.2129\n",
      "Epoch [7/10], Step [260/2737], Loss: 0.2778\n",
      "Epoch [7/10], Step [270/2737], Loss: 0.1962\n",
      "Epoch [7/10], Step [280/2737], Loss: 0.1449\n",
      "Epoch [7/10], Step [290/2737], Loss: 0.2381\n",
      "Epoch [7/10], Step [300/2737], Loss: 0.3047\n",
      "Epoch [7/10], Step [310/2737], Loss: 0.1480\n",
      "Epoch [7/10], Step [320/2737], Loss: 0.2692\n",
      "Epoch [7/10], Step [330/2737], Loss: 0.1334\n",
      "Epoch [7/10], Step [340/2737], Loss: 0.2245\n",
      "Epoch [7/10], Step [350/2737], Loss: 0.1773\n",
      "Epoch [7/10], Step [360/2737], Loss: 0.1841\n",
      "Epoch [7/10], Step [370/2737], Loss: 0.2326\n",
      "Epoch [7/10], Step [380/2737], Loss: 0.1620\n",
      "Epoch [7/10], Step [390/2737], Loss: 0.2517\n",
      "Epoch [7/10], Step [400/2737], Loss: 0.2352\n",
      "Epoch [7/10], Step [410/2737], Loss: 0.2595\n",
      "Epoch [7/10], Step [420/2737], Loss: 0.3060\n",
      "Epoch [7/10], Step [430/2737], Loss: 0.2346\n",
      "Epoch [7/10], Step [440/2737], Loss: 0.2428\n",
      "Epoch [7/10], Step [450/2737], Loss: 0.2326\n",
      "Epoch [7/10], Step [460/2737], Loss: 0.1569\n",
      "Epoch [7/10], Step [470/2737], Loss: 0.1644\n",
      "Epoch [7/10], Step [480/2737], Loss: 0.1732\n",
      "Epoch [7/10], Step [490/2737], Loss: 0.2094\n",
      "Epoch [7/10], Step [500/2737], Loss: 0.2092\n",
      "Epoch [7/10], Step [510/2737], Loss: 0.2192\n",
      "Epoch [7/10], Step [520/2737], Loss: 0.1699\n",
      "Epoch [7/10], Step [530/2737], Loss: 0.3134\n",
      "Epoch [7/10], Step [540/2737], Loss: 0.3118\n",
      "Epoch [7/10], Step [550/2737], Loss: 0.1947\n",
      "Epoch [7/10], Step [560/2737], Loss: 0.1503\n",
      "Epoch [7/10], Step [570/2737], Loss: 0.3264\n",
      "Epoch [7/10], Step [580/2737], Loss: 0.1746\n",
      "Epoch [7/10], Step [590/2737], Loss: 0.1810\n",
      "Epoch [7/10], Step [600/2737], Loss: 0.1886\n",
      "Epoch [7/10], Step [610/2737], Loss: 0.2307\n",
      "Epoch [7/10], Step [620/2737], Loss: 0.2115\n",
      "Epoch [7/10], Step [630/2737], Loss: 0.1996\n",
      "Epoch [7/10], Step [640/2737], Loss: 0.0767\n",
      "Epoch [7/10], Step [650/2737], Loss: 0.1663\n",
      "Epoch [7/10], Step [660/2737], Loss: 0.2456\n",
      "Epoch [7/10], Step [670/2737], Loss: 0.2984\n",
      "Epoch [7/10], Step [680/2737], Loss: 0.2786\n",
      "Epoch [7/10], Step [690/2737], Loss: 0.1676\n",
      "Epoch [7/10], Step [700/2737], Loss: 0.2325\n",
      "Epoch [7/10], Step [710/2737], Loss: 0.3197\n",
      "Epoch [7/10], Step [720/2737], Loss: 0.2149\n",
      "Epoch [7/10], Step [730/2737], Loss: 0.1988\n",
      "Epoch [7/10], Step [740/2737], Loss: 0.2168\n",
      "Epoch [7/10], Step [750/2737], Loss: 0.1506\n",
      "Epoch [7/10], Step [760/2737], Loss: 0.0968\n",
      "Epoch [7/10], Step [770/2737], Loss: 0.1731\n",
      "Epoch [7/10], Step [780/2737], Loss: 0.1989\n",
      "Epoch [7/10], Step [790/2737], Loss: 0.1901\n",
      "Epoch [7/10], Step [800/2737], Loss: 0.2696\n",
      "Epoch [7/10], Step [810/2737], Loss: 0.1745\n",
      "Epoch [7/10], Step [820/2737], Loss: 0.3063\n",
      "Epoch [7/10], Step [830/2737], Loss: 0.1927\n",
      "Epoch [7/10], Step [840/2737], Loss: 0.3013\n",
      "Epoch [7/10], Step [850/2737], Loss: 0.1974\n",
      "Epoch [7/10], Step [860/2737], Loss: 0.2202\n",
      "Epoch [7/10], Step [870/2737], Loss: 0.2393\n",
      "Epoch [7/10], Step [880/2737], Loss: 0.2642\n",
      "Epoch [7/10], Step [890/2737], Loss: 0.1664\n",
      "Epoch [7/10], Step [900/2737], Loss: 0.4001\n",
      "Epoch [7/10], Step [910/2737], Loss: 0.2129\n",
      "Epoch [7/10], Step [920/2737], Loss: 0.2142\n",
      "Epoch [7/10], Step [930/2737], Loss: 0.2200\n",
      "Epoch [7/10], Step [940/2737], Loss: 0.3169\n",
      "Epoch [7/10], Step [950/2737], Loss: 0.2264\n",
      "Epoch [7/10], Step [960/2737], Loss: 0.2587\n",
      "Epoch [7/10], Step [970/2737], Loss: 0.3638\n",
      "Epoch [7/10], Step [980/2737], Loss: 0.3700\n",
      "Epoch [7/10], Step [990/2737], Loss: 0.3850\n",
      "Epoch [7/10], Step [1000/2737], Loss: 0.2045\n",
      "Epoch [7/10], Step [1010/2737], Loss: 0.1953\n",
      "Epoch [7/10], Step [1020/2737], Loss: 0.1775\n",
      "Epoch [7/10], Step [1030/2737], Loss: 0.2780\n",
      "Epoch [7/10], Step [1040/2737], Loss: 0.2603\n",
      "Epoch [7/10], Step [1050/2737], Loss: 0.1485\n",
      "Epoch [7/10], Step [1060/2737], Loss: 0.1929\n",
      "Epoch [7/10], Step [1070/2737], Loss: 0.1719\n",
      "Epoch [7/10], Step [1080/2737], Loss: 0.1744\n",
      "Epoch [7/10], Step [1090/2737], Loss: 0.2331\n",
      "Epoch [7/10], Step [1100/2737], Loss: 0.2258\n",
      "Epoch [7/10], Step [1110/2737], Loss: 0.1914\n",
      "Epoch [7/10], Step [1120/2737], Loss: 0.3127\n",
      "Epoch [7/10], Step [1130/2737], Loss: 0.2979\n",
      "Epoch [7/10], Step [1140/2737], Loss: 0.1384\n",
      "Epoch [7/10], Step [1150/2737], Loss: 0.1913\n",
      "Epoch [7/10], Step [1160/2737], Loss: 0.2485\n",
      "Epoch [7/10], Step [1170/2737], Loss: 0.1755\n",
      "Epoch [7/10], Step [1180/2737], Loss: 0.1635\n",
      "Epoch [7/10], Step [1190/2737], Loss: 0.2555\n",
      "Epoch [7/10], Step [1200/2737], Loss: 0.1258\n",
      "Epoch [7/10], Step [1210/2737], Loss: 0.1290\n",
      "Epoch [7/10], Step [1220/2737], Loss: 0.3626\n",
      "Epoch [7/10], Step [1230/2737], Loss: 0.1622\n",
      "Epoch [7/10], Step [1240/2737], Loss: 0.2020\n",
      "Epoch [7/10], Step [1250/2737], Loss: 0.2498\n",
      "Epoch [7/10], Step [1260/2737], Loss: 0.2769\n",
      "Epoch [7/10], Step [1270/2737], Loss: 0.1761\n",
      "Epoch [7/10], Step [1280/2737], Loss: 0.2620\n",
      "Epoch [7/10], Step [1290/2737], Loss: 0.1851\n",
      "Epoch [7/10], Step [1300/2737], Loss: 0.2876\n",
      "Epoch [7/10], Step [1310/2737], Loss: 0.1730\n",
      "Epoch [7/10], Step [1320/2737], Loss: 0.2232\n",
      "Epoch [7/10], Step [1330/2737], Loss: 0.2293\n",
      "Epoch [7/10], Step [1340/2737], Loss: 0.2648\n",
      "Epoch [7/10], Step [1350/2737], Loss: 0.1774\n",
      "Epoch [7/10], Step [1360/2737], Loss: 0.2513\n",
      "Epoch [7/10], Step [1370/2737], Loss: 0.2056\n",
      "Epoch [7/10], Step [1380/2737], Loss: 0.2625\n",
      "Epoch [7/10], Step [1390/2737], Loss: 0.3453\n",
      "Epoch [7/10], Step [1400/2737], Loss: 0.1519\n",
      "Epoch [7/10], Step [1410/2737], Loss: 0.1841\n",
      "Epoch [7/10], Step [1420/2737], Loss: 0.2491\n",
      "Epoch [7/10], Step [1430/2737], Loss: 0.2675\n",
      "Epoch [7/10], Step [1440/2737], Loss: 0.2127\n",
      "Epoch [7/10], Step [1450/2737], Loss: 0.3254\n",
      "Epoch [7/10], Step [1460/2737], Loss: 0.2443\n",
      "Epoch [7/10], Step [1470/2737], Loss: 0.3133\n",
      "Epoch [7/10], Step [1480/2737], Loss: 0.2541\n",
      "Epoch [7/10], Step [1490/2737], Loss: 0.1899\n",
      "Epoch [7/10], Step [1500/2737], Loss: 0.3725\n",
      "Epoch [7/10], Step [1510/2737], Loss: 0.2089\n",
      "Epoch [7/10], Step [1520/2737], Loss: 0.2527\n",
      "Epoch [7/10], Step [1530/2737], Loss: 0.2174\n",
      "Epoch [7/10], Step [1540/2737], Loss: 0.2590\n",
      "Epoch [7/10], Step [1550/2737], Loss: 0.1368\n",
      "Epoch [7/10], Step [1560/2737], Loss: 0.2123\n",
      "Epoch [7/10], Step [1570/2737], Loss: 0.2211\n",
      "Epoch [7/10], Step [1580/2737], Loss: 0.2424\n",
      "Epoch [7/10], Step [1590/2737], Loss: 0.3763\n",
      "Epoch [7/10], Step [1600/2737], Loss: 0.2396\n",
      "Epoch [7/10], Step [1610/2737], Loss: 0.1850\n",
      "Epoch [7/10], Step [1620/2737], Loss: 0.3015\n",
      "Epoch [7/10], Step [1630/2737], Loss: 0.3206\n",
      "Epoch [7/10], Step [1640/2737], Loss: 0.1969\n",
      "Epoch [7/10], Step [1650/2737], Loss: 0.1662\n",
      "Epoch [7/10], Step [1660/2737], Loss: 0.1850\n",
      "Epoch [7/10], Step [1670/2737], Loss: 0.2493\n",
      "Epoch [7/10], Step [1680/2737], Loss: 0.1830\n",
      "Epoch [7/10], Step [1690/2737], Loss: 0.3316\n",
      "Epoch [7/10], Step [1700/2737], Loss: 0.2777\n",
      "Epoch [7/10], Step [1710/2737], Loss: 0.2358\n",
      "Epoch [7/10], Step [1720/2737], Loss: 0.1856\n",
      "Epoch [7/10], Step [1730/2737], Loss: 0.3463\n",
      "Epoch [7/10], Step [1740/2737], Loss: 0.2679\n",
      "Epoch [7/10], Step [1750/2737], Loss: 0.3528\n",
      "Epoch [7/10], Step [1760/2737], Loss: 0.1326\n",
      "Epoch [7/10], Step [1770/2737], Loss: 0.1366\n",
      "Epoch [7/10], Step [1780/2737], Loss: 0.1762\n",
      "Epoch [7/10], Step [1790/2737], Loss: 0.2101\n",
      "Epoch [7/10], Step [1800/2737], Loss: 0.2836\n",
      "Epoch [7/10], Step [1810/2737], Loss: 0.2742\n",
      "Epoch [7/10], Step [1820/2737], Loss: 0.1778\n",
      "Epoch [7/10], Step [1830/2737], Loss: 0.1366\n",
      "Epoch [7/10], Step [1840/2737], Loss: 0.2884\n",
      "Epoch [7/10], Step [1850/2737], Loss: 0.2024\n",
      "Epoch [7/10], Step [1860/2737], Loss: 0.3372\n",
      "Epoch [7/10], Step [1870/2737], Loss: 0.2788\n",
      "Epoch [7/10], Step [1880/2737], Loss: 0.2697\n",
      "Epoch [7/10], Step [1890/2737], Loss: 0.2226\n",
      "Epoch [7/10], Step [1900/2737], Loss: 0.3160\n",
      "Epoch [7/10], Step [1910/2737], Loss: 0.2057\n",
      "Epoch [7/10], Step [1920/2737], Loss: 0.2190\n",
      "Epoch [7/10], Step [1930/2737], Loss: 0.2346\n",
      "Epoch [7/10], Step [1940/2737], Loss: 0.2371\n",
      "Epoch [7/10], Step [1950/2737], Loss: 0.2098\n",
      "Epoch [7/10], Step [1960/2737], Loss: 0.1894\n",
      "Epoch [7/10], Step [1970/2737], Loss: 0.1966\n",
      "Epoch [7/10], Step [1980/2737], Loss: 0.2520\n",
      "Epoch [7/10], Step [1990/2737], Loss: 0.1832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [2000/2737], Loss: 0.2284\n",
      "Epoch [7/10], Step [2010/2737], Loss: 0.3736\n",
      "Epoch [7/10], Step [2020/2737], Loss: 0.1813\n",
      "Epoch [7/10], Step [2030/2737], Loss: 0.2575\n",
      "Epoch [7/10], Step [2040/2737], Loss: 0.3171\n",
      "Epoch [7/10], Step [2050/2737], Loss: 0.2876\n",
      "Epoch [7/10], Step [2060/2737], Loss: 0.4572\n",
      "Epoch [7/10], Step [2070/2737], Loss: 0.1575\n",
      "Epoch [7/10], Step [2080/2737], Loss: 0.2958\n",
      "Epoch [7/10], Step [2090/2737], Loss: 0.2242\n",
      "Epoch [7/10], Step [2100/2737], Loss: 0.2717\n",
      "Epoch [7/10], Step [2110/2737], Loss: 0.2569\n",
      "Epoch [7/10], Step [2120/2737], Loss: 0.2323\n",
      "Epoch [7/10], Step [2130/2737], Loss: 0.1852\n",
      "Epoch [7/10], Step [2140/2737], Loss: 0.2193\n",
      "Epoch [7/10], Step [2150/2737], Loss: 0.2209\n",
      "Epoch [7/10], Step [2160/2737], Loss: 0.2003\n",
      "Epoch [7/10], Step [2170/2737], Loss: 0.1767\n",
      "Epoch [7/10], Step [2180/2737], Loss: 0.1452\n",
      "Epoch [7/10], Step [2190/2737], Loss: 0.2118\n",
      "Epoch [7/10], Step [2200/2737], Loss: 0.2013\n",
      "Epoch [7/10], Step [2210/2737], Loss: 0.2284\n",
      "Epoch [7/10], Step [2220/2737], Loss: 0.2536\n",
      "Epoch [7/10], Step [2230/2737], Loss: 0.2028\n",
      "Epoch [7/10], Step [2240/2737], Loss: 0.2302\n",
      "Epoch [7/10], Step [2250/2737], Loss: 0.2141\n",
      "Epoch [7/10], Step [2260/2737], Loss: 0.3349\n",
      "Epoch [7/10], Step [2270/2737], Loss: 0.3514\n",
      "Epoch [7/10], Step [2280/2737], Loss: 0.2759\n",
      "Epoch [7/10], Step [2290/2737], Loss: 0.1626\n",
      "Epoch [7/10], Step [2300/2737], Loss: 0.1935\n",
      "Epoch [7/10], Step [2310/2737], Loss: 0.2106\n",
      "Epoch [7/10], Step [2320/2737], Loss: 0.2089\n",
      "Epoch [7/10], Step [2330/2737], Loss: 0.2202\n",
      "Epoch [7/10], Step [2340/2737], Loss: 0.3371\n",
      "Epoch [7/10], Step [2350/2737], Loss: 0.2943\n",
      "Epoch [7/10], Step [2360/2737], Loss: 0.2513\n",
      "Epoch [7/10], Step [2370/2737], Loss: 0.2894\n",
      "Epoch [7/10], Step [2380/2737], Loss: 0.2732\n",
      "Epoch [7/10], Step [2390/2737], Loss: 0.2378\n",
      "Epoch [7/10], Step [2400/2737], Loss: 0.2525\n",
      "Epoch [7/10], Step [2410/2737], Loss: 0.2989\n",
      "Epoch [7/10], Step [2420/2737], Loss: 0.2895\n",
      "Epoch [7/10], Step [2430/2737], Loss: 0.2719\n",
      "Epoch [7/10], Step [2440/2737], Loss: 0.2321\n",
      "Epoch [7/10], Step [2450/2737], Loss: 0.1733\n",
      "Epoch [7/10], Step [2460/2737], Loss: 0.2220\n",
      "Epoch [7/10], Step [2470/2737], Loss: 0.2397\n",
      "Epoch [7/10], Step [2480/2737], Loss: 0.2054\n",
      "Epoch [7/10], Step [2490/2737], Loss: 0.1552\n",
      "Epoch [7/10], Step [2500/2737], Loss: 0.2114\n",
      "Epoch [7/10], Step [2510/2737], Loss: 0.2650\n",
      "Epoch [7/10], Step [2520/2737], Loss: 0.1205\n",
      "Epoch [7/10], Step [2530/2737], Loss: 0.1700\n",
      "Epoch [7/10], Step [2540/2737], Loss: 0.2720\n",
      "Epoch [7/10], Step [2550/2737], Loss: 0.1581\n",
      "Epoch [7/10], Step [2560/2737], Loss: 0.1784\n",
      "Epoch [7/10], Step [2570/2737], Loss: 0.2293\n",
      "Epoch [7/10], Step [2580/2737], Loss: 0.2303\n",
      "Epoch [7/10], Step [2590/2737], Loss: 0.1607\n",
      "Epoch [7/10], Step [2600/2737], Loss: 0.3047\n",
      "Epoch [7/10], Step [2610/2737], Loss: 0.1658\n",
      "Epoch [7/10], Step [2620/2737], Loss: 0.2863\n",
      "Epoch [7/10], Step [2630/2737], Loss: 0.1851\n",
      "Epoch [7/10], Step [2640/2737], Loss: 0.2685\n",
      "Epoch [7/10], Step [2650/2737], Loss: 0.1755\n",
      "Epoch [7/10], Step [2660/2737], Loss: 0.1494\n",
      "Epoch [7/10], Step [2670/2737], Loss: 0.2075\n",
      "Epoch [7/10], Step [2680/2737], Loss: 0.1338\n",
      "Epoch [7/10], Step [2690/2737], Loss: 0.2079\n",
      "Epoch [7/10], Step [2700/2737], Loss: 0.2238\n",
      "Epoch [7/10], Step [2710/2737], Loss: 0.2644\n",
      "Epoch [7/10], Step [2720/2737], Loss: 0.2281\n",
      "Epoch [7/10], Step [2730/2737], Loss: 0.1722\n",
      "Epoch [7/10], train_loss: 0.2280, val_loss: 0.2687\n",
      "Epoch [8/10], Step [10/2737], Loss: 0.3494\n",
      "Epoch [8/10], Step [20/2737], Loss: 0.1875\n",
      "Epoch [8/10], Step [30/2737], Loss: 0.2051\n",
      "Epoch [8/10], Step [40/2737], Loss: 0.3002\n",
      "Epoch [8/10], Step [50/2737], Loss: 0.2417\n",
      "Epoch [8/10], Step [60/2737], Loss: 0.2351\n",
      "Epoch [8/10], Step [70/2737], Loss: 0.2114\n",
      "Epoch [8/10], Step [80/2737], Loss: 0.1969\n",
      "Epoch [8/10], Step [90/2737], Loss: 0.1571\n",
      "Epoch [8/10], Step [100/2737], Loss: 0.2906\n",
      "Epoch [8/10], Step [110/2737], Loss: 0.1626\n",
      "Epoch [8/10], Step [120/2737], Loss: 0.1502\n",
      "Epoch [8/10], Step [130/2737], Loss: 0.1424\n",
      "Epoch [8/10], Step [140/2737], Loss: 0.1938\n",
      "Epoch [8/10], Step [150/2737], Loss: 0.2546\n",
      "Epoch [8/10], Step [160/2737], Loss: 0.2465\n",
      "Epoch [8/10], Step [170/2737], Loss: 0.2173\n",
      "Epoch [8/10], Step [180/2737], Loss: 0.2119\n",
      "Epoch [8/10], Step [190/2737], Loss: 0.2328\n",
      "Epoch [8/10], Step [200/2737], Loss: 0.1985\n",
      "Epoch [8/10], Step [210/2737], Loss: 0.2017\n",
      "Epoch [8/10], Step [220/2737], Loss: 0.3470\n",
      "Epoch [8/10], Step [230/2737], Loss: 0.2520\n",
      "Epoch [8/10], Step [240/2737], Loss: 0.3268\n",
      "Epoch [8/10], Step [250/2737], Loss: 0.2950\n",
      "Epoch [8/10], Step [260/2737], Loss: 0.2087\n",
      "Epoch [8/10], Step [270/2737], Loss: 0.2273\n",
      "Epoch [8/10], Step [280/2737], Loss: 0.1199\n",
      "Epoch [8/10], Step [290/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [300/2737], Loss: 0.1652\n",
      "Epoch [8/10], Step [310/2737], Loss: 0.1934\n",
      "Epoch [8/10], Step [320/2737], Loss: 0.1789\n",
      "Epoch [8/10], Step [330/2737], Loss: 0.3082\n",
      "Epoch [8/10], Step [340/2737], Loss: 0.1909\n",
      "Epoch [8/10], Step [350/2737], Loss: 0.1752\n",
      "Epoch [8/10], Step [360/2737], Loss: 0.1953\n",
      "Epoch [8/10], Step [370/2737], Loss: 0.3456\n",
      "Epoch [8/10], Step [380/2737], Loss: 0.1751\n",
      "Epoch [8/10], Step [390/2737], Loss: 0.1452\n",
      "Epoch [8/10], Step [400/2737], Loss: 0.1593\n",
      "Epoch [8/10], Step [410/2737], Loss: 0.1835\n",
      "Epoch [8/10], Step [420/2737], Loss: 0.2931\n",
      "Epoch [8/10], Step [430/2737], Loss: 0.2489\n",
      "Epoch [8/10], Step [440/2737], Loss: 0.1880\n",
      "Epoch [8/10], Step [450/2737], Loss: 0.2069\n",
      "Epoch [8/10], Step [460/2737], Loss: 0.1470\n",
      "Epoch [8/10], Step [470/2737], Loss: 0.1917\n",
      "Epoch [8/10], Step [480/2737], Loss: 0.1491\n",
      "Epoch [8/10], Step [490/2737], Loss: 0.1841\n",
      "Epoch [8/10], Step [500/2737], Loss: 0.1924\n",
      "Epoch [8/10], Step [510/2737], Loss: 0.1711\n",
      "Epoch [8/10], Step [520/2737], Loss: 0.2473\n",
      "Epoch [8/10], Step [530/2737], Loss: 0.2432\n",
      "Epoch [8/10], Step [540/2737], Loss: 0.1700\n",
      "Epoch [8/10], Step [550/2737], Loss: 0.1841\n",
      "Epoch [8/10], Step [560/2737], Loss: 0.1351\n",
      "Epoch [8/10], Step [570/2737], Loss: 0.2102\n",
      "Epoch [8/10], Step [580/2737], Loss: 0.2431\n",
      "Epoch [8/10], Step [590/2737], Loss: 0.1818\n",
      "Epoch [8/10], Step [600/2737], Loss: 0.2169\n",
      "Epoch [8/10], Step [610/2737], Loss: 0.2693\n",
      "Epoch [8/10], Step [620/2737], Loss: 0.2456\n",
      "Epoch [8/10], Step [630/2737], Loss: 0.1524\n",
      "Epoch [8/10], Step [640/2737], Loss: 0.1269\n",
      "Epoch [8/10], Step [650/2737], Loss: 0.2300\n",
      "Epoch [8/10], Step [660/2737], Loss: 0.2898\n",
      "Epoch [8/10], Step [670/2737], Loss: 0.1603\n",
      "Epoch [8/10], Step [680/2737], Loss: 0.1773\n",
      "Epoch [8/10], Step [690/2737], Loss: 0.2542\n",
      "Epoch [8/10], Step [700/2737], Loss: 0.2885\n",
      "Epoch [8/10], Step [710/2737], Loss: 0.2273\n",
      "Epoch [8/10], Step [720/2737], Loss: 0.2563\n",
      "Epoch [8/10], Step [730/2737], Loss: 0.2152\n",
      "Epoch [8/10], Step [740/2737], Loss: 0.1889\n",
      "Epoch [8/10], Step [750/2737], Loss: 0.2694\n",
      "Epoch [8/10], Step [760/2737], Loss: 0.1720\n",
      "Epoch [8/10], Step [770/2737], Loss: 0.1916\n",
      "Epoch [8/10], Step [780/2737], Loss: 0.2886\n",
      "Epoch [8/10], Step [790/2737], Loss: 0.2547\n",
      "Epoch [8/10], Step [800/2737], Loss: 0.1994\n",
      "Epoch [8/10], Step [810/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [820/2737], Loss: 0.2206\n",
      "Epoch [8/10], Step [830/2737], Loss: 0.2152\n",
      "Epoch [8/10], Step [840/2737], Loss: 0.1237\n",
      "Epoch [8/10], Step [850/2737], Loss: 0.1890\n",
      "Epoch [8/10], Step [860/2737], Loss: 0.2395\n",
      "Epoch [8/10], Step [870/2737], Loss: 0.2348\n",
      "Epoch [8/10], Step [880/2737], Loss: 0.2423\n",
      "Epoch [8/10], Step [890/2737], Loss: 0.3033\n",
      "Epoch [8/10], Step [900/2737], Loss: 0.2576\n",
      "Epoch [8/10], Step [910/2737], Loss: 0.1949\n",
      "Epoch [8/10], Step [920/2737], Loss: 0.1787\n",
      "Epoch [8/10], Step [930/2737], Loss: 0.1472\n",
      "Epoch [8/10], Step [940/2737], Loss: 0.1720\n",
      "Epoch [8/10], Step [950/2737], Loss: 0.1843\n",
      "Epoch [8/10], Step [960/2737], Loss: 0.3166\n",
      "Epoch [8/10], Step [970/2737], Loss: 0.1556\n",
      "Epoch [8/10], Step [980/2737], Loss: 0.1696\n",
      "Epoch [8/10], Step [990/2737], Loss: 0.1944\n",
      "Epoch [8/10], Step [1000/2737], Loss: 0.2340\n",
      "Epoch [8/10], Step [1010/2737], Loss: 0.1634\n",
      "Epoch [8/10], Step [1020/2737], Loss: 0.3584\n",
      "Epoch [8/10], Step [1030/2737], Loss: 0.4108\n",
      "Epoch [8/10], Step [1040/2737], Loss: 0.1886\n",
      "Epoch [8/10], Step [1050/2737], Loss: 0.2562\n",
      "Epoch [8/10], Step [1060/2737], Loss: 0.1952\n",
      "Epoch [8/10], Step [1070/2737], Loss: 0.2236\n",
      "Epoch [8/10], Step [1080/2737], Loss: 0.2993\n",
      "Epoch [8/10], Step [1090/2737], Loss: 0.3349\n",
      "Epoch [8/10], Step [1100/2737], Loss: 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [1110/2737], Loss: 0.2067\n",
      "Epoch [8/10], Step [1120/2737], Loss: 0.2015\n",
      "Epoch [8/10], Step [1130/2737], Loss: 0.2624\n",
      "Epoch [8/10], Step [1140/2737], Loss: 0.1579\n",
      "Epoch [8/10], Step [1150/2737], Loss: 0.3011\n",
      "Epoch [8/10], Step [1160/2737], Loss: 0.2075\n",
      "Epoch [8/10], Step [1170/2737], Loss: 0.1660\n",
      "Epoch [8/10], Step [1180/2737], Loss: 0.1522\n",
      "Epoch [8/10], Step [1190/2737], Loss: 0.1484\n",
      "Epoch [8/10], Step [1200/2737], Loss: 0.1340\n",
      "Epoch [8/10], Step [1210/2737], Loss: 0.2764\n",
      "Epoch [8/10], Step [1220/2737], Loss: 0.2784\n",
      "Epoch [8/10], Step [1230/2737], Loss: 0.2371\n",
      "Epoch [8/10], Step [1240/2737], Loss: 0.3446\n",
      "Epoch [8/10], Step [1250/2737], Loss: 0.1732\n",
      "Epoch [8/10], Step [1260/2737], Loss: 0.1937\n",
      "Epoch [8/10], Step [1270/2737], Loss: 0.2802\n",
      "Epoch [8/10], Step [1280/2737], Loss: 0.2069\n",
      "Epoch [8/10], Step [1290/2737], Loss: 0.1952\n",
      "Epoch [8/10], Step [1300/2737], Loss: 0.3216\n",
      "Epoch [8/10], Step [1310/2737], Loss: 0.2556\n",
      "Epoch [8/10], Step [1320/2737], Loss: 0.3352\n",
      "Epoch [8/10], Step [1330/2737], Loss: 0.2524\n",
      "Epoch [8/10], Step [1340/2737], Loss: 0.2357\n",
      "Epoch [8/10], Step [1350/2737], Loss: 0.2310\n",
      "Epoch [8/10], Step [1360/2737], Loss: 0.2221\n",
      "Epoch [8/10], Step [1370/2737], Loss: 0.1959\n",
      "Epoch [8/10], Step [1380/2737], Loss: 0.1957\n",
      "Epoch [8/10], Step [1390/2737], Loss: 0.2349\n",
      "Epoch [8/10], Step [1400/2737], Loss: 0.3620\n",
      "Epoch [8/10], Step [1410/2737], Loss: 0.3011\n",
      "Epoch [8/10], Step [1420/2737], Loss: 0.2485\n",
      "Epoch [8/10], Step [1430/2737], Loss: 0.1126\n",
      "Epoch [8/10], Step [1440/2737], Loss: 0.1495\n",
      "Epoch [8/10], Step [1450/2737], Loss: 0.2224\n",
      "Epoch [8/10], Step [1460/2737], Loss: 0.2449\n",
      "Epoch [8/10], Step [1470/2737], Loss: 0.2226\n",
      "Epoch [8/10], Step [1480/2737], Loss: 0.2573\n",
      "Epoch [8/10], Step [1490/2737], Loss: 0.2197\n",
      "Epoch [8/10], Step [1500/2737], Loss: 0.1949\n",
      "Epoch [8/10], Step [1510/2737], Loss: 0.2944\n",
      "Epoch [8/10], Step [1520/2737], Loss: 0.1757\n",
      "Epoch [8/10], Step [1530/2737], Loss: 0.2159\n",
      "Epoch [8/10], Step [1540/2737], Loss: 0.2540\n",
      "Epoch [8/10], Step [1550/2737], Loss: 0.2341\n",
      "Epoch [8/10], Step [1560/2737], Loss: 0.1833\n",
      "Epoch [8/10], Step [1570/2737], Loss: 0.2384\n",
      "Epoch [8/10], Step [1580/2737], Loss: 0.1624\n",
      "Epoch [8/10], Step [1590/2737], Loss: 0.2106\n",
      "Epoch [8/10], Step [1600/2737], Loss: 0.2940\n",
      "Epoch [8/10], Step [1610/2737], Loss: 0.2111\n",
      "Epoch [8/10], Step [1620/2737], Loss: 0.2963\n",
      "Epoch [8/10], Step [1630/2737], Loss: 0.2215\n",
      "Epoch [8/10], Step [1640/2737], Loss: 0.3089\n",
      "Epoch [8/10], Step [1650/2737], Loss: 0.2139\n",
      "Epoch [8/10], Step [1660/2737], Loss: 0.3356\n",
      "Epoch [8/10], Step [1670/2737], Loss: 0.1299\n",
      "Epoch [8/10], Step [1680/2737], Loss: 0.3004\n",
      "Epoch [8/10], Step [1690/2737], Loss: 0.2454\n",
      "Epoch [8/10], Step [1700/2737], Loss: 0.1861\n",
      "Epoch [8/10], Step [1710/2737], Loss: 0.3094\n",
      "Epoch [8/10], Step [1720/2737], Loss: 0.3539\n",
      "Epoch [8/10], Step [1730/2737], Loss: 0.2150\n",
      "Epoch [8/10], Step [1740/2737], Loss: 0.2065\n",
      "Epoch [8/10], Step [1750/2737], Loss: 0.1472\n",
      "Epoch [8/10], Step [1760/2737], Loss: 0.3000\n",
      "Epoch [8/10], Step [1770/2737], Loss: 0.2332\n",
      "Epoch [8/10], Step [1780/2737], Loss: 0.2125\n",
      "Epoch [8/10], Step [1790/2737], Loss: 0.2555\n",
      "Epoch [8/10], Step [1800/2737], Loss: 0.2534\n",
      "Epoch [8/10], Step [1810/2737], Loss: 0.1699\n",
      "Epoch [8/10], Step [1820/2737], Loss: 0.1909\n",
      "Epoch [8/10], Step [1830/2737], Loss: 0.2320\n",
      "Epoch [8/10], Step [1840/2737], Loss: 0.2626\n",
      "Epoch [8/10], Step [1850/2737], Loss: 0.1833\n",
      "Epoch [8/10], Step [1860/2737], Loss: 0.1634\n",
      "Epoch [8/10], Step [1870/2737], Loss: 0.2566\n",
      "Epoch [8/10], Step [1880/2737], Loss: 0.1604\n",
      "Epoch [8/10], Step [1890/2737], Loss: 0.1147\n",
      "Epoch [8/10], Step [1900/2737], Loss: 0.1420\n",
      "Epoch [8/10], Step [1910/2737], Loss: 0.3716\n",
      "Epoch [8/10], Step [1920/2737], Loss: 0.2924\n",
      "Epoch [8/10], Step [1930/2737], Loss: 0.3237\n",
      "Epoch [8/10], Step [1940/2737], Loss: 0.1799\n",
      "Epoch [8/10], Step [1950/2737], Loss: 0.2147\n",
      "Epoch [8/10], Step [1960/2737], Loss: 0.2365\n",
      "Epoch [8/10], Step [1970/2737], Loss: 0.2901\n",
      "Epoch [8/10], Step [1980/2737], Loss: 0.1766\n",
      "Epoch [8/10], Step [1990/2737], Loss: 0.2586\n",
      "Epoch [8/10], Step [2000/2737], Loss: 0.1978\n",
      "Epoch [8/10], Step [2010/2737], Loss: 0.2696\n",
      "Epoch [8/10], Step [2020/2737], Loss: 0.1725\n",
      "Epoch [8/10], Step [2030/2737], Loss: 0.2206\n",
      "Epoch [8/10], Step [2040/2737], Loss: 0.2409\n",
      "Epoch [8/10], Step [2050/2737], Loss: 0.1399\n",
      "Epoch [8/10], Step [2060/2737], Loss: 0.3853\n",
      "Epoch [8/10], Step [2070/2737], Loss: 0.2157\n",
      "Epoch [8/10], Step [2080/2737], Loss: 0.2118\n",
      "Epoch [8/10], Step [2090/2737], Loss: 0.2346\n",
      "Epoch [8/10], Step [2100/2737], Loss: 0.2949\n",
      "Epoch [8/10], Step [2110/2737], Loss: 0.2263\n",
      "Epoch [8/10], Step [2120/2737], Loss: 0.2344\n",
      "Epoch [8/10], Step [2130/2737], Loss: 0.2509\n",
      "Epoch [8/10], Step [2140/2737], Loss: 0.2674\n",
      "Epoch [8/10], Step [2150/2737], Loss: 0.2656\n",
      "Epoch [8/10], Step [2160/2737], Loss: 0.2050\n",
      "Epoch [8/10], Step [2170/2737], Loss: 0.1643\n",
      "Epoch [8/10], Step [2180/2737], Loss: 0.1712\n",
      "Epoch [8/10], Step [2190/2737], Loss: 0.3754\n",
      "Epoch [8/10], Step [2200/2737], Loss: 0.1797\n",
      "Epoch [8/10], Step [2210/2737], Loss: 0.3144\n",
      "Epoch [8/10], Step [2220/2737], Loss: 0.1615\n",
      "Epoch [8/10], Step [2230/2737], Loss: 0.1430\n",
      "Epoch [8/10], Step [2240/2737], Loss: 0.3323\n",
      "Epoch [8/10], Step [2250/2737], Loss: 0.1462\n",
      "Epoch [8/10], Step [2260/2737], Loss: 0.2220\n",
      "Epoch [8/10], Step [2270/2737], Loss: 0.2817\n",
      "Epoch [8/10], Step [2280/2737], Loss: 0.2279\n",
      "Epoch [8/10], Step [2290/2737], Loss: 0.1759\n",
      "Epoch [8/10], Step [2300/2737], Loss: 0.2346\n",
      "Epoch [8/10], Step [2310/2737], Loss: 0.1962\n",
      "Epoch [8/10], Step [2320/2737], Loss: 0.2972\n",
      "Epoch [8/10], Step [2330/2737], Loss: 0.2367\n",
      "Epoch [8/10], Step [2340/2737], Loss: 0.1353\n",
      "Epoch [8/10], Step [2350/2737], Loss: 0.2848\n",
      "Epoch [8/10], Step [2360/2737], Loss: 0.2411\n",
      "Epoch [8/10], Step [2370/2737], Loss: 0.1883\n",
      "Epoch [8/10], Step [2380/2737], Loss: 0.1957\n",
      "Epoch [8/10], Step [2390/2737], Loss: 0.1769\n",
      "Epoch [8/10], Step [2400/2737], Loss: 0.2440\n",
      "Epoch [8/10], Step [2410/2737], Loss: 0.1939\n",
      "Epoch [8/10], Step [2420/2737], Loss: 0.1441\n",
      "Epoch [8/10], Step [2430/2737], Loss: 0.3202\n",
      "Epoch [8/10], Step [2440/2737], Loss: 0.3565\n",
      "Epoch [8/10], Step [2450/2737], Loss: 0.2559\n",
      "Epoch [8/10], Step [2460/2737], Loss: 0.1735\n",
      "Epoch [8/10], Step [2470/2737], Loss: 0.2194\n",
      "Epoch [8/10], Step [2480/2737], Loss: 0.4018\n",
      "Epoch [8/10], Step [2490/2737], Loss: 0.2595\n",
      "Epoch [8/10], Step [2500/2737], Loss: 0.2052\n",
      "Epoch [8/10], Step [2510/2737], Loss: 0.2896\n",
      "Epoch [8/10], Step [2520/2737], Loss: 0.2158\n",
      "Epoch [8/10], Step [2530/2737], Loss: 0.1657\n",
      "Epoch [8/10], Step [2540/2737], Loss: 0.3341\n",
      "Epoch [8/10], Step [2550/2737], Loss: 0.1713\n",
      "Epoch [8/10], Step [2560/2737], Loss: 0.2278\n",
      "Epoch [8/10], Step [2570/2737], Loss: 0.1899\n",
      "Epoch [8/10], Step [2580/2737], Loss: 0.1594\n",
      "Epoch [8/10], Step [2590/2737], Loss: 0.2512\n",
      "Epoch [8/10], Step [2600/2737], Loss: 0.2259\n",
      "Epoch [8/10], Step [2610/2737], Loss: 0.2610\n",
      "Epoch [8/10], Step [2620/2737], Loss: 0.1834\n",
      "Epoch [8/10], Step [2630/2737], Loss: 0.2726\n",
      "Epoch [8/10], Step [2640/2737], Loss: 0.3092\n",
      "Epoch [8/10], Step [2650/2737], Loss: 0.3461\n",
      "Epoch [8/10], Step [2660/2737], Loss: 0.2243\n",
      "Epoch [8/10], Step [2670/2737], Loss: 0.2946\n",
      "Epoch [8/10], Step [2680/2737], Loss: 0.1588\n",
      "Epoch [8/10], Step [2690/2737], Loss: 0.2492\n",
      "Epoch [8/10], Step [2700/2737], Loss: 0.1454\n",
      "Epoch [8/10], Step [2710/2737], Loss: 0.2153\n",
      "Epoch [8/10], Step [2720/2737], Loss: 0.1905\n",
      "Epoch [8/10], Step [2730/2737], Loss: 0.1791\n",
      "Epoch [8/10], train_loss: 0.2242, val_loss: 0.2429\n",
      "Epoch [9/10], Step [10/2737], Loss: 0.1734\n",
      "Epoch [9/10], Step [20/2737], Loss: 0.1611\n",
      "Epoch [9/10], Step [30/2737], Loss: 0.2632\n",
      "Epoch [9/10], Step [40/2737], Loss: 0.1943\n",
      "Epoch [9/10], Step [50/2737], Loss: 0.1970\n",
      "Epoch [9/10], Step [60/2737], Loss: 0.1105\n",
      "Epoch [9/10], Step [70/2737], Loss: 0.2177\n",
      "Epoch [9/10], Step [80/2737], Loss: 0.1552\n",
      "Epoch [9/10], Step [90/2737], Loss: 0.2256\n",
      "Epoch [9/10], Step [100/2737], Loss: 0.1815\n",
      "Epoch [9/10], Step [110/2737], Loss: 0.3076\n",
      "Epoch [9/10], Step [120/2737], Loss: 0.3418\n",
      "Epoch [9/10], Step [130/2737], Loss: 0.3221\n",
      "Epoch [9/10], Step [140/2737], Loss: 0.1942\n",
      "Epoch [9/10], Step [150/2737], Loss: 0.1845\n",
      "Epoch [9/10], Step [160/2737], Loss: 0.2217\n",
      "Epoch [9/10], Step [170/2737], Loss: 0.2142\n",
      "Epoch [9/10], Step [180/2737], Loss: 0.1682\n",
      "Epoch [9/10], Step [190/2737], Loss: 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [200/2737], Loss: 0.2798\n",
      "Epoch [9/10], Step [210/2737], Loss: 0.2530\n",
      "Epoch [9/10], Step [220/2737], Loss: 0.2037\n",
      "Epoch [9/10], Step [230/2737], Loss: 0.2435\n",
      "Epoch [9/10], Step [240/2737], Loss: 0.2138\n",
      "Epoch [9/10], Step [250/2737], Loss: 0.2967\n",
      "Epoch [9/10], Step [260/2737], Loss: 0.1176\n",
      "Epoch [9/10], Step [270/2737], Loss: 0.3168\n",
      "Epoch [9/10], Step [280/2737], Loss: 0.1814\n",
      "Epoch [9/10], Step [290/2737], Loss: 0.1772\n",
      "Epoch [9/10], Step [300/2737], Loss: 0.2242\n",
      "Epoch [9/10], Step [310/2737], Loss: 0.1691\n",
      "Epoch [9/10], Step [320/2737], Loss: 0.1888\n",
      "Epoch [9/10], Step [330/2737], Loss: 0.2201\n",
      "Epoch [9/10], Step [340/2737], Loss: 0.2595\n",
      "Epoch [9/10], Step [350/2737], Loss: 0.1666\n",
      "Epoch [9/10], Step [360/2737], Loss: 0.1788\n",
      "Epoch [9/10], Step [370/2737], Loss: 0.2144\n",
      "Epoch [9/10], Step [380/2737], Loss: 0.3076\n",
      "Epoch [9/10], Step [390/2737], Loss: 0.2361\n",
      "Epoch [9/10], Step [400/2737], Loss: 0.2134\n",
      "Epoch [9/10], Step [410/2737], Loss: 0.2044\n",
      "Epoch [9/10], Step [420/2737], Loss: 0.2372\n",
      "Epoch [9/10], Step [430/2737], Loss: 0.2154\n",
      "Epoch [9/10], Step [440/2737], Loss: 0.1749\n",
      "Epoch [9/10], Step [450/2737], Loss: 0.2240\n",
      "Epoch [9/10], Step [460/2737], Loss: 0.2333\n",
      "Epoch [9/10], Step [470/2737], Loss: 0.1636\n",
      "Epoch [9/10], Step [480/2737], Loss: 0.1850\n",
      "Epoch [9/10], Step [490/2737], Loss: 0.1340\n",
      "Epoch [9/10], Step [500/2737], Loss: 0.2271\n",
      "Epoch [9/10], Step [510/2737], Loss: 0.1660\n",
      "Epoch [9/10], Step [520/2737], Loss: 0.1772\n",
      "Epoch [9/10], Step [530/2737], Loss: 0.1489\n",
      "Epoch [9/10], Step [540/2737], Loss: 0.2087\n",
      "Epoch [9/10], Step [550/2737], Loss: 0.1930\n",
      "Epoch [9/10], Step [560/2737], Loss: 0.2243\n",
      "Epoch [9/10], Step [570/2737], Loss: 0.2458\n",
      "Epoch [9/10], Step [580/2737], Loss: 0.2127\n",
      "Epoch [9/10], Step [590/2737], Loss: 0.1873\n",
      "Epoch [9/10], Step [600/2737], Loss: 0.1274\n",
      "Epoch [9/10], Step [610/2737], Loss: 0.2032\n",
      "Epoch [9/10], Step [620/2737], Loss: 0.2218\n",
      "Epoch [9/10], Step [630/2737], Loss: 0.1977\n",
      "Epoch [9/10], Step [640/2737], Loss: 0.1789\n",
      "Epoch [9/10], Step [650/2737], Loss: 0.2571\n",
      "Epoch [9/10], Step [660/2737], Loss: 0.2826\n",
      "Epoch [9/10], Step [670/2737], Loss: 0.2506\n",
      "Epoch [9/10], Step [680/2737], Loss: 0.4403\n",
      "Epoch [9/10], Step [690/2737], Loss: 0.3008\n",
      "Epoch [9/10], Step [700/2737], Loss: 0.3849\n",
      "Epoch [9/10], Step [710/2737], Loss: 0.1937\n",
      "Epoch [9/10], Step [720/2737], Loss: 0.2387\n",
      "Epoch [9/10], Step [730/2737], Loss: 0.1816\n",
      "Epoch [9/10], Step [740/2737], Loss: 0.3086\n",
      "Epoch [9/10], Step [750/2737], Loss: 0.2118\n",
      "Epoch [9/10], Step [760/2737], Loss: 0.1481\n",
      "Epoch [9/10], Step [770/2737], Loss: 0.1165\n",
      "Epoch [9/10], Step [780/2737], Loss: 0.2239\n",
      "Epoch [9/10], Step [790/2737], Loss: 0.2740\n",
      "Epoch [9/10], Step [800/2737], Loss: 0.1870\n",
      "Epoch [9/10], Step [810/2737], Loss: 0.2665\n",
      "Epoch [9/10], Step [820/2737], Loss: 0.2394\n",
      "Epoch [9/10], Step [830/2737], Loss: 0.3570\n",
      "Epoch [9/10], Step [840/2737], Loss: 0.1756\n",
      "Epoch [9/10], Step [850/2737], Loss: 0.1328\n",
      "Epoch [9/10], Step [860/2737], Loss: 0.2855\n",
      "Epoch [9/10], Step [870/2737], Loss: 0.2041\n",
      "Epoch [9/10], Step [880/2737], Loss: 0.2174\n",
      "Epoch [9/10], Step [890/2737], Loss: 0.3751\n",
      "Epoch [9/10], Step [900/2737], Loss: 0.1954\n",
      "Epoch [9/10], Step [910/2737], Loss: 0.1643\n",
      "Epoch [9/10], Step [920/2737], Loss: 0.1437\n",
      "Epoch [9/10], Step [930/2737], Loss: 0.1917\n",
      "Epoch [9/10], Step [940/2737], Loss: 0.2264\n",
      "Epoch [9/10], Step [950/2737], Loss: 0.2618\n",
      "Epoch [9/10], Step [960/2737], Loss: 0.1619\n",
      "Epoch [9/10], Step [970/2737], Loss: 0.1111\n",
      "Epoch [9/10], Step [980/2737], Loss: 0.2804\n",
      "Epoch [9/10], Step [990/2737], Loss: 0.2273\n",
      "Epoch [9/10], Step [1000/2737], Loss: 0.1462\n",
      "Epoch [9/10], Step [1010/2737], Loss: 0.1835\n",
      "Epoch [9/10], Step [1020/2737], Loss: 0.1693\n",
      "Epoch [9/10], Step [1030/2737], Loss: 0.1383\n",
      "Epoch [9/10], Step [1040/2737], Loss: 0.2030\n",
      "Epoch [9/10], Step [1050/2737], Loss: 0.1604\n",
      "Epoch [9/10], Step [1060/2737], Loss: 0.1936\n",
      "Epoch [9/10], Step [1070/2737], Loss: 0.2729\n",
      "Epoch [9/10], Step [1080/2737], Loss: 0.2717\n",
      "Epoch [9/10], Step [1090/2737], Loss: 0.1766\n",
      "Epoch [9/10], Step [1100/2737], Loss: 0.1641\n",
      "Epoch [9/10], Step [1110/2737], Loss: 0.1976\n",
      "Epoch [9/10], Step [1120/2737], Loss: 0.1677\n",
      "Epoch [9/10], Step [1130/2737], Loss: 0.2222\n",
      "Epoch [9/10], Step [1140/2737], Loss: 0.1116\n",
      "Epoch [9/10], Step [1150/2737], Loss: 0.1962\n",
      "Epoch [9/10], Step [1160/2737], Loss: 0.1548\n",
      "Epoch [9/10], Step [1170/2737], Loss: 0.2960\n",
      "Epoch [9/10], Step [1180/2737], Loss: 0.1704\n",
      "Epoch [9/10], Step [1190/2737], Loss: 0.2260\n",
      "Epoch [9/10], Step [1200/2737], Loss: 0.1877\n",
      "Epoch [9/10], Step [1210/2737], Loss: 0.2608\n",
      "Epoch [9/10], Step [1220/2737], Loss: 0.2274\n",
      "Epoch [9/10], Step [1230/2737], Loss: 0.1360\n",
      "Epoch [9/10], Step [1240/2737], Loss: 0.3351\n",
      "Epoch [9/10], Step [1250/2737], Loss: 0.2780\n",
      "Epoch [9/10], Step [1260/2737], Loss: 0.2583\n",
      "Epoch [9/10], Step [1270/2737], Loss: 0.2826\n",
      "Epoch [9/10], Step [1280/2737], Loss: 0.2305\n",
      "Epoch [9/10], Step [1290/2737], Loss: 0.1830\n",
      "Epoch [9/10], Step [1300/2737], Loss: 0.1805\n",
      "Epoch [9/10], Step [1310/2737], Loss: 0.2053\n",
      "Epoch [9/10], Step [1320/2737], Loss: 0.2318\n",
      "Epoch [9/10], Step [1330/2737], Loss: 0.1702\n",
      "Epoch [9/10], Step [1340/2737], Loss: 0.1507\n",
      "Epoch [9/10], Step [1350/2737], Loss: 0.2848\n",
      "Epoch [9/10], Step [1360/2737], Loss: 0.2167\n",
      "Epoch [9/10], Step [1370/2737], Loss: 0.1924\n",
      "Epoch [9/10], Step [1380/2737], Loss: 0.1777\n",
      "Epoch [9/10], Step [1390/2737], Loss: 0.1851\n",
      "Epoch [9/10], Step [1400/2737], Loss: 0.1730\n",
      "Epoch [9/10], Step [1410/2737], Loss: 0.2953\n",
      "Epoch [9/10], Step [1420/2737], Loss: 0.1221\n",
      "Epoch [9/10], Step [1430/2737], Loss: 0.1301\n",
      "Epoch [9/10], Step [1440/2737], Loss: 0.2390\n",
      "Epoch [9/10], Step [1450/2737], Loss: 0.2101\n",
      "Epoch [9/10], Step [1460/2737], Loss: 0.3158\n",
      "Epoch [9/10], Step [1470/2737], Loss: 0.2156\n",
      "Epoch [9/10], Step [1480/2737], Loss: 0.1767\n",
      "Epoch [9/10], Step [1490/2737], Loss: 0.2301\n",
      "Epoch [9/10], Step [1500/2737], Loss: 0.2430\n",
      "Epoch [9/10], Step [1510/2737], Loss: 0.1901\n",
      "Epoch [9/10], Step [1520/2737], Loss: 0.3329\n",
      "Epoch [9/10], Step [1530/2737], Loss: 0.1233\n",
      "Epoch [9/10], Step [1540/2737], Loss: 0.2271\n",
      "Epoch [9/10], Step [1550/2737], Loss: 0.2280\n",
      "Epoch [9/10], Step [1560/2737], Loss: 0.1686\n",
      "Epoch [9/10], Step [1570/2737], Loss: 0.3111\n",
      "Epoch [9/10], Step [1580/2737], Loss: 0.2563\n",
      "Epoch [9/10], Step [1590/2737], Loss: 0.1734\n",
      "Epoch [9/10], Step [1600/2737], Loss: 0.1715\n",
      "Epoch [9/10], Step [1610/2737], Loss: 0.2619\n",
      "Epoch [9/10], Step [1620/2737], Loss: 0.1682\n",
      "Epoch [9/10], Step [1630/2737], Loss: 0.2475\n",
      "Epoch [9/10], Step [1640/2737], Loss: 0.2370\n",
      "Epoch [9/10], Step [1650/2737], Loss: 0.1570\n",
      "Epoch [9/10], Step [1660/2737], Loss: 0.1697\n",
      "Epoch [9/10], Step [1670/2737], Loss: 0.1661\n",
      "Epoch [9/10], Step [1680/2737], Loss: 0.2050\n",
      "Epoch [9/10], Step [1690/2737], Loss: 0.2435\n",
      "Epoch [9/10], Step [1700/2737], Loss: 0.2464\n",
      "Epoch [9/10], Step [1710/2737], Loss: 0.3725\n",
      "Epoch [9/10], Step [1720/2737], Loss: 0.3236\n",
      "Epoch [9/10], Step [1730/2737], Loss: 0.1955\n",
      "Epoch [9/10], Step [1740/2737], Loss: 0.2778\n",
      "Epoch [9/10], Step [1750/2737], Loss: 0.2373\n",
      "Epoch [9/10], Step [1760/2737], Loss: 0.2263\n",
      "Epoch [9/10], Step [1770/2737], Loss: 0.2389\n",
      "Epoch [9/10], Step [1780/2737], Loss: 0.2158\n",
      "Epoch [9/10], Step [1790/2737], Loss: 0.1876\n",
      "Epoch [9/10], Step [1800/2737], Loss: 0.2867\n",
      "Epoch [9/10], Step [1810/2737], Loss: 0.2013\n",
      "Epoch [9/10], Step [1820/2737], Loss: 0.1767\n",
      "Epoch [9/10], Step [1830/2737], Loss: 0.2040\n",
      "Epoch [9/10], Step [1840/2737], Loss: 0.2612\n",
      "Epoch [9/10], Step [1850/2737], Loss: 0.2117\n",
      "Epoch [9/10], Step [1860/2737], Loss: 0.2199\n",
      "Epoch [9/10], Step [1870/2737], Loss: 0.2616\n",
      "Epoch [9/10], Step [1880/2737], Loss: 0.1619\n",
      "Epoch [9/10], Step [1890/2737], Loss: 0.1866\n",
      "Epoch [9/10], Step [1900/2737], Loss: 0.2782\n",
      "Epoch [9/10], Step [1910/2737], Loss: 0.2568\n",
      "Epoch [9/10], Step [1920/2737], Loss: 0.2003\n",
      "Epoch [9/10], Step [1930/2737], Loss: 0.2852\n",
      "Epoch [9/10], Step [1940/2737], Loss: 0.1583\n",
      "Epoch [9/10], Step [1950/2737], Loss: 0.1758\n",
      "Epoch [9/10], Step [1960/2737], Loss: 0.1373\n",
      "Epoch [9/10], Step [1970/2737], Loss: 0.2178\n",
      "Epoch [9/10], Step [1980/2737], Loss: 0.1561\n",
      "Epoch [9/10], Step [1990/2737], Loss: 0.1746\n",
      "Epoch [9/10], Step [2000/2737], Loss: 0.2570\n",
      "Epoch [9/10], Step [2010/2737], Loss: 0.1577\n",
      "Epoch [9/10], Step [2020/2737], Loss: 0.2957\n",
      "Epoch [9/10], Step [2030/2737], Loss: 0.1383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [2040/2737], Loss: 0.2703\n",
      "Epoch [9/10], Step [2050/2737], Loss: 0.2079\n",
      "Epoch [9/10], Step [2060/2737], Loss: 0.1886\n",
      "Epoch [9/10], Step [2070/2737], Loss: 0.1740\n",
      "Epoch [9/10], Step [2080/2737], Loss: 0.2153\n",
      "Epoch [9/10], Step [2090/2737], Loss: 0.1810\n",
      "Epoch [9/10], Step [2100/2737], Loss: 0.3231\n",
      "Epoch [9/10], Step [2110/2737], Loss: 0.2711\n",
      "Epoch [9/10], Step [2120/2737], Loss: 0.1463\n",
      "Epoch [9/10], Step [2130/2737], Loss: 0.1904\n",
      "Epoch [9/10], Step [2140/2737], Loss: 0.1823\n",
      "Epoch [9/10], Step [2150/2737], Loss: 0.2037\n",
      "Epoch [9/10], Step [2160/2737], Loss: 0.3261\n",
      "Epoch [9/10], Step [2170/2737], Loss: 0.2709\n",
      "Epoch [9/10], Step [2180/2737], Loss: 0.2121\n",
      "Epoch [9/10], Step [2190/2737], Loss: 0.2090\n",
      "Epoch [9/10], Step [2200/2737], Loss: 0.1626\n",
      "Epoch [9/10], Step [2210/2737], Loss: 0.1217\n",
      "Epoch [9/10], Step [2220/2737], Loss: 0.2365\n",
      "Epoch [9/10], Step [2230/2737], Loss: 0.2369\n",
      "Epoch [9/10], Step [2240/2737], Loss: 0.2636\n",
      "Epoch [9/10], Step [2250/2737], Loss: 0.1673\n",
      "Epoch [9/10], Step [2260/2737], Loss: 0.3154\n",
      "Epoch [9/10], Step [2270/2737], Loss: 0.2396\n",
      "Epoch [9/10], Step [2280/2737], Loss: 0.2541\n",
      "Epoch [9/10], Step [2290/2737], Loss: 0.1924\n",
      "Epoch [9/10], Step [2300/2737], Loss: 0.1800\n",
      "Epoch [9/10], Step [2310/2737], Loss: 0.2611\n",
      "Epoch [9/10], Step [2320/2737], Loss: 0.2567\n",
      "Epoch [9/10], Step [2330/2737], Loss: 0.2394\n",
      "Epoch [9/10], Step [2340/2737], Loss: 0.2576\n",
      "Epoch [9/10], Step [2350/2737], Loss: 0.2136\n",
      "Epoch [9/10], Step [2360/2737], Loss: 0.1978\n",
      "Epoch [9/10], Step [2370/2737], Loss: 0.1712\n",
      "Epoch [9/10], Step [2380/2737], Loss: 0.1618\n",
      "Epoch [9/10], Step [2390/2737], Loss: 0.2232\n",
      "Epoch [9/10], Step [2400/2737], Loss: 0.1626\n",
      "Epoch [9/10], Step [2410/2737], Loss: 0.3807\n",
      "Epoch [9/10], Step [2420/2737], Loss: 0.2387\n",
      "Epoch [9/10], Step [2430/2737], Loss: 0.1827\n",
      "Epoch [9/10], Step [2440/2737], Loss: 0.2488\n",
      "Epoch [9/10], Step [2450/2737], Loss: 0.2365\n",
      "Epoch [9/10], Step [2460/2737], Loss: 0.1717\n",
      "Epoch [9/10], Step [2470/2737], Loss: 0.1383\n",
      "Epoch [9/10], Step [2480/2737], Loss: 0.2170\n",
      "Epoch [9/10], Step [2490/2737], Loss: 0.2067\n",
      "Epoch [9/10], Step [2500/2737], Loss: 0.1297\n",
      "Epoch [9/10], Step [2510/2737], Loss: 0.2033\n",
      "Epoch [9/10], Step [2520/2737], Loss: 0.2845\n",
      "Epoch [9/10], Step [2530/2737], Loss: 0.2899\n",
      "Epoch [9/10], Step [2540/2737], Loss: 0.1479\n",
      "Epoch [9/10], Step [2550/2737], Loss: 0.1707\n",
      "Epoch [9/10], Step [2560/2737], Loss: 0.2604\n",
      "Epoch [9/10], Step [2570/2737], Loss: 0.2003\n",
      "Epoch [9/10], Step [2580/2737], Loss: 0.2155\n",
      "Epoch [9/10], Step [2590/2737], Loss: 0.2005\n",
      "Epoch [9/10], Step [2600/2737], Loss: 0.2194\n",
      "Epoch [9/10], Step [2610/2737], Loss: 0.1567\n",
      "Epoch [9/10], Step [2620/2737], Loss: 0.2197\n",
      "Epoch [9/10], Step [2630/2737], Loss: 0.2057\n",
      "Epoch [9/10], Step [2640/2737], Loss: 0.2184\n",
      "Epoch [9/10], Step [2650/2737], Loss: 0.2342\n",
      "Epoch [9/10], Step [2660/2737], Loss: 0.3769\n",
      "Epoch [9/10], Step [2670/2737], Loss: 0.2561\n",
      "Epoch [9/10], Step [2680/2737], Loss: 0.2118\n",
      "Epoch [9/10], Step [2690/2737], Loss: 0.1932\n",
      "Epoch [9/10], Step [2700/2737], Loss: 0.1898\n",
      "Epoch [9/10], Step [2710/2737], Loss: 0.1120\n",
      "Epoch [9/10], Step [2720/2737], Loss: 0.2533\n",
      "Epoch [9/10], Step [2730/2737], Loss: 0.2029\n",
      "Epoch [9/10], train_loss: 0.2182, val_loss: 0.2308\n",
      "Epoch [10/10], Step [10/2737], Loss: 0.1392\n",
      "Epoch [10/10], Step [20/2737], Loss: 0.3337\n",
      "Epoch [10/10], Step [30/2737], Loss: 0.1547\n",
      "Epoch [10/10], Step [40/2737], Loss: 0.1544\n",
      "Epoch [10/10], Step [50/2737], Loss: 0.1139\n",
      "Epoch [10/10], Step [60/2737], Loss: 0.1290\n",
      "Epoch [10/10], Step [70/2737], Loss: 0.2748\n",
      "Epoch [10/10], Step [80/2737], Loss: 0.2486\n",
      "Epoch [10/10], Step [90/2737], Loss: 0.1797\n",
      "Epoch [10/10], Step [100/2737], Loss: 0.2076\n",
      "Epoch [10/10], Step [110/2737], Loss: 0.2393\n",
      "Epoch [10/10], Step [120/2737], Loss: 0.1787\n",
      "Epoch [10/10], Step [130/2737], Loss: 0.2610\n",
      "Epoch [10/10], Step [140/2737], Loss: 0.3018\n",
      "Epoch [10/10], Step [150/2737], Loss: 0.1654\n",
      "Epoch [10/10], Step [160/2737], Loss: 0.2627\n",
      "Epoch [10/10], Step [170/2737], Loss: 0.1094\n",
      "Epoch [10/10], Step [180/2737], Loss: 0.1647\n",
      "Epoch [10/10], Step [190/2737], Loss: 0.1664\n",
      "Epoch [10/10], Step [200/2737], Loss: 0.3113\n",
      "Epoch [10/10], Step [210/2737], Loss: 0.2469\n",
      "Epoch [10/10], Step [220/2737], Loss: 0.2220\n",
      "Epoch [10/10], Step [230/2737], Loss: 0.1374\n",
      "Epoch [10/10], Step [240/2737], Loss: 0.1835\n",
      "Epoch [10/10], Step [250/2737], Loss: 0.2294\n",
      "Epoch [10/10], Step [260/2737], Loss: 0.1922\n",
      "Epoch [10/10], Step [270/2737], Loss: 0.2002\n",
      "Epoch [10/10], Step [280/2737], Loss: 0.2806\n",
      "Epoch [10/10], Step [290/2737], Loss: 0.1824\n",
      "Epoch [10/10], Step [300/2737], Loss: 0.2460\n",
      "Epoch [10/10], Step [310/2737], Loss: 0.2293\n",
      "Epoch [10/10], Step [320/2737], Loss: 0.1692\n",
      "Epoch [10/10], Step [330/2737], Loss: 0.1679\n",
      "Epoch [10/10], Step [340/2737], Loss: 0.2227\n",
      "Epoch [10/10], Step [350/2737], Loss: 0.1305\n",
      "Epoch [10/10], Step [360/2737], Loss: 0.1854\n",
      "Epoch [10/10], Step [370/2737], Loss: 0.2065\n",
      "Epoch [10/10], Step [380/2737], Loss: 0.1220\n",
      "Epoch [10/10], Step [390/2737], Loss: 0.1111\n",
      "Epoch [10/10], Step [400/2737], Loss: 0.2478\n",
      "Epoch [10/10], Step [410/2737], Loss: 0.2469\n",
      "Epoch [10/10], Step [420/2737], Loss: 0.1824\n",
      "Epoch [10/10], Step [430/2737], Loss: 0.2488\n",
      "Epoch [10/10], Step [440/2737], Loss: 0.1371\n",
      "Epoch [10/10], Step [450/2737], Loss: 0.2650\n",
      "Epoch [10/10], Step [460/2737], Loss: 0.2677\n",
      "Epoch [10/10], Step [470/2737], Loss: 0.1758\n",
      "Epoch [10/10], Step [480/2737], Loss: 0.2859\n",
      "Epoch [10/10], Step [490/2737], Loss: 0.2639\n",
      "Epoch [10/10], Step [500/2737], Loss: 0.1972\n",
      "Epoch [10/10], Step [510/2737], Loss: 0.2880\n",
      "Epoch [10/10], Step [520/2737], Loss: 0.3013\n",
      "Epoch [10/10], Step [530/2737], Loss: 0.1950\n",
      "Epoch [10/10], Step [540/2737], Loss: 0.2371\n",
      "Epoch [10/10], Step [550/2737], Loss: 0.2454\n",
      "Epoch [10/10], Step [560/2737], Loss: 0.1149\n",
      "Epoch [10/10], Step [570/2737], Loss: 0.3125\n",
      "Epoch [10/10], Step [580/2737], Loss: 0.2027\n",
      "Epoch [10/10], Step [590/2737], Loss: 0.1988\n",
      "Epoch [10/10], Step [600/2737], Loss: 0.3160\n",
      "Epoch [10/10], Step [610/2737], Loss: 0.1694\n",
      "Epoch [10/10], Step [620/2737], Loss: 0.2274\n",
      "Epoch [10/10], Step [630/2737], Loss: 0.2313\n",
      "Epoch [10/10], Step [640/2737], Loss: 0.1561\n",
      "Epoch [10/10], Step [650/2737], Loss: 0.1359\n",
      "Epoch [10/10], Step [660/2737], Loss: 0.1487\n",
      "Epoch [10/10], Step [670/2737], Loss: 0.1958\n",
      "Epoch [10/10], Step [680/2737], Loss: 0.2575\n",
      "Epoch [10/10], Step [690/2737], Loss: 0.2485\n",
      "Epoch [10/10], Step [700/2737], Loss: 0.1722\n",
      "Epoch [10/10], Step [710/2737], Loss: 0.2036\n",
      "Epoch [10/10], Step [720/2737], Loss: 0.2103\n",
      "Epoch [10/10], Step [730/2737], Loss: 0.2086\n",
      "Epoch [10/10], Step [740/2737], Loss: 0.1928\n",
      "Epoch [10/10], Step [750/2737], Loss: 0.3151\n",
      "Epoch [10/10], Step [760/2737], Loss: 0.2202\n",
      "Epoch [10/10], Step [770/2737], Loss: 0.1868\n",
      "Epoch [10/10], Step [780/2737], Loss: 0.1249\n",
      "Epoch [10/10], Step [790/2737], Loss: 0.3104\n",
      "Epoch [10/10], Step [800/2737], Loss: 0.2236\n",
      "Epoch [10/10], Step [810/2737], Loss: 0.2839\n",
      "Epoch [10/10], Step [820/2737], Loss: 0.3045\n",
      "Epoch [10/10], Step [830/2737], Loss: 0.2352\n",
      "Epoch [10/10], Step [840/2737], Loss: 0.3339\n",
      "Epoch [10/10], Step [850/2737], Loss: 0.1646\n",
      "Epoch [10/10], Step [860/2737], Loss: 0.1211\n",
      "Epoch [10/10], Step [870/2737], Loss: 0.1722\n",
      "Epoch [10/10], Step [880/2737], Loss: 0.2247\n",
      "Epoch [10/10], Step [890/2737], Loss: 0.1926\n",
      "Epoch [10/10], Step [900/2737], Loss: 0.3486\n",
      "Epoch [10/10], Step [910/2737], Loss: 0.1082\n",
      "Epoch [10/10], Step [920/2737], Loss: 0.2439\n",
      "Epoch [10/10], Step [930/2737], Loss: 0.2492\n",
      "Epoch [10/10], Step [940/2737], Loss: 0.1461\n",
      "Epoch [10/10], Step [950/2737], Loss: 0.1697\n",
      "Epoch [10/10], Step [960/2737], Loss: 0.1757\n",
      "Epoch [10/10], Step [970/2737], Loss: 0.1736\n",
      "Epoch [10/10], Step [980/2737], Loss: 0.3532\n",
      "Epoch [10/10], Step [990/2737], Loss: 0.1825\n",
      "Epoch [10/10], Step [1000/2737], Loss: 0.1354\n",
      "Epoch [10/10], Step [1010/2737], Loss: 0.3526\n",
      "Epoch [10/10], Step [1020/2737], Loss: 0.2544\n",
      "Epoch [10/10], Step [1030/2737], Loss: 0.1964\n",
      "Epoch [10/10], Step [1040/2737], Loss: 0.2849\n",
      "Epoch [10/10], Step [1050/2737], Loss: 0.1947\n",
      "Epoch [10/10], Step [1060/2737], Loss: 0.2552\n",
      "Epoch [10/10], Step [1070/2737], Loss: 0.1602\n",
      "Epoch [10/10], Step [1080/2737], Loss: 0.1685\n",
      "Epoch [10/10], Step [1090/2737], Loss: 0.1966\n",
      "Epoch [10/10], Step [1100/2737], Loss: 0.2670\n",
      "Epoch [10/10], Step [1110/2737], Loss: 0.1976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [1120/2737], Loss: 0.2197\n",
      "Epoch [10/10], Step [1130/2737], Loss: 0.3016\n",
      "Epoch [10/10], Step [1140/2737], Loss: 0.1099\n",
      "Epoch [10/10], Step [1150/2737], Loss: 0.2740\n",
      "Epoch [10/10], Step [1160/2737], Loss: 0.1880\n",
      "Epoch [10/10], Step [1170/2737], Loss: 0.2404\n",
      "Epoch [10/10], Step [1180/2737], Loss: 0.1721\n",
      "Epoch [10/10], Step [1190/2737], Loss: 0.1212\n",
      "Epoch [10/10], Step [1200/2737], Loss: 0.1743\n",
      "Epoch [10/10], Step [1210/2737], Loss: 0.2954\n",
      "Epoch [10/10], Step [1220/2737], Loss: 0.2257\n",
      "Epoch [10/10], Step [1230/2737], Loss: 0.2344\n",
      "Epoch [10/10], Step [1240/2737], Loss: 0.1430\n",
      "Epoch [10/10], Step [1250/2737], Loss: 0.2773\n",
      "Epoch [10/10], Step [1260/2737], Loss: 0.2767\n",
      "Epoch [10/10], Step [1270/2737], Loss: 0.3063\n",
      "Epoch [10/10], Step [1280/2737], Loss: 0.1755\n",
      "Epoch [10/10], Step [1290/2737], Loss: 0.2841\n",
      "Epoch [10/10], Step [1300/2737], Loss: 0.2305\n",
      "Epoch [10/10], Step [1310/2737], Loss: 0.2226\n",
      "Epoch [10/10], Step [1320/2737], Loss: 0.2135\n",
      "Epoch [10/10], Step [1330/2737], Loss: 0.1451\n",
      "Epoch [10/10], Step [1340/2737], Loss: 0.2489\n",
      "Epoch [10/10], Step [1350/2737], Loss: 0.3313\n",
      "Epoch [10/10], Step [1360/2737], Loss: 0.3601\n",
      "Epoch [10/10], Step [1370/2737], Loss: 0.2787\n",
      "Epoch [10/10], Step [1380/2737], Loss: 0.2966\n",
      "Epoch [10/10], Step [1390/2737], Loss: 0.1635\n",
      "Epoch [10/10], Step [1400/2737], Loss: 0.2455\n",
      "Epoch [10/10], Step [1410/2737], Loss: 0.2190\n",
      "Epoch [10/10], Step [1420/2737], Loss: 0.3436\n",
      "Epoch [10/10], Step [1430/2737], Loss: 0.1978\n",
      "Epoch [10/10], Step [1440/2737], Loss: 0.1603\n",
      "Epoch [10/10], Step [1450/2737], Loss: 0.2791\n",
      "Epoch [10/10], Step [1460/2737], Loss: 0.1416\n",
      "Epoch [10/10], Step [1470/2737], Loss: 0.2008\n",
      "Epoch [10/10], Step [1480/2737], Loss: 0.4177\n",
      "Epoch [10/10], Step [1490/2737], Loss: 0.1881\n",
      "Epoch [10/10], Step [1500/2737], Loss: 0.3451\n",
      "Epoch [10/10], Step [1510/2737], Loss: 0.2251\n",
      "Epoch [10/10], Step [1520/2737], Loss: 0.2519\n",
      "Epoch [10/10], Step [1530/2737], Loss: 0.2406\n",
      "Epoch [10/10], Step [1540/2737], Loss: 0.1789\n",
      "Epoch [10/10], Step [1550/2737], Loss: 0.1936\n",
      "Epoch [10/10], Step [1560/2737], Loss: 0.1901\n",
      "Epoch [10/10], Step [1570/2737], Loss: 0.2661\n",
      "Epoch [10/10], Step [1580/2737], Loss: 0.2225\n",
      "Epoch [10/10], Step [1590/2737], Loss: 0.2027\n",
      "Epoch [10/10], Step [1600/2737], Loss: 0.1860\n",
      "Epoch [10/10], Step [1610/2737], Loss: 0.2581\n",
      "Epoch [10/10], Step [1620/2737], Loss: 0.2363\n",
      "Epoch [10/10], Step [1630/2737], Loss: 0.2093\n",
      "Epoch [10/10], Step [1640/2737], Loss: 0.2475\n",
      "Epoch [10/10], Step [1650/2737], Loss: 0.1705\n",
      "Epoch [10/10], Step [1660/2737], Loss: 0.2609\n",
      "Epoch [10/10], Step [1670/2737], Loss: 0.1409\n",
      "Epoch [10/10], Step [1680/2737], Loss: 0.1993\n",
      "Epoch [10/10], Step [1690/2737], Loss: 0.3245\n",
      "Epoch [10/10], Step [1700/2737], Loss: 0.2450\n",
      "Epoch [10/10], Step [1710/2737], Loss: 0.1756\n",
      "Epoch [10/10], Step [1720/2737], Loss: 0.1975\n",
      "Epoch [10/10], Step [1730/2737], Loss: 0.2700\n",
      "Epoch [10/10], Step [1740/2737], Loss: 0.1975\n",
      "Epoch [10/10], Step [1750/2737], Loss: 0.2438\n",
      "Epoch [10/10], Step [1760/2737], Loss: 0.1817\n",
      "Epoch [10/10], Step [1770/2737], Loss: 0.1381\n",
      "Epoch [10/10], Step [1780/2737], Loss: 0.2590\n",
      "Epoch [10/10], Step [1790/2737], Loss: 0.1736\n",
      "Epoch [10/10], Step [1800/2737], Loss: 0.2907\n",
      "Epoch [10/10], Step [1810/2737], Loss: 0.1649\n",
      "Epoch [10/10], Step [1820/2737], Loss: 0.2494\n",
      "Epoch [10/10], Step [1830/2737], Loss: 0.2031\n",
      "Epoch [10/10], Step [1840/2737], Loss: 0.1832\n",
      "Epoch [10/10], Step [1850/2737], Loss: 0.2286\n",
      "Epoch [10/10], Step [1860/2737], Loss: 0.2176\n",
      "Epoch [10/10], Step [1870/2737], Loss: 0.3079\n",
      "Epoch [10/10], Step [1880/2737], Loss: 0.1673\n",
      "Epoch [10/10], Step [1890/2737], Loss: 0.1924\n",
      "Epoch [10/10], Step [1900/2737], Loss: 0.1911\n",
      "Epoch [10/10], Step [1910/2737], Loss: 0.3091\n",
      "Epoch [10/10], Step [1920/2737], Loss: 0.1848\n",
      "Epoch [10/10], Step [1930/2737], Loss: 0.2079\n",
      "Epoch [10/10], Step [1940/2737], Loss: 0.2052\n",
      "Epoch [10/10], Step [1950/2737], Loss: 0.2392\n",
      "Epoch [10/10], Step [1960/2737], Loss: 0.1733\n",
      "Epoch [10/10], Step [1970/2737], Loss: 0.1247\n",
      "Epoch [10/10], Step [1980/2737], Loss: 0.2927\n",
      "Epoch [10/10], Step [1990/2737], Loss: 0.3074\n",
      "Epoch [10/10], Step [2000/2737], Loss: 0.2301\n",
      "Epoch [10/10], Step [2010/2737], Loss: 0.2669\n",
      "Epoch [10/10], Step [2020/2737], Loss: 0.2825\n",
      "Epoch [10/10], Step [2030/2737], Loss: 0.2138\n",
      "Epoch [10/10], Step [2040/2737], Loss: 0.2353\n",
      "Epoch [10/10], Step [2050/2737], Loss: 0.1321\n",
      "Epoch [10/10], Step [2060/2737], Loss: 0.2847\n",
      "Epoch [10/10], Step [2070/2737], Loss: 0.2354\n",
      "Epoch [10/10], Step [2080/2737], Loss: 0.1395\n",
      "Epoch [10/10], Step [2090/2737], Loss: 0.1830\n",
      "Epoch [10/10], Step [2100/2737], Loss: 0.1773\n",
      "Epoch [10/10], Step [2110/2737], Loss: 0.2147\n",
      "Epoch [10/10], Step [2120/2737], Loss: 0.1646\n",
      "Epoch [10/10], Step [2130/2737], Loss: 0.1584\n",
      "Epoch [10/10], Step [2140/2737], Loss: 0.1600\n",
      "Epoch [10/10], Step [2150/2737], Loss: 0.1443\n",
      "Epoch [10/10], Step [2160/2737], Loss: 0.1930\n",
      "Epoch [10/10], Step [2170/2737], Loss: 0.1535\n",
      "Epoch [10/10], Step [2180/2737], Loss: 0.2113\n",
      "Epoch [10/10], Step [2190/2737], Loss: 0.1840\n",
      "Epoch [10/10], Step [2200/2737], Loss: 0.3094\n",
      "Epoch [10/10], Step [2210/2737], Loss: 0.1603\n",
      "Epoch [10/10], Step [2220/2737], Loss: 0.1622\n",
      "Epoch [10/10], Step [2230/2737], Loss: 0.1609\n",
      "Epoch [10/10], Step [2240/2737], Loss: 0.2266\n",
      "Epoch [10/10], Step [2250/2737], Loss: 0.1576\n",
      "Epoch [10/10], Step [2260/2737], Loss: 0.2656\n",
      "Epoch [10/10], Step [2270/2737], Loss: 0.1933\n",
      "Epoch [10/10], Step [2280/2737], Loss: 0.1660\n",
      "Epoch [10/10], Step [2290/2737], Loss: 0.2047\n",
      "Epoch [10/10], Step [2300/2737], Loss: 0.2190\n",
      "Epoch [10/10], Step [2310/2737], Loss: 0.2846\n",
      "Epoch [10/10], Step [2320/2737], Loss: 0.1435\n",
      "Epoch [10/10], Step [2330/2737], Loss: 0.1700\n",
      "Epoch [10/10], Step [2340/2737], Loss: 0.1942\n",
      "Epoch [10/10], Step [2350/2737], Loss: 0.2444\n",
      "Epoch [10/10], Step [2360/2737], Loss: 0.1585\n",
      "Epoch [10/10], Step [2370/2737], Loss: 0.1480\n",
      "Epoch [10/10], Step [2380/2737], Loss: 0.2093\n",
      "Epoch [10/10], Step [2390/2737], Loss: 0.3816\n",
      "Epoch [10/10], Step [2400/2737], Loss: 0.2101\n",
      "Epoch [10/10], Step [2410/2737], Loss: 0.1672\n",
      "Epoch [10/10], Step [2420/2737], Loss: 0.2782\n",
      "Epoch [10/10], Step [2430/2737], Loss: 0.2075\n",
      "Epoch [10/10], Step [2440/2737], Loss: 0.3285\n",
      "Epoch [10/10], Step [2450/2737], Loss: 0.2670\n",
      "Epoch [10/10], Step [2460/2737], Loss: 0.2678\n",
      "Epoch [10/10], Step [2470/2737], Loss: 0.1935\n",
      "Epoch [10/10], Step [2480/2737], Loss: 0.2410\n",
      "Epoch [10/10], Step [2490/2737], Loss: 0.2540\n",
      "Epoch [10/10], Step [2500/2737], Loss: 0.1330\n",
      "Epoch [10/10], Step [2510/2737], Loss: 0.2250\n",
      "Epoch [10/10], Step [2520/2737], Loss: 0.2292\n",
      "Epoch [10/10], Step [2530/2737], Loss: 0.1830\n",
      "Epoch [10/10], Step [2540/2737], Loss: 0.1376\n",
      "Epoch [10/10], Step [2550/2737], Loss: 0.3230\n",
      "Epoch [10/10], Step [2560/2737], Loss: 0.3098\n",
      "Epoch [10/10], Step [2570/2737], Loss: 0.2886\n",
      "Epoch [10/10], Step [2580/2737], Loss: 0.2772\n",
      "Epoch [10/10], Step [2590/2737], Loss: 0.2022\n",
      "Epoch [10/10], Step [2600/2737], Loss: 0.1424\n",
      "Epoch [10/10], Step [2610/2737], Loss: 0.1405\n",
      "Epoch [10/10], Step [2620/2737], Loss: 0.1948\n",
      "Epoch [10/10], Step [2630/2737], Loss: 0.1561\n",
      "Epoch [10/10], Step [2640/2737], Loss: 0.1495\n",
      "Epoch [10/10], Step [2650/2737], Loss: 0.1894\n",
      "Epoch [10/10], Step [2660/2737], Loss: 0.1990\n",
      "Epoch [10/10], Step [2670/2737], Loss: 0.2807\n",
      "Epoch [10/10], Step [2680/2737], Loss: 0.2009\n",
      "Epoch [10/10], Step [2690/2737], Loss: 0.1188\n",
      "Epoch [10/10], Step [2700/2737], Loss: 0.1688\n",
      "Epoch [10/10], Step [2710/2737], Loss: 0.1714\n",
      "Epoch [10/10], Step [2720/2737], Loss: 0.2713\n",
      "Epoch [10/10], Step [2730/2737], Loss: 0.1836\n",
      "Epoch [10/10], train_loss: 0.2137, val_loss: 0.2472\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "#optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#training\n",
    "train_loss, train_acc, val_loss, val_acc= train(10, train_data_loader,  val_data_loader,criterion, optimizer, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDeBukX4oxnG"
   },
   "source": [
    "## **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Bwgu8k1Xo0dd"
   },
   "outputs": [],
   "source": [
    "path = 'saved_models/task3/task3.pt'\n",
    "torch.save(net.state_dict(), path)\n",
    "\n",
    "np.save('saved_models/task3/train_loss', train_loss)\n",
    "np.save('saved_models/task3/val_loss', val_loss)\n",
    "np.save('saved_models/task3/train_acc', train_acc)\n",
    "np.save('saved_models/task3/val_acc', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0ZFjfN1pL-f"
   },
   "source": [
    "## **Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1651881080701,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "i7zafyKTpTm6",
    "outputId": "f7277920-892e-4bf9-e0ea-5afa55624056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_data_loader)\n",
    "img, test_labels = dataiter.next()\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1651881083253,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "EjIAdRf2qwC7",
    "outputId": "327fea7b-975b-4986-b01b-54d320c883c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "net.load_state_dict(torch.load(path))\n",
    "test_out = net(img.to(device))\n",
    "test_out.shape\n",
    "pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in test_out])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5329\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "path = 'saved_models/task3/epoch10,lr0.001/task3.pt'\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(path))\n",
    "\n",
    "test_steps = len(test_data_loader)\n",
    "t_acc=0\n",
    "for i, data in enumerate(test_data_loader): # iterate over batches\n",
    "    # get image and labels data is in tuple form (inputs, label)\n",
    "    image, labels = data\n",
    "    image = image.to(device)\n",
    "    labels = labels.to(device).float()\n",
    "    outputs = net(image)\n",
    "    pred = torch.tensor([[1 if i > 0.5 else 0 for i in y ] for y in outputs])\n",
    "    t_acc += torch.sum((pred == labels).all(axis = 1))/len(labels)\n",
    "\n",
    "print('Test Accuracy: {:.4f}'.format(t_acc/test_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc9du2KHpMFZ"
   },
   "source": [
    "## **Visualize result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2330,
     "status": "ok",
     "timestamp": 1651918809981,
     "user": {
      "displayName": "Ali Khalid",
      "userId": "10809480287003355230"
     },
     "user_tz": -300
    },
    "id": "myKs939VpTFY",
    "outputId": "6b9b03e2-d08e-4ad6-a550-e81f77705b03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'training and validation loss')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_loss = np.load('saved_models/task3/epoch10,lr0.001/train_loss.npy')\n",
    "val_loss = np.load('saved_models/task3/epoch10,lr0.001/val_loss.npy')\n",
    "plt.plot(train_loss, linewidth=2, label='train')\n",
    "plt.plot(val_loss, linewidth=2, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.legend()\n",
    "plt.title('training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.53932226\n",
      "train accuracy: 0.53932226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'training and validation Accuracy')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_acc = np.load('saved_models/task3/epoch10,lr0.001/val_acc.npy')\n",
    "train_acc = np.load('saved_models/task3/epoch10,lr0.001/train_acc.npy')\n",
    "print('val accuracy:', val_acc[-1])\n",
    "print('train accuracy:', train_acc[-1])\n",
    "plt.plot(train_acc, linewidth=2, label='train')\n",
    "plt.plot(val_acc, linewidth=2, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.legend()\n",
    "plt.title('training and validation Accuracy')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
